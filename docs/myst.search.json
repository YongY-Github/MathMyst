{"version":"1","records":[{"hierarchy":{"lvl1":"Absolute Basics"},"type":"lvl1","url":"/absolute-basics","position":0},{"hierarchy":{"lvl1":"Absolute Basics"},"content":"This chapter introduces the very basics which you need to know, but is not covered in depth here.","type":"content","url":"/absolute-basics","position":1},{"hierarchy":{"lvl1":"Absolute Basics"},"type":"lvl1","url":"/absolute-basics-4aa7-fb3213def91849c4518e8b5f81d01","position":0},{"hierarchy":{"lvl1":"Absolute Basics"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines\noptimality.","type":"content","url":"/absolute-basics-4aa7-fb3213def91849c4518e8b5f81d01","position":1},{"hierarchy":{"lvl1":"Absolute Basics"},"type":"lvl1","url":"/absolute-basics-4aa7258ecf813f328a8d1ac1cf999286","position":0},{"hierarchy":{"lvl1":"Absolute Basics"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines\noptimality.","type":"content","url":"/absolute-basics-4aa7258ecf813f328a8d1ac1cf999286","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/absolute-basics-5b91-1585ec150ed8e344155aaa10be99e","position":0},{"hierarchy":{"lvl1":""},"content":"","type":"content","url":"/absolute-basics-5b91-1585ec150ed8e344155aaa10be99e","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/absolute-basics-5b91116912183d3ffee9b3d1bb7d92f6","position":0},{"hierarchy":{"lvl1":""},"content":"","type":"content","url":"/absolute-basics-5b91116912183d3ffee9b3d1bb7d92f6","position":1},{"hierarchy":{"lvl1":"Absolute Basics"},"type":"lvl1","url":"/absolute-basics-bc82-19145cef509c620d6b899a878f621","position":0},{"hierarchy":{"lvl1":"Absolute Basics"},"content":"This chapter introduces the very basics which you need to know, but is not covered in depth here.","type":"content","url":"/absolute-basics-bc82-19145cef509c620d6b899a878f621","position":1},{"hierarchy":{"lvl1":"Absolute Basics"},"type":"lvl1","url":"/absolute-basics-bc829058a4a38b997aaffcd3cba760b7","position":0},{"hierarchy":{"lvl1":"Absolute Basics"},"content":"This chapter introduces the very basics which you need to know, but is not covered in depth here.","type":"content","url":"/absolute-basics-bc829058a4a38b997aaffcd3cba760b7","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-096ab075981bbf-0516a024e444dcdbf583698588996","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/index-096ab075981bbf-0516a024e444dcdbf583698588996","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-096ab075981bbf-0516a024e444dcdbf583698588996#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nOptimization\n\nIntegration\n\nDifference equations\n\nDifferential equations\n\nNotes prepared by -YY- (please do not sistribute without permission)","type":"content","url":"/index-096ab075981bbf-0516a024e444dcdbf583698588996#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-096ab075981bbfcb726fd53e43a36543","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/index-096ab075981bbfcb726fd53e43a36543","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-096ab075981bbfcb726fd53e43a36543#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nOptimization\n\nIntegration\n\nDifference equations\n\nDifferential equations\n\nNotes prepared by -YY- (please do not sistribute without permission)","type":"content","url":"/index-096ab075981bbfcb726fd53e43a36543#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-3d478498f50bb6-514a4a45561ecc0651a9ecb61c0db","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive,\nexample-driven, and minimally technical where possible.","type":"content","url":"/index-3d478498f50bb6-514a4a45561ecc0651a9ecb61c0db","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-3d478498f50bb6-514a4a45561ecc0651a9ecb61c0db#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basic Concepts)\n\nLinear Algebra (Special Matrices & Applications)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-3d478498f50bb6-514a4a45561ecc0651a9ecb61c0db#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-3d478498f50bb690873cdb7ad5bbc9dc","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive,\nexample-driven, and minimally technical where possible.","type":"content","url":"/index-3d478498f50bb690873cdb7ad5bbc9dc","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-3d478498f50bb690873cdb7ad5bbc9dc#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basic Concepts)\n\nLinear Algebra (Special Matrices & Applications)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-3d478498f50bb690873cdb7ad5bbc9dc#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-43d90c73929375-103eb4a75fa7a33b7851f398d752f","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive,\nexample-driven, and minimally technical where possible.","type":"content","url":"/index-43d90c73929375-103eb4a75fa7a33b7851f398d752f","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-43d90c73929375-103eb4a75fa7a33b7851f398d752f#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Linear Algebra (Basic Concepts)\n\nLinear Algebra (Special Matrices & Applications)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-43d90c73929375-103eb4a75fa7a33b7851f398d752f#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-43d90c73929375c90ca55994b7b7e38e","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive,\nexample-driven, and minimally technical where possible.","type":"content","url":"/index-43d90c73929375c90ca55994b7b7e38e","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-43d90c73929375c90ca55994b7b7e38e#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Linear Algebra (Basic Concepts)\n\nLinear Algebra (Special Matrices & Applications)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-43d90c73929375c90ca55994b7b7e38e#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-91ff80b3111fc6-4845de5cdc26e4843d5e06fcffb0a","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/index-91ff80b3111fc6-4845de5cdc26e4843d5e06fcffb0a","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-91ff80b3111fc6-4845de5cdc26e4843d5e06fcffb0a#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nOptimization\n\nIntegration\n\nDifference equations\n\nDifferential equations\n\nNotes prepared by YY (please do not sistribute without permission)","type":"content","url":"/index-91ff80b3111fc6-4845de5cdc26e4843d5e06fcffb0a#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-91ff80b3111fc63f7caf71752fe527b8","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/index-91ff80b3111fc63f7caf71752fe527b8","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-91ff80b3111fc63f7caf71752fe527b8#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nOptimization\n\nIntegration\n\nDifference equations\n\nDifferential equations\n\nNotes prepared by YY (please do not sistribute without permission)","type":"content","url":"/index-91ff80b3111fc63f7caf71752fe527b8#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-93d9baee4b74ec-ad48d978733d8f8643aba0f108924","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive,\nexample-driven, and minimally technical where possible.","type":"content","url":"/index-93d9baee4b74ec-ad48d978733d8f8643aba0f108924","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-93d9baee4b74ec-ad48d978733d8f8643aba0f108924#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-93d9baee4b74ec-ad48d978733d8f8643aba0f108924#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-93d9baee4b74ec20dfb732a4ecaf38b4","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive,\nexample-driven, and minimally technical where possible.","type":"content","url":"/index-93d9baee4b74ec20dfb732a4ecaf38b4","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-93d9baee4b74ec20dfb732a4ecaf38b4#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-93d9baee4b74ec20dfb732a4ecaf38b4#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-9991fcb42a4e79-c1096553849d68798df5dec844215","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive,\nexample-driven, and minimally technical where possible.","type":"content","url":"/index-9991fcb42a4e79-c1096553849d68798df5dec844215","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-9991fcb42a4e79-c1096553849d68798df5dec844215#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Linear Algebra (Basic Concepts)\n\nLinear Algebra (Special Matrices & Applications)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-9991fcb42a4e79-c1096553849d68798df5dec844215#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-9991fcb42a4e796ee189d995ee4f7396","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive,\nexample-driven, and minimally technical where possible.","type":"content","url":"/index-9991fcb42a4e796ee189d995ee4f7396","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-9991fcb42a4e796ee189d995ee4f7396#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Linear Algebra (Basic Concepts)\n\nLinear Algebra (Special Matrices & Applications)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-9991fcb42a4e796ee189d995ee4f7396#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-bde7df7f9e6963-93c8c2c53dd478bc47f837d0b34b2","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/index-bde7df7f9e6963-93c8c2c53dd478bc47f837d0b34b2","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-bde7df7f9e6963-93c8c2c53dd478bc47f837d0b34b2#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nOptimization\n\nIntegration\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-bde7df7f9e6963-93c8c2c53dd478bc47f837d0b34b2#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-bde7df7f9e696323455e0a40b5c4d7f9","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/index-bde7df7f9e696323455e0a40b5c4d7f9","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-bde7df7f9e696323455e0a40b5c4d7f9#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nOptimization\n\nIntegration\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-bde7df7f9e696323455e0a40b5c4d7f9#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-ceda07ea84cf21-04abfef9cc0d4e0e56357c4a89388","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/index-ceda07ea84cf21-04abfef9cc0d4e0e56357c4a89388","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-ceda07ea84cf21-04abfef9cc0d4e0e56357c4a89388#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nOptimization\n\nIntegration\n\nDifference equations\n\nDifferential equations\n\nNotes prepared by -YY- (please do not distribute without permission) test ","type":"content","url":"/index-ceda07ea84cf21-04abfef9cc0d4e0e56357c4a89388#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-ceda07ea84cf21b9eab6f0b3bae616a8","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/index-ceda07ea84cf21b9eab6f0b3bae616a8","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-ceda07ea84cf21b9eab6f0b3bae616a8#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nOptimization\n\nIntegration\n\nDifference equations\n\nDifferential equations\n\nNotes prepared by -YY- (please do not distribute without permission) test ","type":"content","url":"/index-ceda07ea84cf21b9eab6f0b3bae616a8#topics","position":3},{"hierarchy":{"lvl1":"Mathematical Methods for Economics"},"type":"lvl1","url":"/index-d2aa8c5e4d93fd-544e6a5f98e87f671d87d15b2aa5e","position":0},{"hierarchy":{"lvl1":"Mathematical Methods for Economics"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive,\nexample-driven, and minimally technical where possible.","type":"content","url":"/index-d2aa8c5e4d93fd-544e6a5f98e87f671d87d15b2aa5e","position":1},{"hierarchy":{"lvl1":"Mathematical Methods for Economics","lvl2":"Topics"},"type":"lvl2","url":"/index-d2aa8c5e4d93fd-544e6a5f98e87f671d87d15b2aa5e#topics","position":2},{"hierarchy":{"lvl1":"Mathematical Methods for Economics","lvl2":"Topics"},"content":"Linear Algebra (Basic Concepts)\n\nLinear Algebra (Special Matrices & Applications)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-d2aa8c5e4d93fd-544e6a5f98e87f671d87d15b2aa5e#topics","position":3},{"hierarchy":{"lvl1":"Mathematical Methods for Economics"},"type":"lvl1","url":"/index-d2aa8c5e4d93fd50d1c64d9163ed0905","position":0},{"hierarchy":{"lvl1":"Mathematical Methods for Economics"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive,\nexample-driven, and minimally technical where possible.","type":"content","url":"/index-d2aa8c5e4d93fd50d1c64d9163ed0905","position":1},{"hierarchy":{"lvl1":"Mathematical Methods for Economics","lvl2":"Topics"},"type":"lvl2","url":"/index-d2aa8c5e4d93fd50d1c64d9163ed0905#topics","position":2},{"hierarchy":{"lvl1":"Mathematical Methods for Economics","lvl2":"Topics"},"content":"Linear Algebra (Basic Concepts)\n\nLinear Algebra (Special Matrices & Applications)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-d2aa8c5e4d93fd50d1c64d9163ed0905#topics","position":3},{"hierarchy":{"lvl1":"Math Notes"},"type":"lvl1","url":"/index-d7a06894d49110-249dc81be02f4ff80073e48cae9b2","position":0},{"hierarchy":{"lvl1":"Math Notes"},"content":"This is my first MyST page.\n\nInline math: a^2+b^2=c^2\\int_0^1 x^2\\,dx=\\frac{1}{3}","type":"content","url":"/index-d7a06894d49110-249dc81be02f4ff80073e48cae9b2","position":1},{"hierarchy":{"lvl1":"Math Notes"},"type":"lvl1","url":"/index-d7a06894d49110b1095fb351de84a1ee","position":0},{"hierarchy":{"lvl1":"Math Notes"},"content":"This is my first MyST page.\n\nInline math: a^2+b^2=c^2\\int_0^1 x^2\\,dx=\\frac{1}{3}","type":"content","url":"/index-d7a06894d49110b1095fb351de84a1ee","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-ee71cd2ee3a2fa-7325cf7c843305d1951f214f077b2","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/index-ee71cd2ee3a2fa-7325cf7c843305d1951f214f077b2","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-ee71cd2ee3a2fa-7325cf7c843305d1951f214f077b2#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-ee71cd2ee3a2fa-7325cf7c843305d1951f214f077b2#topics","position":3},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/index-ee71cd2ee3a2fa98c2f5278f7c427c61","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/index-ee71cd2ee3a2fa98c2f5278f7c427c61","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/index-ee71cd2ee3a2fa98c2f5278f7c427c61#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nDifference equations\n\nDifferential equations","type":"content","url":"/index-ee71cd2ee3a2fa98c2f5278f7c427c61#topics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-00e8b7376a9aed81dab8a9ee35ce1","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-00e8b7376a9aed81dab8a9ee35ce1","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-00e8b7376a9aed81dab8a9ee35ce1#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If ( \\mathbf{A} = (a_{ij}) ) and ( \\mathbf{B} = (b_{ij}) ) are two ( m \\times n ) matrices of same dimension, then ( \\mathbf{A} + \\mathbf{B} ) is defined as ( (a_{ij} + b_{ij}) ). That is, we add element by element the two matrices. Clearly ( \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A} ). The rule applies to matrix subtraction.\n\nTranspose of a matrix: If ( \\mathbf{A} ) is an ( m \\times n ) matrix, then ( \\mathbf{A}’ ) is the ( n \\times m ) matrix whose rows are the columns of ( \\mathbf{A} ). So ( \\mathbf{A}’ = (a_{ji}) ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\na & c \\\nb & d\n\\end{bmatrix}\n]\n\nNote: ( (\\mathbf{A} + \\mathbf{B})’ = \\mathbf{A}’ + \\mathbf{B}’ ), however ( (\\mathbf{A}\\mathbf{B})’ = \\mathbf{B}‘\\mathbf{A}’ ).\n\nNull matrix: Has all elements as 0. Clearly ( \\mathbf{A} + \\mathbf{0}{m,n} = \\mathbf{0}{m,n} + \\mathbf{A} = \\mathbf{A} ) for all ( m \\times n ) matrices.\n\nScalar multiplication: If ( \\mathbf{A} = (a_{ij}) ) then for any constant ( k ), define ( k\\mathbf{A} = (k a_{ij}) ). That is, multiply each element in ( \\mathbf{A} ) by ( k ).\n\nMatrix multiplication: Say ( \\mathbf{A} ) is ( m \\times n ), and ( \\mathbf{B} ) is ( n \\times p ), then the ( m \\times p ) matrix ( \\mathbf{A}\\mathbf{B} ) is the product of ( \\mathbf{A} ) and ( \\mathbf{B} ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n\n= \\begin{bmatrix}\nae + bg & af + bh \\\nce + dg & cf + dh\n\\end{bmatrix}\n]\n\nand the matrices ( \\mathbf{A} ) and ( \\mathbf{B} ) are conformable.\n\nDiagonal matrix: A square ( n \\times n ) matrix ( \\mathbf{A} ) is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\n\n[\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\n0 & a_{22} & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n]\n\nNote: The elements in the diagonal of ( \\mathbf{A} ) are the same as those in ( \\mathbf{A}’ ).\n\nTrace of a square matrix: If ( \\mathbf{A} ) is a square matrix, the trace of ( \\mathbf{A} ), denoted ( \\operatorname{tr}(\\mathbf{A}) ), is the sum of the elements on the main/leading diagonal of ( \\mathbf{A} ).\n\nIdentity matrix: Denoted by ( \\mathbf{I}n ), the ( n \\times n ) diagonal matrix with ( a{ii} = 1 ) for all ( i ). That is:\n\n[\n\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\n0 & 1 & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n]\n\nSymmetric matrix: A square matrix ( \\mathbf{A} ) is symmetric if ( \\mathbf{A} = \\mathbf{A}’ ). The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-00e8b7376a9aed81dab8a9ee35ce1#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-00e8b7376a9aed81dab8a9ee35ce1#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if ( \\mathbf{A}^2 = \\mathbf{A} ). Below is an example (try squaring the matrix and see what you get).\n\n[\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\n]\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors ( \\mathbf{x} ) and ( \\mathbf{y} ) (and ( \\mathbf{z} )) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors ( \\mathbf{x} \\neq \\mathbf{0} ) and ( \\mathbf{y} \\neq \\mathbf{0} ) are said to be linearly independent iff the ONLY solution to ( a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} ) is ( a = 0 ) AND ( b = 0 ). And ( \\mathbf{x} ) and ( \\mathbf{y} ) are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-00e8b7376a9aed81dab8a9ee35ce1#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-00e8b7376a9aed81dab8a9ee35ce1#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"content":"Usually ( \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A} ). For example, try ( \\mathbf{A} = \\begin{bmatrix}2 & 0 \\ 3 & 1\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}0 & 1 \\ 0 & 0\\end{bmatrix} ).\n\nWe can have ( \\mathbf{A}\\mathbf{B} = \\mathbf{0} ) even if ( \\mathbf{A} ) or ( \\mathbf{B} ) are not zero. For example, try ( \\mathbf{A} = \\begin{bmatrix}6 & -12 \\ -3 & 6\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}12 & 6 \\ 6 & 3\\end{bmatrix} ).\n\nSay, ( \\mathbf{A} = \\begin{bmatrix}4 & 8 \\ 1 & 2\\end{bmatrix} ), ( \\mathbf{B} = \\begin{bmatrix}2 & 1 \\ 2 & 2\\end{bmatrix} ), and ( \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\ 4 & 2\\end{bmatrix} ). We find that ( \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} ) even though ( \\mathbf{B} \\neq \\mathbf{C} ).\n\nSay ( \\mathbf{A} ) and ( \\mathbf{B} ) are singular matrices. Then ( \\mathbf{A}\\mathbf{B} ) will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-00e8b7376a9aed81dab8a9ee35ce1#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-00e8b7376a9aed81dab8a9ee35ce1#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:\n\n[\n3x_1 + 4x_2 = 5 \\\n7x_1 - 2x_2 = 2\n]\n\nHere, we can define:\n\n[\n\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\ 2 \\end{pmatrix}\n]\n\nThen we see that:\n\n[\n\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\ 7x_1 - 2x_2 \\end{pmatrix}\n]\n\nThe original system is in fact equivalent to the matrix form:\n\n[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-00e8b7376a9aed81dab8a9ee35ce1#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors:\n$$\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\n$$\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = (a_{ij} + b_{ij}).\n\nA matrix can be multiplied by a scalar c:cA = (c a_{ij}).\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is m \\times n and B is n \\times k, then AB is m \\times k.\n\nThe (i,j) element of AB is:(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of transformations matters.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows and columns:(A')_{ij} = a_{ji}.\n\nExample:\n$$\nA =\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\n\\quad\nA’ =\\begin{pmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{pmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric if:A = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfies:AI = IA = A.\n\nFor example:\n$$\nI =\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive identity.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such that:AA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} gives:x = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility usually reflects:\n\nredundancy,\n\nlinear dependence,\n\nlack of unique solutions.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes\nkey properties of the matrix.\n\nFor a 2 \\times 2 matrix:\n$$\nA =\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\n\\quad\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume scaling effect of a linear transformation.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equations:Ax = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution is:x_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems, but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"Columns of a matrix are linearly dependent if one column can be written as\na linear combination of others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or columns.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-03c3b1d0a47e40d1770529b175787#what-comes-next","position":33},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If A=(a_{ij}) and B=(b_{ij}) are two m\\times n matrices of same dimension, then A+B is defined as (a_{ij}+b_{ij}). That is, we add element by element the two matrices. Clearly A+B=B+A. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If A is a m\\times n matrix, then A' is the n\\times m matrix whose rows are the columns of A. So A^{\\prime}=(a_{ji}).\n\nExample and Notes:\n\n[\\begin{matrix}2&-3\\\\ -4&5\\end{matrix}]^{\\prime}=[\\begin{matrix}2&-4\\\\ -3&5\\end{matrix}]\n\n(A+B)^{\\prime}=A^{\\prime}+B^{\\prime}\n\n(AB)^{\\prime}=B^{\\prime}A^{\\prime}\n\nNull matrix: Has all elements as 0. Clearly A+O_{m,n}=0_{m,n}+A=A for all m\\times n matrices.\n\nScalar multiplication: If A=(a_{ij}) then for any constant k define kA=(ka_{ij}). That is, multiply each element in A by k.\n\nMatrix multiplication: Say A is m\\times n and B is n\\times p then the m\\times p matrix AB is the product of A and B. For example:\n\n[\\begin{matrix}1&2\\\\ 4&0\\end{matrix}][\\begin{matrix}3&-1\\\\ 4&3\\end{string}]=[\\begin{matrix}11&5\\\\ 12&-4\\end{matrix}]\n\nThe matrices A and B are comformable.\n\nDiagonal matrix: A square n\\times n matrix A is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\nA=[\\begin{matrix}a_{11}&0&...&0\\\\ 0&a_{22}&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&a_{nn}\\end{matrix}]\nNote: The elements in the diagonal of A are the same as those in A'.\n\nTrace of a square matrix: If A is a square, the trace of A, denoted tr(A), is the sum of the elements on the main/leading diagonal of A.\n\nIdentity matrix: Denoted by I_{n}, the n\\times n diagonal matrix with a_{ij}=1 for all i. That is:\nA=[\\begin{matrix}1&0&...&0\\\\ 0&1&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&1\\end{matrix}]\n\nSymmetric matrix: A square matrix A is symmetric if A=A^{\\prime}. The identity matrix and the square null matrix are symmetric.\n\nIdempotent matrix: A square matrix is said to be idempotent if A^{2}=A.\nExample: A=[\\begin{matrix}2&-2&-4\\\\ -1&3&4\\\\ 1&-2&-3\\end{matrix}]\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has a determinant of zero.\n\nNote that any two (or three) vectors x and y (and z) are said to be linearly dependent iff one can be written as a scalar multiple of the other.\n\nTwo vectors x\\ne0 and y\\ne0 are said to be LINEARLY INDEPENDENT iff the ONLY solution to ax+by=0 is a=0 AND b=0. And x and y are said to be ORTHOGONAL.","type":"content","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"type":"lvl3","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#some-notes-on-matrices","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"content":"Usually AB\\ne BA.\n\nWe can have AB=0 even if A or B are not zero.\n\nWe find that AB=AC even though B\\ne C.\n\nSay A and B are singular matrices. Then AB will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#some-notes-on-matrices","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#systems-of-equations-in-matrix-form","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system:\n3x_{1}+4x_{2}=5\n7x_{1}-2x_{2}=2\n\nThis is equivalent to the matrix form Ax=b where:\nA = [\\begin{matrix}3&4\\\\ 7&-2\\end{matrix}], x=[\\begin{matrix}x_{1}\\\\ x_{2}\\end{matrix}], and b=[\\begin{matrix}5\\\\ 2\\end{matrix}]","type":"content","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#systems-of-equations-in-matrix-form","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#matrix-inversion","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"content":"In fact, A^{-1}A=AA^{-1}=I. To find the inverse of A=[\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}]:\n\nThink of another matrix such that [\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}][\\begin{matrix}a&b\\\\ c&d\\end{matrix}]=[\\begin{matrix}10&0\\\\ 0&10\\end{matrix}].\n\nThe 10 comes from the determinant of A.\n\nThe matrix \\frac{1}{10}[\\begin{matrix}4&2\\\\ -3&1\\end{matrix}] is the inverse of A.","type":"content","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#matrix-inversion","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"type":"lvl3","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#properties-of-the-inverse","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"content":"Property 1: For any nonsingular matrix A, (A^{-1})^{-1}=A.\n\nProperty 2: The determinant of the inverse is the reciprocal of the determinant: |A^{-1}|=\\frac{1}{|A|}.\n\nProperty 3: The inverse of matrix A is unique.\n\nProperty 4: The inverse of the transpose is equal to the transpose of the inverse: (A^{\\prime})^{-1}=(A^{-1})^{\\prime}.\n\nProperty 5: If A and B are nonsingular, AB is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices: (AB)^{-1}=B^{-1}A^{-1}.","type":"content","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#properties-of-the-inverse","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar-value uniquely associated with a square non-singular matrix, denoted as |A|.\n\nFor a 2\\times2 matrix A=[\\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix}], the determinant is defined as |A|=a_{11}a_{22}-a_{12}a_{21}.\n\nFor a 3\\times3 matrix, the Laplace expansion is often used.","type":"content","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#notes-on-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"content":"|A|=|A^{\\prime}| and |A|\\cdot|B|=|B|\\cdot|A|.\n\nInterchanging any two rows or columns changes the sign of the determinant.\n\nMultiplying a single row or column by a scalar multiplies the determinant by that scalar.\n\nAddition or subtraction of a nonzero multiple of a row/column to another does not change the determinant.\n\nThe determinant of a triangular matrix is the product of the diagonal elements.\n\nIf any rows or columns equal zero, the determinant is zero.\n\nIf two rows or columns are linearly dependent, the determinant is zero.","type":"content","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#notes-on-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"type":"lvl2","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#finding-the-inverse-of-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"content":"Defined as: A^{-1}=\\frac{1}{|A|}adjA.\n\nStep 1: The minor |M_{ij}| is the determinant of the submatrix formed by deleting the ith row and jth column.\nStep 2: The cofactor |C_{ij}|=(-1)^{i+j}|M_{ij}|.\nStep 3: The adjoint matrix is the transpose of the cofactor matrix.","type":"content","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#finding-the-inverse-of-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"content":"A simplified method for solving Ax=b through determinants.\nx_{i}=\\frac{|\\Delta_{i}|}{|\\Delta|}\nWhere \\Delta_{i} is the matrix A with the ith column replaced by the b vector.","type":"content","url":"/linear-algebra-basic-04a1c0dbc0180589f94d4265529b7#cramers-rule","position":19},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics"},"type":"lvl1","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics"},"content":"","type":"content","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}' = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Some Notes on Matrices"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:3x_1 + 4x_2 = 5 \\\\\n7x_1 - 2x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\\\ 7x_1 - 2x_2 \\end{pmatrix}\n\nThe original system is in fact equivalent to the matrix form:\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#matrix-inversion","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Inversion"},"content":"To better understand the mechanics of finding the inverse of a square matrix, let’s begin by looking at an example.\\mathbf{A} = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nAssume we have:\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix}\n\nWhere does the 10 in the matrix after the = sign come from? It is the determinant of \\mathbf{A}.\n\nNow we have:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 10 & 0 \\\\ 0 & 10 \\end{bmatrix} = 10 \\mathbf{I}\n\nHence, (pre-)multiplying both sides of the equation by \\frac{1}{10} will give you:\\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\mathbf{I}\n\nThe matrix \\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} is in fact the inverse of \\mathbf{A}. Hence we have the relationship \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}. In fact, \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.\n\nHere are some properties of the inverse of a matrix worth knowing:\n\nProperty 1: For any nonsingular matrix \\mathbf{A}, (\\mathbf{A}^{-1})^{-1} = \\mathbf{A}.\n\nProperty 2: The determinant of the inverse of a matrix is equal to the reciprocal of the determinant of the matrix. That is |\\mathbf{A}^{-1}| = \\frac{1}{|\\mathbf{A}|}.\n\nProperty 3: The inverse of matrix \\mathbf{A} is unique.\n\nProperty 4: For any nonsingular matrix \\mathbf{A}, the inverse of the transpose of a matrix is equal to the transpose of the inverse of the matrix. (\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'.\n\nProperty 5: If \\mathbf{A} and \\mathbf{B} are nonsingular and of the same dimension, then \\mathbf{A}\\mathbf{B} is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices is equal to the product of their inverses in reverse order. (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}.","type":"content","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#matrix-inversion","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar value uniquely associated with a square non-singular matrix, and is usually denoted as |\\mathbf{A}|. The question of whether or not a matrix is nonsingular and therefore invertible is linked to the value of its determinant.\n\nWe can write a generic 2 \\times 2 matrix as:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}\n\nand its determinant is defined as:|\\mathbf{A}| = a_{11}a_{22} - a_{12}a_{21}\n\nFor a 3 \\times 3 matrix, say:\\mathbf{B} = \\begin{bmatrix} b_{11} & b_{12} & b_{13} \\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{bmatrix}\n\nits determinant is:|\\mathbf{B}| = b_{11}(b_{22}b_{33} - b_{23}b_{32}) - b_{12}(b_{21}b_{33} - b_{23}b_{31}) + b_{13}(b_{21}b_{32} - b_{22}b_{31})","type":"content","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#properties-of-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"content":"|\\mathbf{A}| = |\\mathbf{A}'| and |\\mathbf{A}| \\cdot |\\mathbf{B}| = |\\mathbf{B}| \\cdot |\\mathbf{A}|.\n\nInterchanging any two rows or columns will affect the sign of the determinant, but not its absolute value.\n\nMultiplying a single row or column by a scalar will cause the value of the determinant to be multiplied by the scalar.\n\nAddition or subtraction of a nonzero multiple of any row or column to or from another row or column does not change the value of the determinant.\n\nThe determinant of a triangular matrix is equal to the product of the elements along the principal diagonal.\n\nIf any of the rows or columns equal zero, the determinant is also zero.\n\nIf two rows or columns are identical or proportional, i.e. linearly dependent, then the determinant is zero.","type":"content","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#properties-of-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#finding-the-inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix"},"content":"The inverse of a matrix is defined as:\\mathbf{A}^{-1} = \\frac{1}{|\\mathbf{A}|} \\operatorname{adj}(\\mathbf{A})\n\nLet’s say we have the following linear system:2x_1 + 9x_2 + 4x_3 = -13 \\\\\n7x_1 - 8x_2 + 6x_3 = 63 \\\\\n5x_1 + 3x_2 - 6x_3 = 6\n\nWriting the above system in the form \\mathbf{A}\\mathbf{x} = \\mathbf{b}, where:\\mathbf{A} = \\begin{bmatrix} 2 & 9 & 4 \\\\ 7 & -8 & 6 \\\\ 5 & 3 & -6 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nWe can therefore solve the system by \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}, which means that we need the inverse of \\mathbf{A}.\n\nStep 1. The minor |M_{ij}| is the determinant of the submatrix formed by deleting the i-th row and j-th column of the original matrix. So we have:\\begin{bmatrix}\n30 & -72 & 61 \\\\\n-66 & -32 & -39 \\\\\n86 & -16 & -79\n\\end{bmatrix}\n\nStep 2: The cofactor C_{ij} is a minor with the prescribed sign that follows the rule:C_{ij} = (-1)^{i+j} |M_{ij}|\n\nHence, we have:\\begin{bmatrix}\n30 & 72 & 61 \\\\\n66 & -32 & 39 \\\\\n86 & 16 & -79\n\\end{bmatrix}\n\nStep 3: An adjoint matrix is simply the transpose of a cofactor matrix. Continuing the example above, we have:\\operatorname{adj}(\\mathbf{A}) = \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nSince |\\mathbf{A}| = 952, we then have:\\mathbf{A}^{-1} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nand\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix} \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nNot a nice one to solve, though.","type":"content","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#finding-the-inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"content":"Cramer came up with a simplified method for solving a system of linear equations through the use of determinants.\n\nBasically, given \\mathbf{A}\\mathbf{x} = \\mathbf{b}, we have:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n\nDefine:\\Delta = |\\mathbf{A}|\n\nand\\Delta_1 = \\begin{vmatrix} b_1 & a_{12} & a_{13} \\\\ b_2 & a_{22} & a_{23} \\\\ b_3 & a_{32} & a_{33} \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} a_{11} & b_1 & a_{13} \\\\ a_{21} & b_2 & a_{23} \\\\ a_{31} & b_3 & a_{33} \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} a_{11} & a_{12} & b_1 \\\\ a_{21} & a_{22} & b_2 \\\\ a_{31} & a_{32} & b_3 \\end{vmatrix}\n\nwhere the columns in the \\mathbf{A} matrix are replaced one by one by the \\mathbf{b} vector as shown above.\n\nThen:x_i = \\frac{\\Delta_i}{\\Delta}\n\nVerify that you get \\{4.5, -3, 1.25\\}.\n\nLinear algebra 1-5.pptx (17107k) 8 Jan 31, 2018, 7:56 AM v.1Linear algebra 6.pptx (3924k) 8 Feb 5, 2018, 7:15 AM v.1","type":"content","url":"/linear-algebra-basic-1a4903afe0c2fad61d78a0ea0f3b5#cramers-rule","position":19},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-1bb7780ca92ccaff79e7c61f78012","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-1bb7780ca92ccaff79e7c61f78012","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-1bb7780ca92ccaff79e7c61f78012#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If ( \\mathbf{A} = (a_{ij}) ) and ( \\mathbf{B} = (b_{ij}) ) are two ( m \\times n ) matrices of same dimension, then ( \\mathbf{A} + \\mathbf{B} ) is defined as ( (a_{ij} + b_{ij}) ). That is, we add element by element the two matrices.Clearly ( \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A} ). The rule applies to matrix subtraction.\n\nTranspose of a matrix: If ( \\mathbf{A} ) is an ( m \\times n ) matrix, then ( \\mathbf{A}’ ) is the ( n \\times m ) matrix whose rows are the columns of ( \\mathbf{A} ). So ( \\mathbf{A}’ = (a_{ji}) ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\na & c \\\nb & d\n\\end{bmatrix}\n]\n\nNote: ( (\\mathbf{A} + \\mathbf{B})’ = \\mathbf{A}’ + \\mathbf{B}’ ), however ( (\\mathbf{A}\\mathbf{B})’ = \\mathbf{B}‘\\mathbf{A}’ ).\n\nNull matrix: Has all elements as 0. Clearly ( \\mathbf{A} + \\mathbf{0}{m,n} = \\mathbf{0}{m,n} + \\mathbf{A} = \\mathbf{A} ) for all ( m \\times n ) matrices.\n\nScalar multiplication: If ( \\mathbf{A} = (a_{ij}) ) then for any constant ( k ), define ( k\\mathbf{A} = (k a_{ij}) ). That is, multiply each element in ( \\mathbf{A} ) by ( k ).\n\nMatrix multiplication: Say ( \\mathbf{A} ) is ( m \\times n ), and ( \\mathbf{B} ) is ( n \\times p ), then the ( m \\times p ) matrix ( \\mathbf{A}\\mathbf{B} ) is the product of ( \\mathbf{A} ) and ( \\mathbf{B} ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n\n= \\begin{bmatrix}\nae + bg & af + bh \\\nce + dg & cf + dh\n\\end{bmatrix}\n]\n\nand the matrices ( \\mathbf{A} ) and ( \\mathbf{B} ) are conformable.\n\nDiagonal matrix: A square ( n \\times n ) matrix ( \\mathbf{A} ) is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\n\n[\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\n0 & a_{22} & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n]\n\nNote: The elements in the diagonal of ( \\mathbf{A} ) are the same as those in ( \\mathbf{A}’ ).\n\nTrace of a square matrix: If ( \\mathbf{A} ) is a square matrix, the trace of ( \\mathbf{A} ), denoted ( \\operatorname{tr}(\\mathbf{A}) ), is the sum of the elements on the main/leading diagonal of ( \\mathbf{A} ).\n\nIdentity matrix: Denoted by ( \\mathbf{I}n ), the ( n \\times n ) diagonal matrix with ( a{ii} = 1 ) for all ( i ). That is:\n\n[\n\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\n0 & 1 & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n]\n\nSymmetric matrix: A square matrix ( \\mathbf{A} ) is symmetric if ( \\mathbf{A} = \\mathbf{A}’ ). The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-1bb7780ca92ccaff79e7c61f78012#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-1bb7780ca92ccaff79e7c61f78012#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if ( \\mathbf{A}^2 = \\mathbf{A} ). Below is an example (try squaring the matrix and see what you get).\n\n[\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\n]\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors ( \\mathbf{x} ) and ( \\mathbf{y} ) (and ( \\mathbf{z} )) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors ( \\mathbf{x} \\neq \\mathbf{0} ) and ( \\mathbf{y} \\neq \\mathbf{0} ) are said to be linearly independent iff the ONLY solution to ( a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} ) is ( a = 0 ) AND ( b = 0 ). And ( \\mathbf{x} ) and ( \\mathbf{y} ) are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-1bb7780ca92ccaff79e7c61f78012#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-1bb7780ca92ccaff79e7c61f78012#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"content":"Usually ( \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A} ). For example, try ( \\mathbf{A} = \\begin{bmatrix}2 & 0 \\ 3 & 1\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}0 & 1 \\ 0 & 0\\end{bmatrix} ).\n\nWe can have ( \\mathbf{A}\\mathbf{B} = \\mathbf{0} ) even if ( \\mathbf{A} ) or ( \\mathbf{B} ) are not zero. For example, try ( \\mathbf{A} = \\begin{bmatrix}6 & -12 \\ -3 & 6\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}12 & 6 \\ 6 & 3\\end{bmatrix} ).\n\nSay, ( \\mathbf{A} = \\begin{bmatrix}4 & 8 \\ 1 & 2\\end{bmatrix} ), ( \\mathbf{B} = \\begin{bmatrix}2 & 1 \\ 2 & 2\\end{bmatrix} ), and ( \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\ 4 & 2\\end{bmatrix} ). We find that ( \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} ) even though ( \\mathbf{B} \\neq \\mathbf{C} ).\n\nSay ( \\mathbf{A} ) and ( \\mathbf{B} ) are singular matrices. Then ( \\mathbf{A}\\mathbf{B} ) will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-1bb7780ca92ccaff79e7c61f78012#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-1bb7780ca92ccaff79e7c61f78012#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:\n\n[\n3x_1 + 4x_2 = 5 \\\n7x_1 - 2x_2 = 2\n]\n\nHere, we can define:\n\n[\n\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\ 2 \\end{pmatrix}\n]\n\nThen we see that:\n\n[\n\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\ 7x_1 - 2x_2 \\end{pmatrix}\n]\n\nThe original system is in fact equivalent to the matrix form:\n\n[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-1bb7780ca92ccaff79e7c61f78012#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/linear-algebra-basic-278534210994c21e535f926b01e29","position":0},{"hierarchy":{"lvl1":""},"content":"","type":"content","url":"/linear-algebra-basic-278534210994c21e535f926b01e29","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or an interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors can be written asx =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}.\n\nA matrix is a rectangular array of numbers. For example,A =\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}.\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = [a_{ij} + b_{ij}].\n\nA matrix can be multiplied by a scalar c:cA = [c a_{ij}].\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is an m \\times n matrix and B is an n \\times k matrix, then\nAB is an m \\times k matrix.\n\nThe (i,j) element of AB is given by(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of\ntransformations matters.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows\nand columns:(A')_{ij} = a_{ji}.\n\nFor example,\n$$\nA =\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\n\\qquad\nA’ =\\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric ifA = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfiesAI = IA = A.\n\nFor example,\n$$\nI =\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive\nidentity.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such\nthatAA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} yieldsx = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility typically reflects:\n\nredundancy,\n\nlinear dependence,\n\nthe absence of a unique solution.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes key\nproperties of the matrix.\n\nFor a 2 \\times 2 matrix,\n$$\nA =\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}the determinant is given by\n\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume-scaling effect of a linear\ntransformation.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equationsAx = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution isx_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"The columns of a matrix are linearly dependent if one column can be\nexpressed as a linear combination of the others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or\ncolumns.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-29912ac284247e496eed6676c6ab2#what-comes-next","position":33},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"[cite_start]Matrix addition and subtraction: If A=(a_{ij}) and B=(b_{ij}) are two m\\times n matrices of same dimension, then A+B is defined as (a_{ij}+b_{ij})[cite: 28]. [cite_start]That is, we add element by element the two matrices[cite: 29]. [cite_start]Clearly A+B=B+A[cite: 29]. [cite_start]The rule applies to matrix subtraction[cite: 29].\n\n[cite_start]Transpose of a matrix: If A is a m\\times n matrix, the A' is the n\\times m matrix whose rows are the columns of A[cite: 30]. [cite_start]So A^{\\prime}=(a_{ji})[cite: 30]. For example:\n\n[cite_start][\\begin{matrix}2&-3\\\\ -4&5\\end{matrix}]^{\\prime}=[\\begin{matrix}2&-4\\\\ -3&5\\end{matrix}] [cite: 32]\n\n[cite_start]Note: (A+B)^{\\prime}=A^{\\prime}+B^{\\prime} [cite: 31]\n\n[cite_start]Note: (AB)^{\\prime}=B^{\\prime}A^{\\prime} [cite: 33]\n\n[cite_start]Null matrix: Has all elements as 0[cite: 34]. [cite_start]Clearly A+O_{m,n}=0_{m,n}+A=A for all m\\times n matrices[cite: 34].\n\n[cite_start]Scalar multiplication: If A=(a_{ij}) then for any constant k define kA=(ka_{ij})[cite: 35]. [cite_start]That is, multiply each element in A by k[cite: 35].\n\n[cite_start]Matrix multiplication: Say A is m\\times n and B is n\\times p then the m\\times p matrix AB is the product of A and B[cite: 36]. For example:\n\n[cite_start][\\begin{matrix}1&2\\\\ 4&0\\end{matrix}][\\begin{matrix}3&-1\\\\ 4&3\\end{matrix}]=[\\begin{matrix}11&5\\\\ 12&-4\\end{matrix}] [cite: 37]\n\n[cite_start]The matrices A and B are comformable[cite: 38].\n\n[cite_start]Diagonal matrix: A square n\\times n matrix A is diagonal if all entries off the ‘main diagonal’ are zero[cite: 39].\n[cite_start]A=[\\begin{matrix}a_{11}&0&...&0\\\\ 0&a_{22}&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&a_{nn}\\end{matrix}] [cite: 40]\n\n[cite_start]Note: The elements in the diagonal of A are the same as those in A'[cite: 41].\n\n[cite_start]Trace of a square matrix: If A is a square, the trace of A, denoted tr(A), is the sum of the elements on the main/leading diagonal of A[cite: 42, 43].\n\n[cite_start]Identity matrix: Denoted by I_{n}, the n\\times n diagonal matrix with a_{ij}=1 for all i[cite: 44, 46].\n[cite_start]A=[\\begin{matrix}1&0&...&0\\\\ 0&1&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&1\\end{matrix}] [cite: 45]\n\n[cite_start]Symmetric matrix: A square matrix A is symmetric if A=A^{\\prime}[cite: 47]. [cite_start]The identity matrix and the square null matrix are symmetric[cite: 47].\n\n[cite_start]Idempotent matrix: A square matrix is said to be idempotent if A^{2}=A[cite: 48].\n[cite_start]Example: A=[\\begin{matrix}2&-2&-4\\\\ -1&3&4\\\\ 1&-2&-3\\end{matrix}] [cite: 49]\n\n[cite_start]Singular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has a determinant of zero[cite: 50].\n\n[cite_start]Any two (or three) vectors x and y (and z) are linearly dependent iff one can be written as a scalar multiple of the other[cite: 51].\n\n[cite_start]Two vectors x\\ne0 and y\\ne0 are linearly independent iff the only solution to ax+by=0 is a=0 and b=0[cite: 52].\n\n[cite_start]x and y are said to be orthogonal[cite: 53].","type":"content","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"type":"lvl3","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#some-notes-on-matrices","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"content":"[cite_start]Usually AB\\ne BA[cite: 55].\n\n[cite_start]We can have AB=0 even if A or B are not zero[cite: 56].\n\n[cite_start]We can find that AB=AC even though B\\ne C[cite: 58].\n\n[cite_start]If A and B are singular matrices, AB will not be zero, although the product will be a singular matrix[cite: 59].","type":"content","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#some-notes-on-matrices","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#systems-of-equations-in-matrix-form","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"[cite_start]One of the uses of linear algebra in economics is to represent and then solve systems of equations[cite: 61]. For instance:\n[cite_start]3x_{1}+4x_{2}=5 [cite: 68]\n[cite_start]7x_{1}-2x_{2}=2 [cite: 68]\n\n[cite_start]This can be defined as Ax=b[cite: 76], where:\n[cite_start]A = [\\begin{matrix}3&4\\\\ 7&-2\\end{matrix}], x=[\\begin{matrix}x_{1}\\\\ x_{2}\\end{matrix}], and b=[\\begin{matrix}5\\\\ 2\\end{matrix}] [cite: 71, 73, 136]","type":"content","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#systems-of-equations-in-matrix-form","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#matrix-inversion","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"content":"[cite_start]In fact, A^{-1}A=AA^{-1}=I[cite: 94]. [cite_start]To find the inverse of A=[\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}][cite: 79]:\n\n[cite_start]Find a matrix such that [\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}][\\begin{matrix}a&b\\\\ c&d\\end{matrix}]=[\\begin{matrix}10&0\\\\ 0&10\\end{matrix}][cite: 81].\n\n[cite_start]The value 10 is the determinant of A[cite: 85].\n\n[cite_start]The resulting matrix is \\frac{1}{10}[\\begin{matrix}4&2\\\\ -3&1\\end{matrix}][cite: 90].","type":"content","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#matrix-inversion","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"type":"lvl3","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#properties-of-the-inverse","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"content":"[cite_start]Property 1: (A^{-1})^{-1}=A for any nonsingular matrix A[cite: 96, 97].\n\n[cite_start]Property 2: |A^{-1}|=\\frac{1}{|A|}[cite: 101].\n\n[cite_start]Property 3: The inverse of matrix A is unique[cite: 100].\n\n[cite_start]Property 4: (A^{\\prime})^{-1}=(A^{-1})^{\\prime}[cite: 103].\n\n[cite_start]Property 5: If A and B are nonsingular, AB is also nonsingular[cite: 104].\n\n[cite_start]Property 6: (AB)^{-1}=B^{-1}A^{-1}[cite: 106].","type":"content","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#properties-of-the-inverse","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"content":"[cite_start]The determinant is a scalar-value associated with a square non-singular matrix, denoted as |A|[cite: 108].\n\n[cite_start]For a 2\\times2 matrix A=[\\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix}], |A|=a_{11}a_{22}-a_{12}a_{21}[cite: 113].\n\n[cite_start]The Laplace expansion is often used for higher orders[cite: 116].","type":"content","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#notes-on-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"content":"[cite_start]|A|=|A^{\\prime}| and |A|\\cdot|B|=|B|\\cdot|A|[cite: 122].\n\n[cite_start]Interchanging any two rows or columns changes the sign[cite: 123].\n\n[cite_start]Multiplying a single row/column by a scalar k multiplies the determinant by k[cite: 124].\n\n[cite_start]Adding a multiple of one row/column to another does not change the determinant[cite: 125].\n\n[cite_start]The determinant of a triangular matrix is the product of the principal diagonal elements[cite: 126].\n\n[cite_start]If any row or column equals zero, the determinant is zero[cite: 127].\n\n[cite_start]If two rows or columns are linearly dependent, the determinant is zero[cite: 128].","type":"content","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#notes-on-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"type":"lvl2","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#finding-the-inverse-of-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"content":"[cite_start]The inverse is defined as: A^{-1}=\\frac{1}{|A|}adjA[cite: 132].\n\n[cite_start]Step 1: Find the minor |M_{ij}|, the determinant of the submatrix after deleting the ith row and jth column[cite: 138].\n\n[cite_start]Step 2: The cofactor |C_{ij}|=(-1)^{i+j}|M_{ij}|[cite: 141, 143].\n\n[cite_start]Step 3: The adjoint matrix is the transpose of the cofactor matrix[cite: 144].","type":"content","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#finding-the-inverse-of-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"content":"[cite_start]A simplified method for solving Ax=b using determinants[cite: 151, 152].\n[cite_start]x_{i}=\\frac{|\\Delta_{i}|}{|\\Delta|} [cite: 158]\n[cite_start]Where \\Delta_{i} is the matrix A with the ith column replaced by vector b[cite: 157].","type":"content","url":"/linear-algebra-basic-2add610790f83eea4303c702ab2e2#cramers-rule","position":19},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"[cite_start]Matrix addition and subtraction: If A=(a_{ij}) and B=(b_{ij}) are two m\\times n matrices of same dimension, then A+B is defined as (a_{ij}+b_{ij})[cite: 28]. [cite_start]That is, we add element by element the two matrices[cite: 29]. [cite_start]Clearly A+B=B+A[cite: 29]. [cite_start]The rule applies to matrix subtraction[cite: 29].\n\n[cite_start]Transpose of a matrix: If A is a m\\times n matrix, the A' is the n\\times m matrix whose rows are the columns of A[cite: 30]. [cite_start]So A^{\\prime}=(a_{ji})[cite: 30]. For example:\n\n[cite_start][\\begin{matrix}2&-3\\\\ -4&5\\end{matrix}]^{\\prime}=[\\begin{matrix}2&-4\\\\ -3&5\\end{matrix}] [cite: 32]\n\n[cite_start]Note: (A+B)^{\\prime}=A^{\\prime}+B^{\\prime} [cite: 31]\n\n[cite_start]Note: (AB)^{\\prime}=B^{\\prime}A^{\\prime} [cite: 33]\n\n[cite_start]Null matrix: Has all elements as 0[cite: 34]. [cite_start]Clearly A+O_{m,n}=0_{m,n}+A=A for all m\\times n matrices[cite: 34].\n\n[cite_start]Scalar multiplication: If A=(a_{ij}) then for any constant k define kA=(ka_{ij})[cite: 35]. [cite_start]That is, multiply each element in A by k[cite: 35].\n\n[cite_start]Matrix multiplication: Say A is m\\times n and B is n\\times p then the m\\times p matrix AB is the product of A and B[cite: 36]. For example:\n\n[cite_start][\\begin{matrix}1&2\\\\ 4&0\\end{matrix}][\\begin{matrix}3&-1\\\\ 4&3\\end{matrix}]=[\\begin{matrix}11&5\\\\ 12&-4\\end{matrix}] [cite: 37]\n\n[cite_start]The matrices A and B are comformable[cite: 38].\n\n[cite_start]Diagonal matrix: A square n\\times n matrix A is diagonal if all entries off the ‘main diagonal’ are zero[cite: 39].\n[cite_start]A=[\\begin{matrix}a_{11}&0&...&0\\\\ 0&a_{22}&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&a_{nn}\\end{matrix}] [cite: 40]\n\n[cite_start]Note: The elements in the diagonal of A are the same as those in A'[cite: 41].\n\n[cite_start]Trace of a square matrix: If A is a square, the trace of A, denoted tr(A), is the sum of the elements on the main/leading diagonal of A[cite: 42, 43].\n\n[cite_start]Identity matrix: Denoted by I_{n}, the n\\times n diagonal matrix with a_{ij}=1 for all i[cite: 44, 46].\n[cite_start]A=[\\begin{matrix}1&0&...&0\\\\ 0&1&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&1\\end{matrix}] [cite: 45]\n\n[cite_start]Symmetric matrix: A square matrix A is symmetric if A=A^{\\prime}[cite: 47]. [cite_start]The identity matrix and the square null matrix are symmetric[cite: 47].\n\n[cite_start]Idempotent matrix: A square matrix is said to be idempotent if A^{2}=A[cite: 48].\n[cite_start]Example: A=[\\begin{matrix}2&-2&-4\\\\ -1&3&4\\\\ 1&-2&-3\\end{matrix}] [cite: 49]\n\n[cite_start]Singular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has a determinant of zero[cite: 50].\n\n[cite_start]Any two (or three) vectors x and y (and z) are linearly dependent iff one can be written as a scalar multiple of the other[cite: 51].\n\n[cite_start]Two vectors x\\ne0 and y\\ne0 are linearly independent iff the only solution to ax+by=0 is a=0 and b=0[cite: 52].\n\n[cite_start]x and y are said to be orthogonal[cite: 53].","type":"content","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"type":"lvl3","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#some-notes-on-matrices","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"content":"[cite_start]Usually AB\\ne BA[cite: 55].\n\n[cite_start]We can have AB=0 even if A or B are not zero[cite: 56].\n\n[cite_start]We can find that AB=AC even though B\\ne C[cite: 58].\n\n[cite_start]If A and B are singular matrices, AB will not be zero, although the product will be a singular matrix[cite: 59].","type":"content","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#some-notes-on-matrices","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#systems-of-equations-in-matrix-form","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"[cite_start]One of the uses of linear algebra in economics is to represent and then solve systems of equations[cite: 61]. For instance:\n[cite_start]3x_{1}+4x_{2}=5 [cite: 68]\n[cite_start]7x_{1}-2x_{2}=2 [cite: 68]\n\n[cite_start]This can be defined as Ax=b[cite: 76], where:\n[cite_start]A = [\\begin{matrix}3&4\\\\ 7&-2\\end{matrix}], x=[\\begin{matrix}x_{1}\\\\ x_{2}\\end{matrix}], and b=[\\begin{matrix}5\\\\ 2\\end{matrix}] [cite: 71, 73, 136]","type":"content","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#systems-of-equations-in-matrix-form","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#matrix-inversion","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"content":"[cite_start]In fact, A^{-1}A=AA^{-1}=I[cite: 94]. [cite_start]To find the inverse of A=[\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}][cite: 79]:\n\n[cite_start]Find a matrix such that [\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}][\\begin{matrix}a&b\\\\ c&d\\end{matrix}]=[\\begin{matrix}10&0\\\\ 0&10\\end{matrix}][cite: 81].\n\n[cite_start]The value 10 is the determinant of A[cite: 85].\n\n[cite_start]The resulting matrix is \\frac{1}{10}[\\begin{matrix}4&2\\\\ -3&1\\end{matrix}][cite: 90].","type":"content","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#matrix-inversion","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"type":"lvl3","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#properties-of-the-inverse","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"content":"[cite_start]Property 1: (A^{-1})^{-1}=A for any nonsingular matrix A[cite: 96, 97].\n\n[cite_start]Property 2: |A^{-1}|=\\frac{1}{|A|}[cite: 101].\n\n[cite_start]Property 3: The inverse of matrix A is unique[cite: 100].\n\n[cite_start]Property 4: (A^{\\prime})^{-1}=(A^{-1})^{\\prime}[cite: 103].\n\n[cite_start]Property 5: If A and B are nonsingular, AB is also nonsingular[cite: 104].\n\n[cite_start]Property 6: (AB)^{-1}=B^{-1}A^{-1}[cite: 106].","type":"content","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#properties-of-the-inverse","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"content":"[cite_start]The determinant is a scalar-value associated with a square non-singular matrix, denoted as |A|[cite: 108].\n\n[cite_start]For a 2\\times2 matrix A=[\\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix}], |A|=a_{11}a_{22}-a_{12}a_{21}[cite: 113].\n\n[cite_start]The Laplace expansion is often used for higher orders[cite: 116].","type":"content","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#notes-on-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"content":"[cite_start]|A|=|A^{\\prime}| and |A|\\cdot|B|=|B|\\cdot|A|[cite: 122].\n\n[cite_start]Interchanging any two rows or columns changes the sign[cite: 123].\n\n[cite_start]Multiplying a single row/column by a scalar k multiplies the determinant by k[cite: 124].\n\n[cite_start]Adding a multiple of one row/column to another does not change the determinant[cite: 125].\n\n[cite_start]The determinant of a triangular matrix is the product of the principal diagonal elements[cite: 126].\n\n[cite_start]If any row or column equals zero, the determinant is zero[cite: 127].\n\n[cite_start]If two rows or columns are linearly dependent, the determinant is zero[cite: 128].","type":"content","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#notes-on-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"type":"lvl2","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#finding-the-inverse-of-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"content":"[cite_start]The inverse is defined as: A^{-1}=\\frac{1}{|A|}adjA[cite: 132].\n\n[cite_start]Step 1: Find the minor |M_{ij}|, the determinant of the submatrix after deleting the ith row and jth column[cite: 138].\n\n[cite_start]Step 2: The cofactor |C_{ij}|=(-1)^{i+j}|M_{ij}|[cite: 141, 143].\n\n[cite_start]Step 3: The adjoint matrix is the transpose of the cofactor matrix[cite: 144].","type":"content","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#finding-the-inverse-of-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"content":"[cite_start]A simplified method for solving Ax=b using determinants[cite: 151, 152].\n[cite_start]x_{i}=\\frac{|\\Delta_{i}|}{|\\Delta|} [cite: 158]\n[cite_start]Where \\Delta_{i} is the matrix A with the ith column replaced by vector b[cite: 157].","type":"content","url":"/linear-algebra-basic-2cf4a7d1872569648ad57a32a1dc6#cramers-rule","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or an interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors can be written as\n$$\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\n$$\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = [a_{ij} + b_{ij}].\n\nA matrix can be multiplied by a scalar c:cA = [c a_{ij}].\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is an m \\times n matrix and B is an n \\times k matrix, then\nAB is an m \\times k matrix.\n\nThe (i,j) element of AB is given by(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of\ntransformations matters.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows\nand columns:(A')_{ij} = a_{ji}.\n\nFor example,\n$$\nA =\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\n\\qquad\nA’ =\\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric ifA = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfiesAI = IA = A.\n\nFor example,\n$$\nI =\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive\nidentity.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such\nthatAA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} yieldsx = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility typically reflects:\n\nredundancy,\n\nlinear dependence,\n\nthe absence of a unique solution.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes key\nproperties of the matrix.\n\nFor a 2 \\times 2 matrix,\n$$\nA =\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}the determinant is given by\n\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume-scaling effect of a linear\ntransformation.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equationsAx = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution isx_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"The columns of a matrix are linearly dependent if one column can be\nexpressed as a linear combination of the others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or\ncolumns.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-3b8db11d7455c268c2f43a68b2315#what-comes-next","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors:\n$$\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\n$$\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = (a_{ij} + b_{ij}).\n\nA matrix can be multiplied by a scalar c:cA = (c a_{ij}).\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is m \\times n and B is n \\times k, then AB is m \\times k.\n\nThe (i,j) element of AB is:(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of transformations matters.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows and columns:(A')_{ij} = a_{ji}.\n\nExample:\n$$\nA =\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\n\\quad\nA’ =\\begin{pmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{pmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric if:A = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfies:AI = IA = A.\n\nFor example:\n$$\nI =\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive identity.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such that:AA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} gives:x = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility usually reflects:\n\nredundancy,\n\nlinear dependence,\n\nlack of unique solutions.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes\nkey properties of the matrix.\n\nFor a 2 \\times 2 matrix:\n$$\nA =\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\n\\quad\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume scaling effect of a linear transformation.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equations:Ax = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution is:x_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems, but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"Columns of a matrix are linearly dependent if one column can be written as\na linear combination of others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or columns.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-3c13cc7c616ce5bdeeff3dec5dbc3#what-comes-next","position":33},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-3c7229a3411f6d2ccd9b832018d70","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-3c7229a3411f6d2ccd9b832018d70","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-3c7229a3411f6d2ccd9b832018d70#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If ( \\mathbf{A} = (a_{ij}) ) and ( \\mathbf{B} = (b_{ij}) ) are two ( m \\times n ) matrices of same dimension, then ( \\mathbf{A} + \\mathbf{B} ) is defined as ( (a_{ij} + b_{ij}) ). That is, we add element by element the two matrices. Clearly ( \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A} ). The rule applies to matrix subtraction.\n\nTranspose of a matrix: If ( \\mathbf{A} ) is an ( m \\times n ) matrix, then ( \\mathbf{A}’ ) is the ( n \\times m ) matrix whose rows are the columns of ( \\mathbf{A} ). So ( \\mathbf{A}’ = (a_{ji}) ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\na & c \\\nb & d\n\\end{bmatrix}\n]\n\nNote: ( (\\mathbf{A} + \\mathbf{B})’ = \\mathbf{A}’ + \\mathbf{B}’ ), however ( (\\mathbf{A}\\mathbf{B})’ = \\mathbf{B}‘\\mathbf{A}’ ).\n\nNull matrix: Has all elements as 0. Clearly ( \\mathbf{A} + \\mathbf{0}{m,n} = \\mathbf{0}{m,n} + \\mathbf{A} = \\mathbf{A} ) for all ( m \\times n ) matrices.\n\nScalar multiplication: If ( \\mathbf{A} = (a_{ij}) ) then for any constant ( k ), define ( k\\mathbf{A} = (k a_{ij}) ). That is, multiply each element in ( \\mathbf{A} ) by ( k ).\n\nMatrix multiplication: Say ( \\mathbf{A} ) is ( m \\times n ), and ( \\mathbf{B} ) is ( n \\times p ), then the ( m \\times p ) matrix ( \\mathbf{A}\\mathbf{B} ) is the product of ( \\mathbf{A} ) and ( \\mathbf{B} ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n\n= \\begin{bmatrix}\nae + bg & af + bh \\\nce + dg & cf + dh\n\\end{bmatrix}\n]\n\nand the matrices ( \\mathbf{A} ) and ( \\mathbf{B} ) are conformable.\n\nDiagonal matrix: A square ( n \\times n ) matrix ( \\mathbf{A} ) is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\n\n[\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\n0 & a_{22} & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n]\n\nNote: The elements in the diagonal of ( \\mathbf{A} ) are the same as those in ( \\mathbf{A}’ ).\n\nTrace of a square matrix: If ( \\mathbf{A} ) is a square matrix, the trace of ( \\mathbf{A} ), denoted ( \\operatorname{tr}(\\mathbf{A}) ), is the sum of the elements on the main/leading diagonal of ( \\mathbf{A} ).\n\nIdentity matrix: Denoted by ( \\mathbf{I}n ), the ( n \\times n ) diagonal matrix with ( a{ii} = 1 ) for all ( i ). That is:\n\n[\n\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\n0 & 1 & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n]\n\nSymmetric matrix: A square matrix ( \\mathbf{A} ) is symmetric if ( \\mathbf{A} = \\mathbf{A}’ ). The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-3c7229a3411f6d2ccd9b832018d70#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-3c7229a3411f6d2ccd9b832018d70#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if ( \\mathbf{A}^2 = \\mathbf{A} ). Below is an example (try squaring the matrix and see what you get).\n\n[\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\n]\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors ( \\mathbf{x} ) and ( \\mathbf{y} ) (and ( \\mathbf{z} )) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors ( \\mathbf{x} \\neq \\mathbf{0} ) and ( \\mathbf{y} \\neq \\mathbf{0} ) are said to be linearly independent iff the ONLY solution to ( a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} ) is ( a = 0 ) AND ( b = 0 ). And ( \\mathbf{x} ) and ( \\mathbf{y} ) are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-3c7229a3411f6d2ccd9b832018d70#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-3c7229a3411f6d2ccd9b832018d70#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"content":"Usually ( \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A} ). For example, try ( \\mathbf{A} = \\begin{bmatrix}2 & 0 \\ 3 & 1\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}0 & 1 \\ 0 & 0\\end{bmatrix} ).\n\nWe can have ( \\mathbf{A}\\mathbf{B} = \\mathbf{0} ) even if ( \\mathbf{A} ) or ( \\mathbf{B} ) are not zero. For example, try ( \\mathbf{A} = \\begin{bmatrix}6 & -12 \\ -3 & 6\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}12 & 6 \\ 6 & 3\\end{bmatrix} ).\n\nSay, ( \\mathbf{A} = \\begin{bmatrix}4 & 8 \\ 1 & 2\\end{bmatrix} ), ( \\mathbf{B} = \\begin{bmatrix}2 & 1 \\ 2 & 2\\end{bmatrix} ), and ( \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\ 4 & 2\\end{bmatrix} ). We find that ( \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} ) even though ( \\mathbf{B} \\neq \\mathbf{C} ).\n\nSay ( \\mathbf{A} ) and ( \\mathbf{B} ) are singular matrices. Then ( \\mathbf{A}\\mathbf{B} ) will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-3c7229a3411f6d2ccd9b832018d70#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-3c7229a3411f6d2ccd9b832018d70#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:\n\n[\n3x_1 + 4x_2 = 5 \\\n7x_1 - 2x_2 = 2\n]\n\nHere, we can define:\n\n[\n\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\ 2 \\end{pmatrix}\n]\n\nThen we see that:\n\n[\n\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\ 7x_1 - 2x_2 \\end{pmatrix}\n]\n\nThe original system is in fact equivalent to the matrix form:\n\n[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-3c7229a3411f6d2ccd9b832018d70#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or an interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors can be written asx =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}.\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = [a_{ij} + b_{ij}].\n\nA matrix can be multiplied by a scalar c:cA = [c a_{ij}].\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is an m \\times n matrix and B is an n \\times k matrix, then\nAB is an m \\times k matrix.\n\nThe (i,j) element of AB is given by(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of\ntransformations matters.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows\nand columns:(A')_{ij} = a_{ji}.\n\nFor example,\n$$\nA =\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\n\\qquad\nA’ =\\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric ifA = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfiesAI = IA = A.\n\nFor example,\n$$\nI =\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive\nidentity.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such\nthatAA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} yieldsx = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility typically reflects:\n\nredundancy,\n\nlinear dependence,\n\nthe absence of a unique solution.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes key\nproperties of the matrix.\n\nFor a 2 \\times 2 matrix,\n$$\nA =\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}the determinant is given by\n\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume-scaling effect of a linear\ntransformation.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equationsAx = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution isx_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"The columns of a matrix are linearly dependent if one column can be\nexpressed as a linear combination of the others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or\ncolumns.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-4033767d61a1a17c9b6edc2d35a0c#what-comes-next","position":33},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If A=(a_{ij}) and B=(b_{ij}) are two m\\times n matrices of same dimension, then A+B is defined as (a_{ij}+b_{ij}). That is, we add element by element the two matrices. Clearly A+B=B+A. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If A is a m\\times n matrix, then A' is the n\\times m matrix whose rows are the columns of A. So A^{\\prime}=(a_{ji}).\n\nExample and Notes:\n\n[\\begin{matrix}2&-3\\\\ -4&5\\end{matrix}]^{\\prime}=[\\begin{matrix}2&-4\\\\ -3&5\\end{matrix}]\n\n(A+B)^{\\prime}=A^{\\prime}+B^{\\prime}\n\n(AB)^{\\prime}=B^{\\prime}A^{\\prime}\n\nNull matrix: Has all elements as 0. Clearly A+O_{m,n}=0_{m,n}+A=A for all m\\times n matrices.\n\nScalar multiplication: If A=(a_{ij}) then for any constant k define kA=(ka_{ij}). That is, multiply each element in A by k.\n\nMatrix multiplication: Say A is m\\times n and B is n\\times p then the m\\times p matrix AB is the product of A and B. For example:\n\n[\\begin{matrix}1&2\\\\ 4&0\\end{matrix}][\\begin{matrix}3&-1\\\\ 4&3\\end{string}]=[\\begin{matrix}11&5\\\\ 12&-4\\end{matrix}]\n\nThe matrices A and B are comformable.\n\nDiagonal matrix: A square n\\times n matrix A is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\nA=[\\begin{matrix}a_{11}&0&...&0\\\\ 0&a_{22}&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&a_{nn}\\end{matrix}]\nNote: The elements in the diagonal of A are the same as those in A'.\n\nTrace of a square matrix: If A is a square, the trace of A, denoted tr(A), is the sum of the elements on the main/leading diagonal of A.\n\nIdentity matrix: Denoted by I_{n}, the n\\times n diagonal matrix with a_{ij}=1 for all i. That is:\nA=[\\begin{matrix}1&0&...&0\\\\ 0&1&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&1\\end{matrix}]\n\nSymmetric matrix: A square matrix A is symmetric if A=A^{\\prime}. The identity matrix and the square null matrix are symmetric.\n\nIdempotent matrix: A square matrix is said to be idempotent if A^{2}=A.\nExample: A=[\\begin{matrix}2&-2&-4\\\\ -1&3&4\\\\ 1&-2&-3\\end{matrix}]\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has a determinant of zero.\n\nNote that any two (or three) vectors x and y (and z) are said to be linearly dependent iff one can be written as a scalar multiple of the other.\n\nTwo vectors x\\ne0 and y\\ne0 are said to be LINEARLY INDEPENDENT iff the ONLY solution to ax+by=0 is a=0 AND b=0. And x and y are said to be ORTHOGONAL.","type":"content","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"type":"lvl3","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#some-notes-on-matrices","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"content":"Usually AB\\ne BA.\n\nWe can have AB=0 even if A or B are not zero.\n\nWe find that AB=AC even though B\\ne C.\n\nSay A and B are singular matrices. Then AB will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#some-notes-on-matrices","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#systems-of-equations-in-matrix-form","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system:\n3x_{1}+4x_{2}=5\n7x_{1}-2x_{2}=2\n\nThis is equivalent to the matrix form Ax=b where:\nA = [\\begin{matrix}3&4\\\\ 7&-2\\end{matrix}], x=[\\begin{matrix}x_{1}\\\\ x_{2}\\end{matrix}], and b=[\\begin{matrix}5\\\\ 2\\end{matrix}]","type":"content","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#systems-of-equations-in-matrix-form","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#matrix-inversion","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"content":"In fact, A^{-1}A=AA^{-1}=I. To find the inverse of A=[\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}]:\n\nThink of another matrix such that [\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}][\\begin{matrix}a&b\\\\ c&d\\end{matrix}]=[\\begin{matrix}10&0\\\\ 0&10\\end{matrix}].\n\nThe 10 comes from the determinant of A.\n\nThe matrix \\frac{1}{10}[\\begin{matrix}4&2\\\\ -3&1\\end{matrix}] is the inverse of A.","type":"content","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#matrix-inversion","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"type":"lvl3","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#properties-of-the-inverse","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"content":"Property 1: For any nonsingular matrix A, (A^{-1})^{-1}=A.\n\nProperty 2: The determinant of the inverse is the reciprocal of the determinant: |A^{-1}|=\\frac{1}{|A|}.\n\nProperty 3: The inverse of matrix A is unique.\n\nProperty 4: The inverse of the transpose is equal to the transpose of the inverse: (A^{\\prime})^{-1}=(A^{-1})^{\\prime}.\n\nProperty 5: If A and B are nonsingular, AB is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices: (AB)^{-1}=B^{-1}A^{-1}.","type":"content","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#properties-of-the-inverse","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar-value uniquely associated with a square non-singular matrix, denoted as |A|.\n\nFor a 2\\times2 matrix A=[\\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix}], the determinant is defined as |A|=a_{11}a_{22}-a_{12}a_{21}.\n\nFor a 3\\times3 matrix, the Laplace expansion is often used.","type":"content","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#notes-on-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"content":"|A|=|A^{\\prime}| and |A|\\cdot|B|=|B|\\cdot|A|.\n\nInterchanging any two rows or columns changes the sign of the determinant.\n\nMultiplying a single row or column by a scalar multiplies the determinant by that scalar.\n\nAddition or subtraction of a nonzero multiple of a row/column to another does not change the determinant.\n\nThe determinant of a triangular matrix is the product of the diagonal elements.\n\nIf any rows or columns equal zero, the determinant is zero.\n\nIf two rows or columns are linearly dependent, the determinant is zero.","type":"content","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#notes-on-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"type":"lvl2","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#finding-the-inverse-of-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"content":"Defined as: A^{-1}=\\frac{1}{|A|}adjA.\n\nStep 1: The minor |M_{ij}| is the determinant of the submatrix formed by deleting the ith row and jth column.\nStep 2: The cofactor |C_{ij}|=(-1)^{i+j}|M_{ij}|.\nStep 3: The adjoint matrix is the transpose of the cofactor matrix.","type":"content","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#finding-the-inverse-of-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"content":"A simplified method for solving Ax=b through determinants.\nx_{i}=\\frac{|\\Delta_{i}|}{|\\Delta|}\nWhere \\Delta_{i} is the matrix A with the ith column replaced by the b vector.","type":"content","url":"/linear-algebra-basic-458d396dbfb3a87f9fbe2781c5bbc#cramers-rule","position":19},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-5048351aff3d2b2f23432c69e0c57","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-5048351aff3d2b2f23432c69e0c57","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-5048351aff3d2b2f23432c69e0c57#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If ( \\mathbf{A} = (a_{ij}) ) and ( \\mathbf{B} = (b_{ij}) ) are two ( m \\times n ) matrices of same dimension, then ( \\mathbf{A} + \\mathbf{B} ) is defined as ( (a_{ij} + b_{ij}) ). That is, we add element by element the two matrices.Clearly ( \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A} ). The rule applies to matrix subtraction.\n\nTranspose of a matrix: If ( \\mathbf{A} ) is an ( m \\times n ) matrix, then ( \\mathbf{A}’ ) is the ( n \\times m ) matrix whose rows are the columns of ( \\mathbf{A} ). So ( \\mathbf{A}’ = (a_{ji}) ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\na & c \\\nb & d\n\\end{bmatrix}\n]\n\nNote: ( (\\mathbf{A} + \\mathbf{B})’ = \\mathbf{A}’ + \\mathbf{B}’ ), however ( (\\mathbf{A}\\mathbf{B})’ = \\mathbf{B}‘\\mathbf{A}’ ).\n\nNull matrix: Has all elements as 0. Clearly ( \\mathbf{A} + \\mathbf{0}{m,n} = \\mathbf{0}{m,n} + \\mathbf{A} = \\mathbf{A} ) for all ( m \\times n ) matrices.\n\nScalar multiplication: If ( \\mathbf{A} = (a_{ij}) ) then for any constant ( k ), define ( k\\mathbf{A} = (k a_{ij}) ). That is, multiply each element in ( \\mathbf{A} ) by ( k ).\n\nMatrix multiplication: Say ( \\mathbf{A} ) is ( m \\times n ), and ( \\mathbf{B} ) is ( n \\times p ), then the ( m \\times p ) matrix ( \\mathbf{A}\\mathbf{B} ) is the product of ( \\mathbf{A} ) and ( \\mathbf{B} ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n\n= \\begin{bmatrix}\nae + bg & af + bh \\\nce + dg & cf + dh\n\\end{bmatrix}\n]\n\nand the matrices ( \\mathbf{A} ) and ( \\mathbf{B} ) are conformable.\n\nDiagonal matrix: A square ( n \\times n ) matrix ( \\mathbf{A} ) is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\n\n[\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\n0 & a_{22} & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n]\n\nNote: The elements in the diagonal of ( \\mathbf{A} ) are the same as those in ( \\mathbf{A}’ ).\n\nTrace of a square matrix: If ( \\mathbf{A} ) is a square matrix, the trace of ( \\mathbf{A} ), denoted ( \\operatorname{tr}(\\mathbf{A}) ), is the sum of the elements on the main/leading diagonal of ( \\mathbf{A} ).\n\nIdentity matrix: Denoted by ( \\mathbf{I}n ), the ( n \\times n ) diagonal matrix with ( a{ii} = 1 ) for all ( i ). That is:\n\n[\n\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\n0 & 1 & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n]\n\nSymmetric matrix: A square matrix ( \\mathbf{A} ) is symmetric if ( \\mathbf{A} = \\mathbf{A}’ ). The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-5048351aff3d2b2f23432c69e0c57#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-5048351aff3d2b2f23432c69e0c57#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if ( \\mathbf{A}^2 = \\mathbf{A} ). Below is an example (try squaring the matrix and see what you get).\n\n[\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\n]\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors ( \\mathbf{x} ) and ( \\mathbf{y} ) (and ( \\mathbf{z} )) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors ( \\mathbf{x} \\neq \\mathbf{0} ) and ( \\mathbf{y} \\neq \\mathbf{0} ) are said to be linearly independent iff the ONLY solution to ( a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} ) is ( a = 0 ) AND ( b = 0 ). And ( \\mathbf{x} ) and ( \\mathbf{y} ) are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-5048351aff3d2b2f23432c69e0c57#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-5048351aff3d2b2f23432c69e0c57#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"content":"Usually ( \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A} ). For example, try ( \\mathbf{A} = \\begin{bmatrix}2 & 0 \\ 3 & 1\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}0 & 1 \\ 0 & 0\\end{bmatrix} ).\n\nWe can have ( \\mathbf{A}\\mathbf{B} = \\mathbf{0} ) even if ( \\mathbf{A} ) or ( \\mathbf{B} ) are not zero. For example, try ( \\mathbf{A} = \\begin{bmatrix}6 & -12 \\ -3 & 6\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}12 & 6 \\ 6 & 3\\end{bmatrix} ).\n\nSay, ( \\mathbf{A} = \\begin{bmatrix}4 & 8 \\ 1 & 2\\end{bmatrix} ), ( \\mathbf{B} = \\begin{bmatrix}2 & 1 \\ 2 & 2\\end{bmatrix} ), and ( \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\ 4 & 2\\end{bmatrix} ). We find that ( \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} ) even though ( \\mathbf{B} \\neq \\mathbf{C} ).\n\nSay ( \\mathbf{A} ) and ( \\mathbf{B} ) are singular matrices. Then ( \\mathbf{A}\\mathbf{B} ) will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-5048351aff3d2b2f23432c69e0c57#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-5048351aff3d2b2f23432c69e0c57#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:\n\n[\n3x_1 + 4x_2 = 5 \\\n7x_1 - 2x_2 = 2\n]\n\nHere, we can define:\n\n[\n\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\ 2 \\end{pmatrix}\n]\n\nThen we see that:\n\n[\n\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\ 7x_1 - 2x_2 \\end{pmatrix}\n]\n\nThe original system is in fact equivalent to the matrix form:\n\n[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-5048351aff3d2b2f23432c69e0c57#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors:\n$$\nx =\\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{pmatrix}\n\n$$\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = (a_{ij} + b_{ij}).\n\nA matrix can be multiplied by a scalar c:cA = (c a_{ij}).\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is m \\times n and B is n \\times k, then AB is m \\times k.\n\nThe (i,j) element of AB is:(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of transformations matters.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows and columns:(A')_{ij} = a_{ji}.\n\nExample:\n$$\nA =\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix}\n\n\\quad\nA’ =\\begin{pmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{pmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric if:A = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfies:AI = IA = A.\n\nFor example:\n$$\nI =\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive identity.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such that:AA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} gives:x = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility usually reflects:\n\nredundancy,\n\nlinear dependence,\n\nlack of unique solutions.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes\nkey properties of the matrix.\n\nFor a 2 \\times 2 matrix:\n$$\nA =\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\n\\quad\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume scaling effect of a linear transformation.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equations:Ax = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution is:x_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems, but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"Columns of a matrix are linearly dependent if one column can be written as\na linear combination of others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or columns.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-6961a60340496a693434b6eb51587#what-comes-next","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors:\n$$\nx =\\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{pmatrix}\n\n$$\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = (a_{ij} + b_{ij}).\n\nA matrix can be multiplied by a scalar c:cA = (c a_{ij}).\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is m \\times n and B is n \\times k, then AB is m \\times k.\n\nThe (i,j) element of AB is:(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of transformations matters.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows and columns:(A')_{ij} = a_{ji}.\n\nExample:\n$$\nA =\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix}\n\n\\quad\nA’ =\\begin{pmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{pmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric if:A = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfies:AI = IA = A.\n\nFor example:\n$$\nI =\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive identity.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such that:AA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} gives:x = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility usually reflects:\n\nredundancy,\n\nlinear dependence,\n\nlack of unique solutions.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes\nkey properties of the matrix.\n\nFor a 2 \\times 2 matrix:\n$$\nA =\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\n\\quad\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume scaling effect of a linear transformation.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equations:Ax = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution is:x_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems, but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"Columns of a matrix are linearly dependent if one column can be written as\na linear combination of others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or columns.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-6a3d42869e0c91f5786d848773c8c#what-comes-next","position":33},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-6cec187d1b19d2f7044706be6bc77","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-6cec187d1b19d2f7044706be6bc77","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-6cec187d1b19d2f7044706be6bc77#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}' = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-6cec187d1b19d2f7044706be6bc77#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-6cec187d1b19d2f7044706be6bc77#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-6cec187d1b19d2f7044706be6bc77#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-6cec187d1b19d2f7044706be6bc77#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-6cec187d1b19d2f7044706be6bc77#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-6cec187d1b19d2f7044706be6bc77#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:3x_1 + 4x_2 = 5 \\\\\n7x_1 - 2x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\\\ 7x_1 - 2x_2 \\end{pmatrix}\n\nThe original system is in fact equivalent to the matrix form:\n\n$$\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-6cec187d1b19d2f7044706be6bc77#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}' = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:3x_1 + 4x_2 = 5 \\\\\n7x_1 - 2x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\\\ 7x_1 - 2x_2 \\end{pmatrix}\n\nThe original system is in fact equivalent to the matrix form:\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#matrix-inversion","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"content":"To better understand the mechanics of finding the inverse of a square matrix, let’s begin by looking at an example.\\mathbf{A} = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nAssume we have:\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix}\n\nWhere does the 10 in the matrix after the = sign come from? It is the determinant of \\mathbf{A}.\n\nNow we have:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 10 & 0 \\\\ 0 & 10 \\end{bmatrix} = 10 \\mathbf{I}\n\nHence, (pre-)multiplying both sides of the equation by \\frac{1}{10} will give you:\\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\mathbf{I}\n\nThe matrix \\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} is in fact the inverse of \\mathbf{A}. Hence we have the relationship \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}. In fact, \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.\n\nHere are some properties of the inverse of a matrix worth knowing:\n\nProperty 1: For any nonsingular matrix \\mathbf{A}, (\\mathbf{A}^{-1})^{-1} = \\mathbf{A}.\n\nProperty 2: The determinant of the inverse of a matrix is equal to the reciprocal of the determinant of the matrix. That is |\\mathbf{A}^{-1}| = \\frac{1}{|\\mathbf{A}|}.\n\nProperty 3: The inverse of matrix \\mathbf{A} is unique.\n\nProperty 4: For any nonsingular matrix \\mathbf{A}, the inverse of the transpose of a matrix is equal to the transpose of the inverse of the matrix. (\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'.\n\nProperty 5: If \\mathbf{A} and \\mathbf{B} are nonsingular and of the same dimension, then \\mathbf{A}\\mathbf{B} is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices is equal to the product of their inverses in reverse order. (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}.","type":"content","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#matrix-inversion","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar value uniquely associated with a square non-singular matrix, and is usually denoted as |\\mathbf{A}|. The question of whether or not a matrix is nonsingular and therefore invertible is linked to the value of its determinant.\n\nWe can write a generic 2 \\times 2 matrix as:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}\n\nand its determinant is defined as:|\\mathbf{A}| = a_{11}a_{22} - a_{12}a_{21}\n\nFor a 3 \\times 3 matrix, say:\\mathbf{B} = \\begin{bmatrix} b_{11} & b_{12} & b_{13} \\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{bmatrix}\n\nits determinant is:|\\mathbf{B}| = b_{11}(b_{22}b_{33} - b_{23}b_{32}) - b_{12}(b_{21}b_{33} - b_{23}b_{31}) + b_{13}(b_{21}b_{32} - b_{22}b_{31})","type":"content","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#properties-of-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"content":"|\\mathbf{A}| = |\\mathbf{A}'| and |\\mathbf{A}| \\cdot |\\mathbf{B}| = |\\mathbf{B}| \\cdot |\\mathbf{A}|.\n\nInterchanging any two rows or columns will affect the sign of the determinant, but not its absolute value.\n\nMultiplying a single row or column by a scalar will cause the value of the determinant to be multiplied by the scalar.\n\nAddition or subtraction of a nonzero multiple of any row or column to or from another row or column does not change the value of the determinant.\n\nThe determinant of a triangular matrix is equal to the product of the elements along the principal diagonal.\n\nIf any of the rows or columns equal zero, the determinant is also zero.\n\nIf two rows or columns are identical or proportional, i.e. linearly dependent, then the determinant is zero.","type":"content","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#properties-of-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#finding-the-inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of a Matrix"},"content":"The inverse of a matrix is defined as:\\mathbf{A}^{-1} = \\frac{1}{|\\mathbf{A}|} \\operatorname{adj}(\\mathbf{A})\n\nLet’s say we have the following linear system:2x_1 + 9x_2 + 4x_3 = -13 \\\\\n7x_1 - 8x_2 + 6x_3 = 63 \\\\\n5x_1 + 3x_2 - 6x_3 = 6\n\nWriting the above system in the form \\mathbf{A}\\mathbf{x} = \\mathbf{b}, where:\\mathbf{A} = \\begin{bmatrix} 2 & 9 & 4 \\\\ 7 & -8 & 6 \\\\ 5 & 3 & -6 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nWe can therefore solve the system by \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}, which means that we need the inverse of \\mathbf{A}.\n\nStep 1. The minor |M_{ij}| is the determinant of the submatrix formed by deleting the i-th row and j-th column of the original matrix. So we have:\\begin{bmatrix}\n30 & -72 & 61 \\\\\n-66 & -32 & -39 \\\\\n86 & -16 & -79\n\\end{bmatrix}\n\nStep 2: The cofactor C_{ij} is a minor with the prescribed sign that follows the rule:C_{ij} = (-1)^{i+j} |M_{ij}|\n\nHence, we have:\\begin{bmatrix}\n30 & 72 & 61 \\\\\n66 & -32 & 39 \\\\\n86 & 16 & -79\n\\end{bmatrix}\n\nStep 3: An adjoint matrix is simply the transpose of a cofactor matrix. Continuing the example above, we have:\\operatorname{adj}(\\mathbf{A}) = \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nSince |\\mathbf{A}| = 952, we then have:\\mathbf{A}^{-1} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nand\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix} \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nNot a nice one to solve, though.","type":"content","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#finding-the-inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"content":"Cramer came up with a simplified method for solving a system of linear equations through the use of determinants.\n\nBasically, given \\mathbf{A}\\mathbf{x} = \\mathbf{b}, we have:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n\nDefine:\\Delta = |\\mathbf{A}|\n\nand\\Delta_1 = \\begin{vmatrix} b_1 & a_{12} & a_{13} \\\\ b_2 & a_{22} & a_{23} \\\\ b_3 & a_{32} & a_{33} \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} a_{11} & b_1 & a_{13} \\\\ a_{21} & b_2 & a_{23} \\\\ a_{31} & b_3 & a_{33} \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} a_{11} & a_{12} & b_1 \\\\ a_{21} & a_{22} & b_2 \\\\ a_{31} & a_{32} & b_3 \\end{vmatrix}\n\nwhere the columns in the \\mathbf{A} matrix are replaced one by one by the \\mathbf{b} vector as shown above.\n\nThen:x_i = \\frac{\\Delta_i}{\\Delta}\n\nVerify that you get \\{4.5, -3, 1.25\\}.\n\nLinear algebra 1-5.pptx (17107k) 8 Jan 31, 2018, 7:56 AM v.1Linear algebra 6.pptx (3924k) 8 Feb 5, 2018, 7:15 AM v.1","type":"content","url":"/linear-algebra-basic-79ff306783b1849f5a909c7b845f9#cramers-rule","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or an interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors can be written asx =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}.\n\nA matrix is a rectangular array of numbers. For example,A =\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}.\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = [a_{ij} + b_{ij}].\n\nA matrix can be multiplied by a scalar c:cA = [c a_{ij}].\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is an m \\times n matrix and B is an n \\times k matrix, then\nAB is an m \\times k matrix.\n\nThe (i,j) element of AB is given by(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of\ntransformations matters.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows\nand columns:(A')_{ij} = a_{ji}.\n\nFor example,A =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix},\n\\qquad\nA' =\n\\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric ifA = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfiesAI = IA = A.\n\nFor example,I =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}.\n\nThe zero matrix has all elements equal to zero and acts as the additive\nidentity.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such\nthatAA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} yieldsx = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility typically reflects:\n\nredundancy,\n\nlinear dependence,\n\nthe absence of a unique solution.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes key\nproperties of the matrix.\n\nFor a 2 \\times 2 matrix,A =\n\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix},\n\nthe determinant is given by\\det(A) = ad - bc.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume-scaling effect of a linear\ntransformation.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equationsAx = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution isx_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"The columns of a matrix are linearly dependent if one column can be\nexpressed as a linear combination of the others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or\ncolumns.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-7efad8175bc9cc1256592746dcad0#what-comes-next","position":33},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-812f3616abd114c983c58b08abd95","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-812f3616abd114c983c58b08abd95","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-812f3616abd114c983c58b08abd95#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.\nClearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If ( \\mathbf{A} ) is an ( m \\times n ) matrix, then ( \\mathbf{A}’ ) is the ( n \\times m ) matrix whose rows are the columns of ( \\mathbf{A} ). So ( \\mathbf{A}’ = (a_{ji}) ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\na & c \\\nb & d\n\\end{bmatrix}\n]\n\nNote: ( (\\mathbf{A} + \\mathbf{B})’ = \\mathbf{A}’ + \\mathbf{B}’ ), however ( (\\mathbf{A}\\mathbf{B})’ = \\mathbf{B}‘\\mathbf{A}’ ).\n\nNull matrix: Has all elements as 0. Clearly ( \\mathbf{A} + \\mathbf{0}{m,n} = \\mathbf{0}{m,n} + \\mathbf{A} = \\mathbf{A} ) for all ( m \\times n ) matrices.\n\nScalar multiplication: If ( \\mathbf{A} = (a_{ij}) ) then for any constant ( k ), define ( k\\mathbf{A} = (k a_{ij}) ). That is, multiply each element in ( \\mathbf{A} ) by ( k ).\n\nMatrix multiplication: Say ( \\mathbf{A} ) is ( m \\times n ), and ( \\mathbf{B} ) is ( n \\times p ), then the ( m \\times p ) matrix ( \\mathbf{A}\\mathbf{B} ) is the product of ( \\mathbf{A} ) and ( \\mathbf{B} ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n\n= \\begin{bmatrix}\nae + bg & af + bh \\\nce + dg & cf + dh\n\\end{bmatrix}\n]\n\nand the matrices ( \\mathbf{A} ) and ( \\mathbf{B} ) are conformable.\n\nDiagonal matrix: A square ( n \\times n ) matrix ( \\mathbf{A} ) is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\n\n[\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\n0 & a_{22} & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n]\n\nNote: The elements in the diagonal of ( \\mathbf{A} ) are the same as those in ( \\mathbf{A}’ ).\n\nTrace of a square matrix: If ( \\mathbf{A} ) is a square matrix, the trace of ( \\mathbf{A} ), denoted ( \\operatorname{tr}(\\mathbf{A}) ), is the sum of the elements on the main/leading diagonal of ( \\mathbf{A} ).\n\nIdentity matrix: Denoted by ( \\mathbf{I}n ), the ( n \\times n ) diagonal matrix with ( a{ii} = 1 ) for all ( i ). That is:\n\n[\n\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\n0 & 1 & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n]\n\nSymmetric matrix: A square matrix ( \\mathbf{A} ) is symmetric if ( \\mathbf{A} = \\mathbf{A}’ ). The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-812f3616abd114c983c58b08abd95#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-812f3616abd114c983c58b08abd95#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if ( \\mathbf{A}^2 = \\mathbf{A} ). Below is an example (try squaring the matrix and see what you get).\n\n[\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\n]\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors ( \\mathbf{x} ) and ( \\mathbf{y} ) (and ( \\mathbf{z} )) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors ( \\mathbf{x} \\neq \\mathbf{0} ) and ( \\mathbf{y} \\neq \\mathbf{0} ) are said to be linearly independent iff the ONLY solution to ( a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} ) is ( a = 0 ) AND ( b = 0 ). And ( \\mathbf{x} ) and ( \\mathbf{y} ) are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-812f3616abd114c983c58b08abd95#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-812f3616abd114c983c58b08abd95#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"content":"Usually ( \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A} ). For example, try ( \\mathbf{A} = \\begin{bmatrix}2 & 0 \\ 3 & 1\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}0 & 1 \\ 0 & 0\\end{bmatrix} ).\n\nWe can have ( \\mathbf{A}\\mathbf{B} = \\mathbf{0} ) even if ( \\mathbf{A} ) or ( \\mathbf{B} ) are not zero. For example, try ( \\mathbf{A} = \\begin{bmatrix}6 & -12 \\ -3 & 6\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}12 & 6 \\ 6 & 3\\end{bmatrix} ).\n\nSay, ( \\mathbf{A} = \\begin{bmatrix}4 & 8 \\ 1 & 2\\end{bmatrix} ), ( \\mathbf{B} = \\begin{bmatrix}2 & 1 \\ 2 & 2\\end{bmatrix} ), and ( \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\ 4 & 2\\end{bmatrix} ). We find that ( \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} ) even though ( \\mathbf{B} \\neq \\mathbf{C} ).\n\nSay ( \\mathbf{A} ) and ( \\mathbf{B} ) are singular matrices. Then ( \\mathbf{A}\\mathbf{B} ) will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-812f3616abd114c983c58b08abd95#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-812f3616abd114c983c58b08abd95#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:\n\n[\n3x_1 + 4x_2 = 5 \\\n7x_1 - 2x_2 = 2\n]\n\nHere, we can define:\n\n[\n\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\ 2 \\end{pmatrix}\n]\n\nThen we see that:\n\n[\n\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\ 7x_1 - 2x_2 \\end{pmatrix}\n]\n\nThe original system is in fact equivalent to the matrix form:\n\n[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-812f3616abd114c983c58b08abd95#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or an interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors can be written asx =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}.\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = [a_{ij} + b_{ij}].\n\nA matrix can be multiplied by a scalar c:cA = [c a_{ij}].\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is an m \\times n matrix and B is an n \\times k matrix, then\nAB is an m \\times k matrix.\n\nThe (i,j) element of AB is given by(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of\ntransformations matters.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows\nand columns:(A')_{ij} = a_{ji}.\n\nFor example,\n$$\nA =\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\n\\qquad\nA’ =\\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric ifA = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfiesAI = IA = A.\n\nFor example,\n$$\nI =\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive\nidentity.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such\nthatAA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} yieldsx = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility typically reflects:\n\nredundancy,\n\nlinear dependence,\n\nthe absence of a unique solution.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes key\nproperties of the matrix.\n\nFor a 2 \\times 2 matrix,\n$$\nA =\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}the determinant is given by\n\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume-scaling effect of a linear\ntransformation.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equationsAx = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution isx_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"The columns of a matrix are linearly dependent if one column can be\nexpressed as a linear combination of the others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or\ncolumns.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-822f99c036452faad5ae621380d6a#what-comes-next","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or an interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors can be written as\n$$\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\n$$\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = [a_{ij} + b_{ij}].\n\nA matrix can be multiplied by a scalar c:cA = [c a_{ij}].\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is an m \\times n matrix and B is an n \\times k matrix, then\nAB is an m \\times k matrix.\n\nThe (i,j) element of AB is given by(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of\ntransformations matters.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows\nand columns:(A')_{ij} = a_{ji}.\n\nFor example,\n$$\nA =\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\n\\qquad\nA’ =\\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric ifA = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfiesAI = IA = A.\n\nFor example,\n$$\nI =\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive\nidentity.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such\nthatAA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} yieldsx = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility typically reflects:\n\nredundancy,\n\nlinear dependence,\n\nthe absence of a unique solution.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes key\nproperties of the matrix.\n\nFor a 2 \\times 2 matrix,\n$$\nA =\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}the determinant is given by\n\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume-scaling effect of a linear\ntransformation.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equationsAx = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution isx_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"The columns of a matrix are linearly dependent if one column can be\nexpressed as a linear combination of the others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or\ncolumns.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-8b97905f523953a3d514cd6733086#what-comes-next","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-93b84f02b990651527886929993f3","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors:\n$$\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\n$$\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = (a_{ij} + b_{ij}).\n\nA matrix can be multiplied by a scalar c:cA = (c a_{ij}).\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is m \\times n and B is n \\times k, then AB is m \\times k.\n\nThe (i,j) element of AB is:(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of transformations matters.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows and columns:(A')_{ij} = a_{ji}.\n\nExample:\n$$\nA =\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\n\\quad\nA’ =\\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric if:A = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfies:AI = IA = A.\n\nFor example:\n$$\nI =\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive identity.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such that:AA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} gives:x = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility usually reflects:\n\nredundancy,\n\nlinear dependence,\n\nlack of unique solutions.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes\nkey properties of the matrix.\n\nFor a 2 \\times 2 matrix:\n$$\nA =\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\n\\quad\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume scaling effect of a linear transformation.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equations:Ax = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution is:x_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems, but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"Columns of a matrix are linearly dependent if one column can be written as\na linear combination of others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or columns.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-93b84f02b990651527886929993f3#what-comes-next","position":33},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics"},"type":"lvl1","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics"},"content":"","type":"content","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}' = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Some Notes on Matrices"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:3x_1 + 4x_2 = 5 \\\\\n7x_1 - 2x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\\\ 7x_1 - 2x_2 \\end{pmatrix}\n\nThe original system is in fact equivalent to the matrix form:\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#matrix-inversion","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Inversion"},"content":"To better understand the mechanics of finding the inverse of a square matrix, let’s begin by looking at an example.\\mathbf{A} = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nAssume we have:\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix}\n\nWhere does the 10 in the matrix after the = sign come from? It is the determinant of \\mathbf{A}.\n\nNow we have:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 10 & 0 \\\\ 0 & 10 \\end{bmatrix} = 10 \\mathbf{I}\n\nHence, (pre-)multiplying both sides of the equation by \\frac{1}{10} will give you:\\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\mathbf{I}\n\nThe matrix \\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} is in fact the inverse of \\mathbf{A}. Hence we have the relationship \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}. In fact, \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.\n\nHere are some properties of the inverse of a matrix worth knowing:\n\nProperty 1: For any nonsingular matrix \\mathbf{A}, (\\mathbf{A}^{-1})^{-1} = \\mathbf{A}.\n\nProperty 2: The determinant of the inverse of a matrix is equal to the reciprocal of the determinant of the matrix. That is |\\mathbf{A}^{-1}| = \\frac{1}{|\\mathbf{A}|}.\n\nProperty 3: The inverse of matrix \\mathbf{A} is unique.\n\nProperty 4: For any nonsingular matrix \\mathbf{A}, the inverse of the transpose of a matrix is equal to the transpose of the inverse of the matrix. (\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'.\n\nProperty 5: If \\mathbf{A} and \\mathbf{B} are nonsingular and of the same dimension, then \\mathbf{A}\\mathbf{B} is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices is equal to the product of their inverses in reverse order. (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}.","type":"content","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#matrix-inversion","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar value uniquely associated with a square non-singular matrix, and is usually denoted as |\\mathbf{A}|. The question of whether or not a matrix is nonsingular and therefore invertible is linked to the value of its determinant.\n\nWe can write a generic 2 \\times 2 matrix as:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}\n\nand its determinant is defined as:|\\mathbf{A}| = a_{11}a_{22} - a_{12}a_{21}\n\nFor a 3 \\times 3 matrix, say:\\mathbf{B} = \\begin{bmatrix} b_{11} & b_{12} & b_{13} \\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{bmatrix}\n\nits determinant is:|\\mathbf{B}| = b_{11}(b_{22}b_{33} - b_{23}b_{32}) - b_{12}(b_{21}b_{33} - b_{23}b_{31}) + b_{13}(b_{21}b_{32} - b_{22}b_{31})","type":"content","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#properties-of-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"content":"|\\mathbf{A}| = |\\mathbf{A}'| and |\\mathbf{A}| \\cdot |\\mathbf{B}| = |\\mathbf{B}| \\cdot |\\mathbf{A}|.\n\nInterchanging any two rows or columns will affect the sign of the determinant, but not its absolute value.\n\nMultiplying a single row or column by a scalar will cause the value of the determinant to be multiplied by the scalar.\n\nAddition or subtraction of a nonzero multiple of any row or column to or from another row or column does not change the value of the determinant.\n\nThe determinant of a triangular matrix is equal to the product of the elements along the principal diagonal.\n\nIf any of the rows or columns equal zero, the determinant is also zero.\n\nIf two rows or columns are identical or proportional, i.e. linearly dependent, then the determinant is zero.","type":"content","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#properties-of-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#finding-the-inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix"},"content":"The inverse of a matrix is defined as:\\mathbf{A}^{-1} = \\frac{1}{|\\mathbf{A}|} \\operatorname{adj}(\\mathbf{A})\n\nLet’s say we have the following linear system:2x_1 + 9x_2 + 4x_3 = -13 \\\\\n7x_1 - 8x_2 + 6x_3 = 63 \\\\\n5x_1 + 3x_2 - 6x_3 = 6\n\nWriting the above system in the form \\mathbf{A}\\mathbf{x} = \\mathbf{b}, where:\\mathbf{A} = \\begin{bmatrix} 2 & 9 & 4 \\\\ 7 & -8 & 6 \\\\ 5 & 3 & -6 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nWe can therefore solve the system by \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}, which means that we need the inverse of \\mathbf{A}.\n\nStep 1. The minor |M_{ij}| is the determinant of the submatrix formed by deleting the i-th row and j-th column of the original matrix. So we have:\\begin{bmatrix}\n30 & -72 & 61 \\\\\n-66 & -32 & -39 \\\\\n86 & -16 & -79\n\\end{bmatrix}\n\nStep 2: The cofactor C_{ij} is a minor with the prescribed sign that follows the rule:C_{ij} = (-1)^{i+j} |M_{ij}|\n\nHence, we have:\\begin{bmatrix}\n30 & 72 & 61 \\\\\n66 & -32 & 39 \\\\\n86 & 16 & -79\n\\end{bmatrix}\n\nStep 3: An adjoint matrix is simply the transpose of a cofactor matrix. Continuing the example above, we have:\\operatorname{adj}(\\mathbf{A}) = \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nSince |\\mathbf{A}| = 952, we then have:\\mathbf{A}^{-1} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nand\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix} \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nNot a nice one to solve, though.","type":"content","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#finding-the-inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"content":"Cramer came up with a simplified method for solving a system of linear equations through the use of determinants.\n\nBasically, given \\mathbf{A}\\mathbf{x} = \\mathbf{b}, we have:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n\nDefine:\\Delta = |\\mathbf{A}|\n\nand\\Delta_1 = \\begin{vmatrix} b_1 & a_{12} & a_{13} \\\\ b_2 & a_{22} & a_{23} \\\\ b_3 & a_{32} & a_{33} \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} a_{11} & b_1 & a_{13} \\\\ a_{21} & b_2 & a_{23} \\\\ a_{31} & b_3 & a_{33} \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} a_{11} & a_{12} & b_1 \\\\ a_{21} & a_{22} & b_2 \\\\ a_{31} & a_{32} & b_3 \\end{vmatrix}\n\nwhere the columns in the \\mathbf{A} matrix are replaced one by one by the \\mathbf{b} vector as shown above.\n\nThen:x_i = \\frac{\\Delta_i}{\\Delta}\n\nVerify that you get \\{4.5, -3, 1.25\\}.\n\nLinear algebra 1-5.pptx (17107k) 8 Jan 31, 2018, 7:56 AM v.1Linear algebra 6.pptx (3924k) 8 Feb 5, 2018, 7:15 AM v.1","type":"content","url":"/linear-algebra-basic-99438489baeaad4731e685c88635e#cramers-rule","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or an interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors can be written asx =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}.\n\nA matrix is a rectangular array of numbers. For example,A =\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}.\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = [a_{ij} + b_{ij}].\n\nA matrix can be multiplied by a scalar c:cA = [c a_{ij}].\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is an m \\times n matrix and B is an n \\times k matrix, then\nAB is an m \\times k matrix.\n\nThe (i,j) element of AB is given by(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of\ntransformations matters.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows\nand columns:(A')_{ij} = a_{ji}.\n\nFor example,\n$$\nA =\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\n\\qquad\nA’ =\\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric ifA = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfiesAI = IA = A.\n\nFor example,\n$$\nI =\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive\nidentity.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such\nthatAA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} yieldsx = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility typically reflects:\n\nredundancy,\n\nlinear dependence,\n\nthe absence of a unique solution.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes key\nproperties of the matrix.\n\nFor a 2 \\times 2 matrix,\n$$\nA =\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}the determinant is given by\n\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume-scaling effect of a linear\ntransformation.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equationsAx = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution isx_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"The columns of a matrix are linearly dependent if one column can be\nexpressed as a linear combination of the others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or\ncolumns.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-a3549cae7572151e42b9e6a3d3d51#what-comes-next","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors:\n$$\nx =\\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{pmatrix}\n\n$$\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = (a_{ij} + b_{ij}).\n\nA matrix can be multiplied by a scalar c:cA = (c a_{ij}).\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is m \\times n and B is n \\times k, then AB is m \\times k.\n\nThe (i,j) element of AB is:(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of transformations matters.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows and columns:(A')_{ij} = a_{ji}.\n\nExample:\n$$\nA =\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix}\n\n\\quad\nA’ =\\begin{pmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{pmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric if:A = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfies:AI = IA = A.\n\nFor example:\n$$\nI =\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive identity.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such that:AA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} gives:x = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility usually reflects:\n\nredundancy,\n\nlinear dependence,\n\nlack of unique solutions.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes\nkey properties of the matrix.\n\nFor a 2 \\times 2 matrix:\n$$\nA =\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\n\\quad\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume scaling effect of a linear transformation.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equations:Ax = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution is:x_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems, but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"Columns of a matrix are linearly dependent if one column can be written as\na linear combination of others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or columns.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-a724cb5a07be119f062d7709b34ae#what-comes-next","position":33},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/linear-algebra-basic-b6b5ffe5a53787febcd9c630647b9","position":0},{"hierarchy":{"lvl1":""},"content":"","type":"content","url":"/linear-algebra-basic-b6b5ffe5a53787febcd9c630647b9","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"[cite_start]Matrix addition and subtraction: If A=(a_{ij}) and B=(b_{ij}) are two m\\times n matrices of same dimension, then A+B is defined as (a_{ij}+b_{ij})[cite: 28]. [cite_start]That is, we add element by element the two matrices[cite: 29]. [cite_start]Clearly A+B=B+A[cite: 29]. [cite_start]The rule applies to matrix subtraction[cite: 29].\n\n[cite_start]Transpose of a matrix: If A is a m\\times n matrix, the A' is the n\\times m matrix whose rows are the columns of A[cite: 30]. [cite_start]So A^{\\prime}=(a_{ji})[cite: 30]. For example:\n\n[cite_start][\\begin{matrix}2&-3\\\\ -4&5\\end{matrix}]^{\\prime}=[\\begin{matrix}2&-4\\\\ -3&5\\end{matrix}] [cite: 32]\n\n[cite_start]Note: (A+B)^{\\prime}=A^{\\prime}+B^{\\prime} [cite: 31]\n\n[cite_start]Note: (AB)^{\\prime}=B^{\\prime}A^{\\prime} [cite: 33]\n\n[cite_start]Null matrix: Has all elements as 0[cite: 34]. [cite_start]Clearly A+O_{m,n}=0_{m,n}+A=A for all m\\times n matrices[cite: 34].\n\n[cite_start]Scalar multiplication: If A=(a_{ij}) then for any constant k define kA=(ka_{ij})[cite: 35]. [cite_start]That is, multiply each element in A by k[cite: 35].\n\n[cite_start]Matrix multiplication: Say A is m\\times n and B is n\\times p then the m\\times p matrix AB is the product of A and B[cite: 36]. For example:\n\n[cite_start][\\begin{matrix}1&2\\\\ 4&0\\end{matrix}][\\begin{matrix}3&-1\\\\ 4&3\\end{matrix}]=[\\begin{matrix}11&5\\\\ 12&-4\\end{matrix}] [cite: 37]\n\n[cite_start]The matrices A and B are comformable[cite: 38].\n\n[cite_start]Diagonal matrix: A square n\\times n matrix A is diagonal if all entries off the ‘main diagonal’ are zero[cite: 39].\n[cite_start]A=[\\begin{matrix}a_{11}&0&...&0\\\\ 0&a_{22}&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&a_{nn}\\end{matrix}] [cite: 40]\n\n[cite_start]Note: The elements in the diagonal of A are the same as those in A'[cite: 41].\n\n[cite_start]Trace of a square matrix: If A is a square, the trace of A, denoted tr(A), is the sum of the elements on the main/leading diagonal of A[cite: 42, 43].\n\n[cite_start]Identity matrix: Denoted by I_{n}, the n\\times n diagonal matrix with a_{ij}=1 for all i[cite: 44, 46].\n[cite_start]A=[\\begin{matrix}1&0&...&0\\\\ 0&1&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&1\\end{matrix}] [cite: 45]\n\n[cite_start]Symmetric matrix: A square matrix A is symmetric if A=A^{\\prime}[cite: 47]. [cite_start]The identity matrix and the square null matrix are symmetric[cite: 47].\n\n[cite_start]Idempotent matrix: A square matrix is said to be idempotent if A^{2}=A[cite: 48].\n[cite_start]Example: A=[\\begin{matrix}2&-2&-4\\\\ -1&3&4\\\\ 1&-2&-3\\end{matrix}] [cite: 49]\n\n[cite_start]Singular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has a determinant of zero[cite: 50].\n\n[cite_start]Any two (or three) vectors x and y (and z) are linearly dependent iff one can be written as a scalar multiple of the other[cite: 51].\n\n[cite_start]Two vectors x\\ne0 and y\\ne0 are linearly independent iff the only solution to ax+by=0 is a=0 and b=0[cite: 52].\n\n[cite_start]x and y are said to be orthogonal[cite: 53].","type":"content","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"type":"lvl3","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#some-notes-on-matrices","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"content":"[cite_start]Usually AB\\ne BA[cite: 55].\n\n[cite_start]We can have AB=0 even if A or B are not zero[cite: 56].\n\n[cite_start]We can find that AB=AC even though B\\ne C[cite: 58].\n\n[cite_start]If A and B are singular matrices, AB will not be zero, although the product will be a singular matrix[cite: 59].","type":"content","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#some-notes-on-matrices","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#systems-of-equations-in-matrix-form","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"[cite_start]One of the uses of linear algebra in economics is to represent and then solve systems of equations[cite: 61]. For instance:\n[cite_start]3x_{1}+4x_{2}=5 [cite: 68]\n[cite_start]7x_{1}-2x_{2}=2 [cite: 68]\n\n[cite_start]This can be defined as Ax=b[cite: 76], where:\n[cite_start]A = [\\begin{matrix}3&4\\\\ 7&-2\\end{matrix}], x=[\\begin{matrix}x_{1}\\\\ x_{2}\\end{matrix}], and b=[\\begin{matrix}5\\\\ 2\\end{matrix}] [cite: 71, 73, 136]","type":"content","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#systems-of-equations-in-matrix-form","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#matrix-inversion","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"content":"[cite_start]In fact, A^{-1}A=AA^{-1}=I[cite: 94]. [cite_start]To find the inverse of A=[\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}][cite: 79]:\n\n[cite_start]Find a matrix such that [\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}][\\begin{matrix}a&b\\\\ c&d\\end{matrix}]=[\\begin{matrix}10&0\\\\ 0&10\\end{matrix}][cite: 81].\n\n[cite_start]The value 10 is the determinant of A[cite: 85].\n\n[cite_start]The resulting matrix is \\frac{1}{10}[\\begin{matrix}4&2\\\\ -3&1\\end{matrix}][cite: 90].","type":"content","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#matrix-inversion","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"type":"lvl3","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#properties-of-the-inverse","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"content":"[cite_start]Property 1: (A^{-1})^{-1}=A for any nonsingular matrix A[cite: 96, 97].\n\n[cite_start]Property 2: |A^{-1}|=\\frac{1}{|A|}[cite: 101].\n\n[cite_start]Property 3: The inverse of matrix A is unique[cite: 100].\n\n[cite_start]Property 4: (A^{\\prime})^{-1}=(A^{-1})^{\\prime}[cite: 103].\n\n[cite_start]Property 5: If A and B are nonsingular, AB is also nonsingular[cite: 104].\n\n[cite_start]Property 6: (AB)^{-1}=B^{-1}A^{-1}[cite: 106].","type":"content","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#properties-of-the-inverse","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"content":"[cite_start]The determinant is a scalar-value associated with a square non-singular matrix, denoted as |A|[cite: 108].\n\n[cite_start]For a 2\\times2 matrix A=[\\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix}], |A|=a_{11}a_{22}-a_{12}a_{21}[cite: 113].\n\n[cite_start]The Laplace expansion is often used for higher orders[cite: 116].","type":"content","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#notes-on-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"content":"[cite_start]|A|=|A^{\\prime}| and |A|\\cdot|B|=|B|\\cdot|A|[cite: 122].\n\n[cite_start]Interchanging any two rows or columns changes the sign[cite: 123].\n\n[cite_start]Multiplying a single row/column by a scalar k multiplies the determinant by k[cite: 124].\n\n[cite_start]Adding a multiple of one row/column to another does not change the determinant[cite: 125].\n\n[cite_start]The determinant of a triangular matrix is the product of the principal diagonal elements[cite: 126].\n\n[cite_start]If any row or column equals zero, the determinant is zero[cite: 127].\n\n[cite_start]If two rows or columns are linearly dependent, the determinant is zero[cite: 128].","type":"content","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#notes-on-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"type":"lvl2","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#finding-the-inverse-of-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"content":"[cite_start]The inverse is defined as: A^{-1}=\\frac{1}{|A|}adjA[cite: 132].\n\n[cite_start]Step 1: Find the minor |M_{ij}|, the determinant of the submatrix after deleting the ith row and jth column[cite: 138].\n\n[cite_start]Step 2: The cofactor |C_{ij}|=(-1)^{i+j}|M_{ij}|[cite: 141, 143].\n\n[cite_start]Step 3: The adjoint matrix is the transpose of the cofactor matrix[cite: 144].","type":"content","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#finding-the-inverse-of-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"content":"[cite_start]A simplified method for solving Ax=b using determinants[cite: 151, 152].\n[cite_start]x_{i}=\\frac{|\\Delta_{i}|}{|\\Delta|} [cite: 158]\n[cite_start]Where \\Delta_{i} is the matrix A with the ith column replaced by vector b[cite: 157].","type":"content","url":"/linear-algebra-basic-baf7427132877eb9626dcaed32a5a#cramers-rule","position":19},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}' = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:3x_1 + 4x_2 = 5 \\\\\n7x_1 - 2x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\\\ 7x_1 - 2x_2 \\end{pmatrix}\n\nThe original system is in fact equivalent to the matrix form:\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#matrix-inversion","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"content":"To better understand the mechanics of finding the inverse of a square matrix, let’s begin by looking at an example.\\mathbf{A} = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nAssume we have:\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix}\n\nWhere does the 10 in the matrix after the = sign come from? It is the determinant of \\mathbf{A}.\n\nNow we have:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 10 & 0 \\\\ 0 & 10 \\end{bmatrix} = 10 \\mathbf{I}\n\nHence, (pre-)multiplying both sides of the equation by \\frac{1}{10} will give you:\\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\mathbf{I}\n\nThe matrix \\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} is in fact the inverse of \\mathbf{A}. Hence we have the relationship \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}. In fact, \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.\n\nHere are some properties of the inverse of a matrix worth knowing:\n\nProperty 1: For any nonsingular matrix \\mathbf{A}, (\\mathbf{A}^{-1})^{-1} = \\mathbf{A}.\n\nProperty 2: The determinant of the inverse of a matrix is equal to the reciprocal of the determinant of the matrix. That is |\\mathbf{A}^{-1}| = \\frac{1}{|\\mathbf{A}|}.\n\nProperty 3: The inverse of matrix \\mathbf{A} is unique.\n\nProperty 4: For any nonsingular matrix \\mathbf{A}, the inverse of the transpose of a matrix is equal to the transpose of the inverse of the matrix. (\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'.\n\nProperty 5: If \\mathbf{A} and \\mathbf{B} are nonsingular and of the same dimension, then \\mathbf{A}\\mathbf{B} is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices is equal to the product of their inverses in reverse order. (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}.","type":"content","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#matrix-inversion","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar value uniquely associated with a square non-singular matrix, and is usually denoted as |\\mathbf{A}|. The question of whether or not a matrix is nonsingular and therefore invertible is linked to the value of its determinant.\n\nWe can write a generic 2 \\times 2 matrix as:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}\n\nand its determinant is defined as:|\\mathbf{A}| = a_{11}a_{22} - a_{12}a_{21}\n\nFor a 3 \\times 3 matrix, say:\\mathbf{B} = \\begin{bmatrix} b_{11} & b_{12} & b_{13} \\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{bmatrix}\n\nits determinant is:|\\mathbf{B}| = b_{11}(b_{22}b_{33} - b_{23}b_{32}) - b_{12}(b_{21}b_{33} - b_{23}b_{31}) + b_{13}(b_{21}b_{32} - b_{22}b_{31})","type":"content","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#properties-of-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"content":"|\\mathbf{A}| = |\\mathbf{A}'| and |\\mathbf{A}| \\cdot |\\mathbf{B}| = |\\mathbf{B}| \\cdot |\\mathbf{A}|.\n\nInterchanging any two rows or columns will affect the sign of the determinant, but not its absolute value.\n\nMultiplying a single row or column by a scalar will cause the value of the determinant to be multiplied by the scalar.\n\nAddition or subtraction of a nonzero multiple of any row or column to or from another row or column does not change the value of the determinant.\n\nThe determinant of a triangular matrix is equal to the product of the elements along the principal diagonal.\n\nIf any of the rows or columns equal zero, the determinant is also zero.\n\nIf two rows or columns are identical or proportional, i.e. linearly dependent, then the determinant is zero.","type":"content","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#properties-of-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#finding-the-inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of a Matrix"},"content":"The inverse of a matrix is defined as:\\mathbf{A}^{-1} = \\frac{1}{|\\mathbf{A}|} \\operatorname{adj}(\\mathbf{A})\n\nLet’s say we have the following linear system:2x_1 + 9x_2 + 4x_3 = -13 \\\\\n7x_1 - 8x_2 + 6x_3 = 63 \\\\\n5x_1 + 3x_2 - 6x_3 = 6\n\nWriting the above system in the form \\mathbf{A}\\mathbf{x} = \\mathbf{b}, where:\\mathbf{A} = \\begin{bmatrix} 2 & 9 & 4 \\\\ 7 & -8 & 6 \\\\ 5 & 3 & -6 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nWe can therefore solve the system by \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}, which means that we need the inverse of \\mathbf{A}.\n\nStep 1. The minor |M_{ij}| is the determinant of the submatrix formed by deleting the i-th row and j-th column of the original matrix. So we have:\\begin{bmatrix}\n30 & -72 & 61 \\\\\n-66 & -32 & -39 \\\\\n86 & -16 & -79\n\\end{bmatrix}\n\nStep 2: The cofactor C_{ij} is a minor with the prescribed sign that follows the rule:C_{ij} = (-1)^{i+j} |M_{ij}|\n\nHence, we have:\\begin{bmatrix}\n30 & 72 & 61 \\\\\n66 & -32 & 39 \\\\\n86 & 16 & -79\n\\end{bmatrix}\n\nStep 3: An adjoint matrix is simply the transpose of a cofactor matrix. Continuing the example above, we have:\\operatorname{adj}(\\mathbf{A}) = \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nSince |\\mathbf{A}| = 952, we then have:\\mathbf{A}^{-1} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nand\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix} \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nNot a nice one to solve, though.","type":"content","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#finding-the-inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"content":"Cramer came up with a simplified method for solving a system of linear equations through the use of determinants.\n\nBasically, given \\mathbf{A}\\mathbf{x} = \\mathbf{b}, we have:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n\nDefine:\\Delta = |\\mathbf{A}|\n\nand\\Delta_1 = \\begin{vmatrix} b_1 & a_{12} & a_{13} \\\\ b_2 & a_{22} & a_{23} \\\\ b_3 & a_{32} & a_{33} \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} a_{11} & b_1 & a_{13} \\\\ a_{21} & b_2 & a_{23} \\\\ a_{31} & b_3 & a_{33} \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} a_{11} & a_{12} & b_1 \\\\ a_{21} & a_{22} & b_2 \\\\ a_{31} & a_{32} & b_3 \\end{vmatrix}\n\nwhere the columns in the \\mathbf{A} matrix are replaced one by one by the \\mathbf{b} vector as shown above.\n\nThen:x_i = \\frac{\\Delta_i}{\\Delta}\n\nVerify that you get \\{4.5, -3, 1.25\\}.\n\nLinear algebra 1-5.pptx (17107k) 8 Jan 31, 2018, 7:56 AM v.1Linear algebra 6.pptx (3924k) 8 Feb 5, 2018, 7:15 AM v.1","type":"content","url":"/linear-algebra-basic-c5b5e31212ad36d3176e2dd6c813d#cramers-rule","position":19},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-d2dfef4747a450ddaff5abc4e64e0","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-d2dfef4747a450ddaff5abc4e64e0","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-d2dfef4747a450ddaff5abc4e64e0#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}' = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-d2dfef4747a450ddaff5abc4e64e0#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-d2dfef4747a450ddaff5abc4e64e0#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-d2dfef4747a450ddaff5abc4e64e0#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-d2dfef4747a450ddaff5abc4e64e0#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-d2dfef4747a450ddaff5abc4e64e0#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-d2dfef4747a450ddaff5abc4e64e0#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:3x_1 + 4x_2 = 5 \\\\\n7x_1 - 2x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\\\ 7x_1 - 2x_2 \\end{pmatrix}\n\nThe original system is in fact equivalent to the matrix form:\n\n$$\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-d2dfef4747a450ddaff5abc4e64e0#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-d9ca6a93e28eb419a884f30e33b72","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-d9ca6a93e28eb419a884f30e33b72","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-d9ca6a93e28eb419a884f30e33b72#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.\nClearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If ( \\mathbf{A} ) is an ( m \\times n ) matrix, then ( \\mathbf{A}’ ) is the ( n \\times m ) matrix whose rows are the columns of ( \\mathbf{A} ). So ( \\mathbf{A}’ = (a_{ji}) ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\na & c \\\nb & d\n\\end{bmatrix}\n]\n\nNote: ( (\\mathbf{A} + \\mathbf{B})’ = \\mathbf{A}’ + \\mathbf{B}’ ), however ( (\\mathbf{A}\\mathbf{B})’ = \\mathbf{B}‘\\mathbf{A}’ ).\n\nNull matrix: Has all elements as 0. Clearly ( \\mathbf{A} + \\mathbf{0}{m,n} = \\mathbf{0}{m,n} + \\mathbf{A} = \\mathbf{A} ) for all ( m \\times n ) matrices.\n\nScalar multiplication: If ( \\mathbf{A} = (a_{ij}) ) then for any constant ( k ), define ( k\\mathbf{A} = (k a_{ij}) ). That is, multiply each element in ( \\mathbf{A} ) by ( k ).\n\nMatrix multiplication: Say ( \\mathbf{A} ) is ( m \\times n ), and ( \\mathbf{B} ) is ( n \\times p ), then the ( m \\times p ) matrix ( \\mathbf{A}\\mathbf{B} ) is the product of ( \\mathbf{A} ) and ( \\mathbf{B} ). For example:\n\n[\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n\n= \\begin{bmatrix}\nae + bg & af + bh \\\nce + dg & cf + dh\n\\end{bmatrix}\n]\n\nand the matrices ( \\mathbf{A} ) and ( \\mathbf{B} ) are conformable.\n\nDiagonal matrix: A square ( n \\times n ) matrix ( \\mathbf{A} ) is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\n\n[\n\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\n0 & a_{22} & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n]\n\nNote: The elements in the diagonal of ( \\mathbf{A} ) are the same as those in ( \\mathbf{A}’ ).\n\nTrace of a square matrix: If ( \\mathbf{A} ) is a square matrix, the trace of ( \\mathbf{A} ), denoted ( \\operatorname{tr}(\\mathbf{A}) ), is the sum of the elements on the main/leading diagonal of ( \\mathbf{A} ).\n\nIdentity matrix: Denoted by ( \\mathbf{I}n ), the ( n \\times n ) diagonal matrix with ( a{ii} = 1 ) for all ( i ). That is:\n\n[\n\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\n0 & 1 & \\cdots & 0 \\\n\\vdots & \\vdots & \\ddots & \\vdots \\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n]\n\nSymmetric matrix: A square matrix ( \\mathbf{A} ) is symmetric if ( \\mathbf{A} = \\mathbf{A}’ ). The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic-d9ca6a93e28eb419a884f30e33b72#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic-d9ca6a93e28eb419a884f30e33b72#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if ( \\mathbf{A}^2 = \\mathbf{A} ). Below is an example (try squaring the matrix and see what you get).\n\n[\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\n]\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors ( \\mathbf{x} ) and ( \\mathbf{y} ) (and ( \\mathbf{z} )) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors ( \\mathbf{x} \\neq \\mathbf{0} ) and ( \\mathbf{y} \\neq \\mathbf{0} ) are said to be linearly independent iff the ONLY solution to ( a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} ) is ( a = 0 ) AND ( b = 0 ). And ( \\mathbf{x} ) and ( \\mathbf{y} ) are said to be orthogonal.","type":"content","url":"/linear-algebra-basic-d9ca6a93e28eb419a884f30e33b72#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic-d9ca6a93e28eb419a884f30e33b72#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Some Notes on Matrices"},"content":"Usually ( \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A} ). For example, try ( \\mathbf{A} = \\begin{bmatrix}2 & 0 \\ 3 & 1\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}0 & 1 \\ 0 & 0\\end{bmatrix} ).\n\nWe can have ( \\mathbf{A}\\mathbf{B} = \\mathbf{0} ) even if ( \\mathbf{A} ) or ( \\mathbf{B} ) are not zero. For example, try ( \\mathbf{A} = \\begin{bmatrix}6 & -12 \\ -3 & 6\\end{bmatrix} ) and ( \\mathbf{B} = \\begin{bmatrix}12 & 6 \\ 6 & 3\\end{bmatrix} ).\n\nSay, ( \\mathbf{A} = \\begin{bmatrix}4 & 8 \\ 1 & 2\\end{bmatrix} ), ( \\mathbf{B} = \\begin{bmatrix}2 & 1 \\ 2 & 2\\end{bmatrix} ), and ( \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\ 4 & 2\\end{bmatrix} ). We find that ( \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} ) even though ( \\mathbf{B} \\neq \\mathbf{C} ).\n\nSay ( \\mathbf{A} ) and ( \\mathbf{B} ) are singular matrices. Then ( \\mathbf{A}\\mathbf{B} ) will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic-d9ca6a93e28eb419a884f30e33b72#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-d9ca6a93e28eb419a884f30e33b72#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:\n\n[\n3x_1 + 4x_2 = 5 \\\n7x_1 - 2x_2 = 2\n]\n\nHere, we can define:\n\n[\n\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\ 2 \\end{pmatrix}\n]\n\nThen we see that:\n\n[\n\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\ 7x_1 - 2x_2 \\end{pmatrix}\n]\n\nThe original system is in fact equivalent to the matrix form:\n\n[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic-d9ca6a93e28eb419a884f30e33b72#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors:\n$$\nx =\\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{pmatrix}\n\n$$\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{pmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{pmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = (a_{ij} + b_{ij}).\n\nA matrix can be multiplied by a scalar c:cA = (c a_{ij}).\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is m \\times n and B is n \\times k, then AB is m \\times k.\n\nThe (i,j) element of AB is:(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of transformations matters.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows and columns:(A')_{ij} = a_{ji}.\n\nExample:\n$$\nA =\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix}\n\n\\quad\nA’ =\\begin{pmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{pmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric if:A = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfies:AI = IA = A.\n\nFor example:\n$$\nI =\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive identity.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such that:AA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} gives:x = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility usually reflects:\n\nredundancy,\n\nlinear dependence,\n\nlack of unique solutions.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes\nkey properties of the matrix.\n\nFor a 2 \\times 2 matrix:\n$$\nA =\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\n\\quad\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume scaling effect of a linear transformation.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equations:Ax = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution is:x_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems, but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"Columns of a matrix are linearly dependent if one column can be written as\na linear combination of others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or columns.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-e697718024669ed8f84a268ffc1b8#what-comes-next","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors:\n$$\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\n$$\n\nA matrix is a rectangular array of numbers. For example,\n$$\nA =\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\n$$\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = (a_{ij} + b_{ij}).\n\nA matrix can be multiplied by a scalar c:cA = (c a_{ij}).\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is m \\times n and B is n \\times k, then AB is m \\times k.\n\nThe (i,j) element of AB is:(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of transformations matters.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows and columns:(A')_{ij} = a_{ji}.\n\nExample:\n$$\nA =\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\n\\quad\nA’ =\\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric if:A = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfies:AI = IA = A.\n\nFor example:\n$$\nI =\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\n$$\n\nThe zero matrix has all elements equal to zero and acts as the additive identity.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such that:AA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} gives:x = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility usually reflects:\n\nredundancy,\n\nlinear dependence,\n\nlack of unique solutions.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes\nkey properties of the matrix.\n\nFor a 2 \\times 2 matrix:\n$$\nA =\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\n\\quad\n\\det(A) = ad - bc.\n$$","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume scaling effect of a linear transformation.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equations:Ax = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution is:x_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems, but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"Columns of a matrix are linearly dependent if one column can be written as\na linear combination of others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or columns.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-e88784f92dff5e642032285dff575#what-comes-next","position":33},{"hierarchy":{"lvl1":"2. Linear Algebra"},"type":"lvl1","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra"},"content":"","type":"content","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Basics"},"content":"[cite_start]Matrix addition and subtraction: If A=(a_{ij}) and B=(b_{ij}) are two m\\times n matrices of same dimension, then A+B is defined as (a_{ij}+b_{ij})[cite: 28]. [cite_start]That is, we add element by element the two matrices[cite: 29]. [cite_start]Clearly A+B=B+A[cite: 29]. [cite_start]The rule applies to matrix subtraction[cite: 29].\n\n[cite_start]Transpose of a matrix: If A is a m\\times n matrix, the A' is the n\\times m matrix whose rows are the columns of A[cite: 30]. [cite_start]So A^{\\prime}=(a_{ji})[cite: 30]. For example:\n\n[cite_start][\\begin{matrix}2&-3\\\\ -4&5\\end{matrix}]^{\\prime}=[\\begin{matrix}2&-4\\\\ -3&5\\end{matrix}] [cite: 32]\n\n[cite_start]Note: (A+B)^{\\prime}=A^{\\prime}+B^{\\prime} [cite: 31]\n\n[cite_start]Note: (AB)^{\\prime}=B^{\\prime}A^{\\prime} [cite: 33]\n\n[cite_start]Null matrix: Has all elements as 0[cite: 34]. [cite_start]Clearly A+O_{m,n}=0_{m,n}+A=A for all m\\times n matrices[cite: 34].\n\n[cite_start]Scalar multiplication: If A=(a_{ij}) then for any constant k define kA=(ka_{ij})[cite: 35]. [cite_start]That is, multiply each element in A by k[cite: 35].\n\n[cite_start]Matrix multiplication: Say A is m\\times n and B is n\\times p then the m\\times p matrix AB is the product of A and B[cite: 36]. For example:\n\n[cite_start][\\begin{matrix}1&2\\\\ 4&0\\end{matrix}][\\begin{matrix}3&-1\\\\ 4&3\\end{matrix}]=[\\begin{matrix}11&5\\\\ 12&-4\\end{matrix}] [cite: 37]\n\n[cite_start]The matrices A and B are comformable[cite: 38].\n\n[cite_start]Diagonal matrix: A square n\\times n matrix A is diagonal if all entries off the ‘main diagonal’ are zero[cite: 39].\n[cite_start]A=[\\begin{matrix}a_{11}&0&...&0\\\\ 0&a_{22}&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&a_{nn}\\end{matrix}] [cite: 40]\n\n[cite_start]Note: The elements in the diagonal of A are the same as those in A'[cite: 41].\n\n[cite_start]Trace of a square matrix: If A is a square, the trace of A, denoted tr(A), is the sum of the elements on the main/leading diagonal of A[cite: 42, 43].\n\n[cite_start]Identity matrix: Denoted by I_{n}, the n\\times n diagonal matrix with a_{ij}=1 for all i[cite: 44, 46].\n[cite_start]A=[\\begin{matrix}1&0&...&0\\\\ 0&1&...&0\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\ 0&0&...&1\\end{matrix}] [cite: 45]\n\n[cite_start]Symmetric matrix: A square matrix A is symmetric if A=A^{\\prime}[cite: 47]. [cite_start]The identity matrix and the square null matrix are symmetric[cite: 47].\n\n[cite_start]Idempotent matrix: A square matrix is said to be idempotent if A^{2}=A[cite: 48].\n[cite_start]Example: A=[\\begin{matrix}2&-2&-4\\\\ -1&3&4\\\\ 1&-2&-3\\end{matrix}] [cite: 49]\n\n[cite_start]Singular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has a determinant of zero[cite: 50].\n\n[cite_start]Any two (or three) vectors x and y (and z) are linearly dependent iff one can be written as a scalar multiple of the other[cite: 51].\n\n[cite_start]Two vectors x\\ne0 and y\\ne0 are linearly independent iff the only solution to ax+by=0 is a=0 and b=0[cite: 52].\n\n[cite_start]x and y are said to be orthogonal[cite: 53].","type":"content","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"type":"lvl3","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#some-notes-on-matrices","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Some Notes on Matrices","lvl2":"Matrix Basics"},"content":"[cite_start]Usually AB\\ne BA[cite: 55].\n\n[cite_start]We can have AB=0 even if A or B are not zero[cite: 56].\n\n[cite_start]We can find that AB=AC even though B\\ne C[cite: 58].\n\n[cite_start]If A and B are singular matrices, AB will not be zero, although the product will be a singular matrix[cite: 59].","type":"content","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#some-notes-on-matrices","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#systems-of-equations-in-matrix-form","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Systems of Equations in Matrix Form"},"content":"[cite_start]One of the uses of linear algebra in economics is to represent and then solve systems of equations[cite: 61]. For instance:\n[cite_start]3x_{1}+4x_{2}=5 [cite: 68]\n[cite_start]7x_{1}-2x_{2}=2 [cite: 68]\n\n[cite_start]This can be defined as Ax=b[cite: 76], where:\n[cite_start]A = [\\begin{matrix}3&4\\\\ 7&-2\\end{matrix}], x=[\\begin{matrix}x_{1}\\\\ x_{2}\\end{matrix}], and b=[\\begin{matrix}5\\\\ 2\\end{matrix}] [cite: 71, 73, 136]","type":"content","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#systems-of-equations-in-matrix-form","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#matrix-inversion","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Matrix Inversion"},"content":"[cite_start]In fact, A^{-1}A=AA^{-1}=I[cite: 94]. [cite_start]To find the inverse of A=[\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}][cite: 79]:\n\n[cite_start]Find a matrix such that [\\begin{matrix}1&-2\\\\ 3&4\\end{matrix}][\\begin{matrix}a&b\\\\ c&d\\end{matrix}]=[\\begin{matrix}10&0\\\\ 0&10\\end{matrix}][cite: 81].\n\n[cite_start]The value 10 is the determinant of A[cite: 85].\n\n[cite_start]The resulting matrix is \\frac{1}{10}[\\begin{matrix}4&2\\\\ -3&1\\end{matrix}][cite: 90].","type":"content","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#matrix-inversion","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"type":"lvl3","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#properties-of-the-inverse","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Properties of the Inverse","lvl2":"Matrix Inversion"},"content":"[cite_start]Property 1: (A^{-1})^{-1}=A for any nonsingular matrix A[cite: 96, 97].\n\n[cite_start]Property 2: |A^{-1}|=\\frac{1}{|A|}[cite: 101].\n\n[cite_start]Property 3: The inverse of matrix A is unique[cite: 100].\n\n[cite_start]Property 4: (A^{\\prime})^{-1}=(A^{-1})^{\\prime}[cite: 103].\n\n[cite_start]Property 5: If A and B are nonsingular, AB is also nonsingular[cite: 104].\n\n[cite_start]Property 6: (AB)^{-1}=B^{-1}A^{-1}[cite: 106].","type":"content","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#properties-of-the-inverse","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"The Determinant of a Matrix"},"content":"[cite_start]The determinant is a scalar-value associated with a square non-singular matrix, denoted as |A|[cite: 108].\n\n[cite_start]For a 2\\times2 matrix A=[\\begin{matrix}a_{11}&a_{12}\\\\ a_{21}&a_{22}\\end{matrix}], |A|=a_{11}a_{22}-a_{12}a_{21}[cite: 113].\n\n[cite_start]The Laplace expansion is often used for higher orders[cite: 116].","type":"content","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#notes-on-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl3":"Notes on Determinants","lvl2":"The Determinant of a Matrix"},"content":"[cite_start]|A|=|A^{\\prime}| and |A|\\cdot|B|=|B|\\cdot|A|[cite: 122].\n\n[cite_start]Interchanging any two rows or columns changes the sign[cite: 123].\n\n[cite_start]Multiplying a single row/column by a scalar k multiplies the determinant by k[cite: 124].\n\n[cite_start]Adding a multiple of one row/column to another does not change the determinant[cite: 125].\n\n[cite_start]The determinant of a triangular matrix is the product of the principal diagonal elements[cite: 126].\n\n[cite_start]If any row or column equals zero, the determinant is zero[cite: 127].\n\n[cite_start]If two rows or columns are linearly dependent, the determinant is zero[cite: 128].","type":"content","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#notes-on-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"type":"lvl2","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#finding-the-inverse-of-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Finding the Inverse of Matrix"},"content":"[cite_start]The inverse is defined as: A^{-1}=\\frac{1}{|A|}adjA[cite: 132].\n\n[cite_start]Step 1: Find the minor |M_{ij}|, the determinant of the submatrix after deleting the ith row and jth column[cite: 138].\n\n[cite_start]Step 2: The cofactor |C_{ij}|=(-1)^{i+j}|M_{ij}|[cite: 141, 143].\n\n[cite_start]Step 3: The adjoint matrix is the transpose of the cofactor matrix[cite: 144].","type":"content","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#finding-the-inverse-of-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra","lvl2":"Cramer’s Rule"},"content":"[cite_start]A simplified method for solving Ax=b using determinants[cite: 151, 152].\n[cite_start]x_{i}=\\frac{|\\Delta_{i}|}{|\\Delta|} [cite: 158]\n[cite_start]Where \\Delta_{i} is the matrix A with the ith column replaced by vector b[cite: 157].","type":"content","url":"/linear-algebra-basic-e8b40c3bfcfdeb13ea864ea8b4c4f#cramers-rule","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"type":"lvl1","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts"},"content":"Linear algebra provides the language and tools for representing and solving\nsystems of equations, analyzing economic models, and studying optimization\nproblems. This chapter introduces the basic objects and operations that will\nbe used throughout the rest of these notes.\n\nThe emphasis is on intuition, structure, and economic relevance,\nrather than formal proofs.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"type":"lvl2","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#scalars-vectors-and-matrices","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Scalars, Vectors, and Matrices"},"content":"A scalar is a single number, such as income, price, or an interest rate.\n\nA vector is an ordered collection of numbers. For example, a vector of\noutputs from different sectors can be written asx =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}.\n\nA matrix is a rectangular array of numbers. For example,A =\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}.\n\nMatrices are used to represent:\n\nsystems of linear equations,\n\ntechnological relationships,\n\nlinear transformations.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#scalars-vectors-and-matrices","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"type":"lvl2","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#matrix-operations","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Matrix Operations"},"content":"","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#matrix-operations","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#matrix-addition-and-scalar-multiplication","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Addition and Scalar Multiplication","lvl2":"Matrix Operations"},"content":"Two matrices of the same dimension can be added element by element:A + B = [a_{ij} + b_{ij}].\n\nA matrix can be multiplied by a scalar c:cA = [c a_{ij}].\n\nThese operations behave much like addition and multiplication of numbers.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#matrix-addition-and-scalar-multiplication","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"type":"lvl3","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#matrix-multiplication","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Matrix Multiplication","lvl2":"Matrix Operations"},"content":"Matrix multiplication is defined so that dimensions must be compatible.\nIf A is an m \\times n matrix and B is an n \\times k matrix, then\nAB is an m \\times k matrix.\n\nThe (i,j) element of AB is given by(AB)_{ij} = \\sum_{k} a_{ik} b_{kj}.\n\nMatrix multiplication is not commutative in general:AB \\neq BA.\n\nThis non-commutativity is economically important: the order of\ntransformations matters.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#matrix-multiplication","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#transpose-of-a-matrix","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Transpose of a Matrix"},"content":"The transpose of a matrix A, denoted A', is obtained by swapping rows\nand columns:(A')_{ij} = a_{ji}.\n\nFor example,A =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix},\n\\qquad\nA' =\n\\begin{bmatrix}\n1 & 3 \\\\\n2 & 4\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#transpose-of-a-matrix","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#symmetric-matrices","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Symmetric Matrices","lvl2":"Transpose of a Matrix"},"content":"A matrix is symmetric ifA = A'.\n\nSymmetric matrices arise naturally in:\n\nquadratic forms,\n\nHessian matrices,\n\nsecond-order conditions in optimization.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#symmetric-matrices","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"type":"lvl2","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#identity-and-zero-matrices","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Identity and Zero Matrices"},"content":"The identity matrix I satisfiesAI = IA = A.\n\nFor example,I =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}.\n\nThe zero matrix has all elements equal to zero and acts as the additive\nidentity.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#identity-and-zero-matrices","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Inverse of a Matrix"},"content":"A square matrix A is invertible if there exists a matrix A^{-1} such\nthatAA^{-1} = A^{-1}A = I.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#interpretation","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Interpretation","lvl2":"Inverse of a Matrix"},"content":"If Ax = b, then multiplying both sides by A^{-1} yieldsx = A^{-1} b.\n\nThus, the inverse matrix allows us to solve systems of linear equations.\n\nNot all matrices are invertible. Non-invertibility typically reflects:\n\nredundancy,\n\nlinear dependence,\n\nthe absence of a unique solution.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#interpretation","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#determinant-of-a-matrix","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Determinant of a Matrix"},"content":"The determinant of a square matrix is a scalar value that summarizes key\nproperties of the matrix.\n\nFor a 2 \\times 2 matrix,A =\n\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix},\n\nthe determinant is given by\\det(A) = ad - bc.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#determinant-of-a-matrix","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#key-properties","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Key Properties","lvl2":"Determinant of a Matrix"},"content":"If \\det(A) = 0, the matrix is singular (not invertible).\n\nIf \\det(A) \\neq 0, the matrix is invertible.\n\nThe determinant reflects the volume-scaling effect of a linear\ntransformation.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#key-properties","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#cramers-rule","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule provides an explicit solution to a system of linear equationsAx = b,\n\nwhen A is invertible.\n\nFor each component x_i, the solution isx_i = \\frac{\\det(A_i)}{\\det(A)},\n\nwhere A_i is obtained by replacing the i-th column of A with b.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#cramers-rule","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"type":"lvl3","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#remarks","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl3":"Remarks","lvl2":"Cramer’s Rule"},"content":"Cramer’s Rule is mainly of theoretical interest.\n\nIt shows how solutions depend on determinants.\n\nIt is inefficient for large systems but conceptually illuminating.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#remarks","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"type":"lvl2","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#linear-dependence-and-rank-intuition","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Linear Dependence and Rank (Intuition)"},"content":"The columns of a matrix are linearly dependent if one column can be\nexpressed as a linear combination of the others.\n\nLinear dependence implies:\n\nloss of information,\n\nnon-invertibility,\n\nmultiple or no solutions.\n\nThe rank of a matrix measures the number of linearly independent rows or\ncolumns.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#linear-dependence-and-rank-intuition","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"type":"lvl2","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#why-linear-algebra-matters-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"Why Linear Algebra Matters in Economics"},"content":"Linear algebra underpins:\n\nequilibrium analysis,\n\ncomparative statics,\n\ninput–output models,\n\noptimization problems,\n\ndynamic systems.\n\nLater chapters will show how these basic tools are used to analyze:\n\nstability,\n\nconvexity,\n\noptimality conditions.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#why-linear-algebra-matters-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#what-comes-next","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Basic Concepts","lvl2":"What Comes Next"},"content":"In the next chapter, we extend these ideas to special matrices that play a\ncentral role in optimization and economic modeling, including:\n\nJacobian matrices,\n\nHessian matrices,\n\nquadratic forms and definiteness.","type":"content","url":"/linear-algebra-basic-f0b9d9ad8c67dc04c33772a74d213#what-comes-next","position":33},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices"},"type":"lvl1","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1","position":0},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices"},"content":"Let’s look at some special matrices and determinants that are useful in economic analysis.","type":"content","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1","position":1},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"type":"lvl2","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#the-jacobian","position":2},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"content":"The Jacobian (named after German mathematician Karl Gustav Jacobi, 1804–1851) is generally used in conjunction with partial derivatives to provide an easy test for the existence of functional dependence (linear and nonlinear). A Jacobian determinant |\\mathbf{J}| is composed of all the first‑order partial derivatives of a system of equations, arranged in order sequence. Giveny_{1} = f_{1}(x_{1},x_{2},x_{3}) \\\\\ny_{2} = f_{2}(x_{1},x_{2},x_{3}) \\\\\ny_{3} = f_{3}(x_{1},x_{2},x_{3})\n\nThen|\\mathbf{J}| = \n\\begin{vmatrix}\n\\frac{\\partial y_{1}}{\\partial x_{1}} & \\frac{\\partial y_{1}}{\\partial x_{2}} & \\frac{\\partial y_{1}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} & \\frac{\\partial y_{2}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{3}}{\\partial x_{1}} & \\frac{\\partial y_{3}}{\\partial x_{2}} & \\frac{\\partial y_{3}}{\\partial x_{3}}\n\\end{vmatrix}\n\nFor example, say you havey_{1} = 5x_{1} + 3x_{2} \\\\\ny_{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}\n\nThen taking the first‑order partials gives\\frac{\\partial y_1}{\\partial x_1} = 5,\\qquad \n\\frac{\\partial y_1}{\\partial x_2} = 3,\\qquad \n\\frac{\\partial y_2}{\\partial x_1} = 50x_1 + 30x_2,\\qquad \n\\frac{\\partial y_2}{\\partial x_2} = 30x_1 + 18x_2\n\nAnd the Jacobian is|\\mathbf{J}| = \n\\begin{vmatrix}\n5 & 3 \\\\\n50x_1 + 30x_2 & 30x_1 + 18x_2\n\\end{vmatrix}\n\nThe determinant of the Jacobian matrix is|\\mathbf{J}| = 5(30x_1 + 18x_2) - 3(50x_1 + 30x_2) = 0\n\nSince |\\mathbf{J}| = 0, there is functional dependence between the equations. (Note: (5x_{1} + 3x_{2})^{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}.)","type":"content","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#the-jacobian","position":3},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Hessian"},"type":"lvl2","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#the-hessian","position":4},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Hessian"},"content":"Given that the first‑order conditions z_{x} = z_{y} = 0 are met, a sufficient condition for a multivariate function z = f(x,y) to be an optimum is\n\nz_{xx}, z_{yy} > 0 for a minimum; z_{xx}, z_{yy} < 0 for a maximum.\n\nz_{xx} z_{yy} > (z_{xy})^{2}.\n\nA convenient test for this second‑order condition is provided by the Hessian. A Hessian |\\mathbf{H}| is a determinant composed of all the second‑order partial derivatives, with the second‑order direct partials on the principal diagonal and the second‑order cross partials off the principal diagonal. That is,|\\mathbf{H}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n\nwhere (by Young’s Theorem) z_{xy} = z_{yx}.\n\nIf the first element on the principal diagonal, a.k.a. the first principal minor, |H_{1}| = z_{xx} is positive, and the second principal minor|H_{2}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n= z_{xx} z_{yy} - (z_{xy})^{2} > 0,\n\nthen the second‑order conditions for a minimum are set. That is, when |H_{1}| > 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be positive definite. And a positive definite Hessian fulfills the second‑order conditions for a minimum.\n\nIf the first principal minor |H_{1}| < 0, while the second principal minor remains positive, then the second‑order conditions for a maximum are met. That is, when |H_{1}| < 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example:\n\nGivenz = f(x,y) = 2x^{2} - xy + 2y^{2} - 5x - 6y + 20,\n\nthenz_x = 4x - y - 5,\\quad z_y = -x + 4y - 6,\\quad z_{xx} = 4,\\quad z_{yy} = 4,\\quad z_{xy} = z_{yx} = -1.\n\nThe Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\n4 & -1 \\\\\n-1 & 4\n\\end{vmatrix}\n\nWe have |H_{1}| = 4 > 0 and |H_{2}| = (4)(4) - (-1)^2 = 16 - 1 = 15 > 0. Since |H_{1}| > 0 and |H_{2}| > 0, i.e. the Hessian is positive definite, then the function z is characterized by a minimum at the critical values (can you find these?).","type":"content","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#the-hessian","position":5},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"type":"lvl2","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#the-discriminant","position":6},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"content":"Determinants can be used to test for positive and negative definiteness of any quadratic form. The determinant of a quadratic form is called a discriminant |\\mathbf{D}|. Given the quadratic formz = a x^{2} + b x y + c y^{2},\n\nthe determinant is formed by placing the coefficients of the squared terms on the principal diagonal and dividing the coefficients of the non‑squared terms equally between the off‑diagonal positions. Hence, we have|\\mathbf{D}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n\nWe then evaluate the principal minors like we did for the Hessian test, where|D_{1}| = a \\quad \\text{and} \\quad |D_{2}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n= a c - \\frac{b^{2}}{4}.\n\nIf |D_{1}|, |D_{2}| > 0, |\\mathbf{D}| is positive definite and z is positive for all nonzero values of x and y. If |D_{1}| < 0 and |D_{2}| > 0, |\\mathbf{D}| is negative definite and z is negative for all nonzero values of x and y. If |D_{2}| \\neq 0, z is not sign definite and z may assume both positive and negative values.\n\nLet’s take an example to test for sign definiteness of the following quadratic formz = 2x^{2} + 5xy + 8y^{2}.\n\nWe can easily form the discriminant|\\mathbf{D}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n\nThen evaluating the principal minors gives|D_{1}| = 2 > 0, \\qquad |D_{2}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n= 16 - 6.25 = 9.75 > 0.\n\nHence, z is positive definite, meaning that it will be greater than zero for all nonzero values of x and y.","type":"content","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#the-discriminant","position":7},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"type":"lvl2","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#the-quadratic-form","position":8},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"content":"A quadratic form is defined as a polynomial expression in which each component term has a uniform degree.\n\nHere are some examples:6x^{2} - 2xy + 3y^{2} \\quad \\text{is a quadratic form in 2 variables.} \\\\\nx^{2} + 2xy + 4xz + 2yz + y^{2} + z^{2} \\quad \\text{is a quadratic form in 3 variables.}\n\nIt would be useful to determine the sign definiteness so we can make statements concerning the optimum value of the function as to whether it is a minimum or a maximum.\n\nMore generally, a quadratic form in n variables (x_{1},x_{2},\\ldots ,x_{n}) can be written as \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} where\n\n\\mathbf{x}^{\\prime} is a row vector [x_{1},x_{2},\\dots,x_{n}] and\\mathbf{A} is an n\\times n matrix of scalar elements.\n\nTo see this more clearly,\\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = \n\\begin{bmatrix}\nx_{1} & x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 3 \\\\\n4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= \n\\begin{bmatrix}\nx_{1} + 4x_{2} & 3x_{1} + 5x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= x_{1}^{2} + 7x_{1}x_{2} + 5x_{2}^{2}.\n\nA quadratic form is said to be:\n\na) Positive definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} > 0 for all \\mathbf{x} \\neq \\mathbf{0}.b) Positive semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\geq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.c) Negative definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} < 0 for all \\mathbf{x} \\neq \\mathbf{0}.d) Negative semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\leq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.e) Sign indefinite if it takes both positive and negative values.\n\nQuadratic forms are particularly useful in determining the concavity or convexity of a differentiable function.\n\nFor a function y = f(x_{1},x_{2},\\dots,x_{n}), we can form the Hessian consisting of second‑order partial derivatives and construct a quadratic form as \\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x}. Then\n\na) The function y is strictly convex if the quadratic form is positive (implying that the Hessian is positive definite, i.e., the determinants of the principal minors are all positive).b) The function y is strictly concave if the quadratic form is negative (that is, the Hessian is negative definite, i.e. the determinants of the principal minors alternate in sign).\n\nLet’s take an example to see this more clearly.\n\nSay we havez = x^{2} + y^{2}.\n\nThenz_{x} = 2x,\\quad z_{xx} = 2,\\quad z_{xy} = 0, \\\\\nz_{y} = 2y,\\quad z_{yx} = 0,\\quad z_{yy} = 2.\n\nBut\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 2x^{2} + 2y^{2} > 0.\n\nHence the function z = x^{2} + y^{2} is characterized by a minimum at its optimal value and is therefore a strictly convex function. And it is easy to see that the Hessian is positive definite!\n\nHere’s another example:\n\nSay we have a quadratic form in 2 variablesz = 8x^{2} + 6xy + 2y^{2}.\n\nThis can be rearranged asz = 8x^{2} + 3xy + 3xy + 2y^{2}.\n\nWe can write this as\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 8x^{2} + 2y^{2} + (3)(3)xy = 8x^{2} + 2y^{2} + 9xy.\n\nThe Hessian is\\mathbf{H} = \n\\begin{bmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{bmatrix}\n\nThe principal minors are |H_{1}| = 8 and $|H_{2}| =\\begin{vmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{vmatrix}\n\nBoth are positive so we have the Hessian and the quadratic form as positive definite.","type":"content","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#the-quadratic-form","position":9},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"type":"lvl2","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#higher-order-hessian","position":10},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"content":"Given y = f(x_{1},x_{2},x_{3}), the third‑order Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\ny_{11} & y_{12} & y_{13} \\\\\ny_{21} & y_{22} & y_{23} \\\\\ny_{31} & y_{32} & y_{33}\n\\end{vmatrix}\n\nwhere the elements are the various second‑order partial derivatives of y:y_{11} = \\frac{\\partial^{2}y}{\\partial x_{1}^{2}},\\quad \ny_{12} = \\frac{\\partial^{2}y}{\\partial x_{2}\\partial x_{1}},\\quad \ny_{23} = \\frac{\\partial^{2}y}{\\partial x_{3}\\partial x_{2}},\\quad \\text{etc.}\n\nConditions for a relative minimum or maximum depend on the signs of the first, second, and third principal minors, respectively. If|H_{1}| > 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| > 0,\n\nthen |\\mathbf{H}| is positive definite and fulfills the second‑order conditions for a minimum.\n\nIf|H_{1}| < 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| < 0,\n\nthen |\\mathbf{H}| is negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example.\n\nGiven the functiony = -5x_{1}^{2} + 10x_{1} + x_{1}x_{3} - 2x_{2}^{2} + 4x_{2} + 2x_{2}x_{3} - 4x_{3}^{2}.\n\nThe first order conditions (F.O.C) are\\frac{\\partial y}{\\partial x_1} = y_1 = -10x_1 + 10 + x_3 = 0, \\\\\n\\frac{\\partial y}{\\partial x_2} = y_2 = -4x_2 + 2x_3 + 4 = 0, \\\\\n\\frac{\\partial y}{\\partial x_3} = y_3 = x_1 + 2x_2 - 8x_3 = 0.\n\nwhich can be expressed in matrix form as\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-10 \\\\ -4 \\\\ 0\n\\end{bmatrix}.\n\nUsing Cramer’s rule we get x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43 (please verify this).\n\nLet’s take the second partial derivatives from the first‑order conditions to create the Hessian,y_{11} = -10,\\quad y_{12} = 0,\\quad y_{13} = 1, \\\\\ny_{21} = 0,\\quad y_{22} = -4,\\quad y_{23} = 2, \\\\\ny_{31} = 1,\\quad y_{32} = 2,\\quad y_{33} = -8.\n\nThus,\\mathbf{H} = \n\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\nFinally, applying the Hessian test, we have|H_1| = -10 < 0,\\quad \n|H_2| = \n\\begin{vmatrix}\n-10 & 0 \\\\\n0 & -4\n\\end{vmatrix} = 40 > 0,\\quad \n|H_3| = |\\mathbf{H}| = \n\\begin{vmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{vmatrix} = -80 < 0.\n\nSince the principal minors alternate correctly in sign, the Hessian is negative definite and the function is maximized at x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43.","type":"content","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#higher-order-hessian","position":11},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#the-bordered-hessian","position":12},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"content":"To optimize a function f(x_1, x_2) subject to a constraint g(x_1, x_2) the first‑order conditions can be found by setting up what is known as the Lagrangian function F(x_1, x_2, \\lambda) = f(x_1, x_2) - \\lambda(g(x_1, x_2) - c).\n\nThe second‑order conditions can be expressed in terms of a bordered Hessian |\\bar{\\mathbf{H}}| as|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & F_{11} & F_{12} \\\\\ng_2 & F_{21} & F_{22}\n\\end{vmatrix}\n\nor|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & f_{11} & f_{12} \\\\\ng_2 & f_{21} & f_{22}\n\\end{vmatrix}\n\nwhich is the usual Hessian bordered by the first derivatives of the constraint with zero on the principal diagonal.\n\nThe order of a bordered principal minor is determined by the order of the principal minor being bordered. Hence |\\bar{H}_2| above represents a second bordered principal minor, because the principal minor being bordered has dimensions 2\\times 2.\n\nFor the bivariate case with a single constraint, we simply look at |\\bar{H}_2|. If this is negative, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum. However, if it is positive, the bordered Hessian is said to be negative definite, and meets the sufficient conditions for a maximum.\n\nLet’s try optimizing the following objective functionf(x_1, x_2) = 4x_1^2 + 3x_2^2 - 2x_1x_2\n\nsubject tox_1 + x_2 = 56.\n\nSetting up the Lagrangian function, taking the first‑order partials and solving gives x_1^* = 36, x_2^* = 20 (and \\lambda = 348). (You should verify this.)\n\nThe bordered Hessian for this optimization problem is|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & 1 & 1 \\\\\n1 & 8 & -2 \\\\\n1 & -2 & 6\n\\end{vmatrix}\n\nStarting with the second principal minor, we have|\\bar{H}_2| = |\\bar{\\mathbf{H}}| = 0 \\cdot \n\\begin{vmatrix}\n8 & -2 \\\\\n-2 & 6\n\\end{vmatrix}\n- 1 \\cdot \n\\begin{vmatrix}\n1 & -2 \\\\\n1 & 6\n\\end{vmatrix}\n+ 1 \\cdot \n\\begin{vmatrix}\n1 & 8 \\\\\n1 & -2\n\\end{vmatrix}\n= - (6+2) + (-2-8) = -8 -10 = -18.\n\nwhich is negative, hence |\\bar{\\mathbf{H}}| is positive definite and we have met sufficient conditions for a minimum.\n\nFor the more general case in which the objective function has say n variables, i.e., f(x_1, \\ldots , x_n) which is subject to some constraint g(x_1, \\ldots , x_n), we can set up the bordered Hessian as|\\bar{\\mathbf{H}}| = \n\\begin{bmatrix}\n0 & g_1 & g_2 & \\dots & g_n \\\\\ng_1 & F_{11} & F_{12} & \\dots & F_{1n} \\\\\ng_2 & F_{21} & F_{22} & \\dots & F_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ng_n & F_{n1} & F_{n2} & \\dots & F_{nn}\n\\end{bmatrix}\n\nwhere |\\bar{\\mathbf{H}}| = |\\bar{H}_n| because the n \\times n principal minor is bordered.\n\nIn this case, if all the principal minors are negative, i.e. |\\bar{H}_2|, |\\bar{H}_3|, \\ldots , |\\bar{H}_n| < 0, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum.\n\nOn the other hand, if the principal minors alternate consistently in sign from positive to negative, i.e. |\\bar{H}_2| > 0, |\\bar{H}_3| < 0, |\\bar{H}_4| > 0 etc., the bordered Hessian is negative definite, and meets the sufficient conditions for a maximum.","type":"content","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#the-bordered-hessian","position":13},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"type":"lvl2","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#input-output-analysis","position":14},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"content":"If a_{ij} is a technical coefficient representing the value of input i required to produce one dollar’s worth of product j, the total demand for good i can be expressed asx_{i} = a_{i1}x_{1} + a_{i2}x_{2} + \\dots + a_{in}x_{n} + b_{i}\n\nwhere b_i is the final demand for product i. What is important to realize here is that the total demand for a product consists of that product being the final demand plus that product being an intermediate good required for the production of other products.\n\nIn matrix form we have\\mathbf{X} = \\mathbf{A}\\mathbf{X} + \\mathbf{B}\n\nwhere, for an n sector economy,\\mathbf{X} = \n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n\\quad\n\\mathbf{A} = \n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nn}\n\\end{bmatrix},\n\\quad\n\\mathbf{B} = \n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n\\end{bmatrix}.\n\n\\mathbf{A} is called the matrix of technical coefficients.\n\nTo find the output (intermediate and final goods) needed to satisfy demand, all we have to do is to solve for \\mathbf{X}:\\mathbf{X} - \\mathbf{A}\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad (\\mathbf{I} - \\mathbf{A})\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad \\mathbf{X} = (\\mathbf{I} - \\mathbf{A})^{-1}\\mathbf{B}.","type":"content","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#input-output-analysis","position":15},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"type":"lvl2","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":16},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"content":"For a square matrix \\mathbf{A}:\n\na) All characteristic roots \\lambda are positive \\Rightarrow \\mathbf{A} is positive definite.b) All \\lambda’s are negative \\Rightarrow \\mathbf{A} is negative definite.c) All \\lambda’s are nonnegative and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is positive semi‑definite.d) All \\lambda’s are nonpositive and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is negative semi‑definite.e) Some \\lambda’s are positive and some negative \\Rightarrow \\mathbf{A} is sign indefinite.\n\nLet’s take an example.\n\nGiven a square matrix\\mathbf{A} = \n\\begin{bmatrix}\n-6 & 3 \\\\\n3 & -6\n\\end{bmatrix}\n\nTo find the characteristic roots (eigenvalues) of \\mathbf{A}, we simply set |\\mathbf{A} - \\lambda \\mathbf{I}| = 0:|\\mathbf{A} - \\lambda \\mathbf{I}| = \n\\begin{vmatrix}\n-6 - \\lambda & 3 \\\\\n3 & -6 - \\lambda\n\\end{vmatrix} = 0\n\nThis means(-6 - \\lambda)(-6 - \\lambda) - 9 = 0 \\\\\n\\lambda^2 + 12\\lambda + 27 = 0 \\\\\n(\\lambda + 9)(\\lambda + 3) = 0 \\\\\n\\lambda = -9, -3.\n\nSince both characteristic roots \\lambda are negative, we say \\mathbf{A} is negative definite.\n\nNote:\\text{(i) } \\sum \\lambda_i = \\operatorname{tr}(\\mathbf{A}), \\qquad\n\\text{(ii) } \\prod \\lambda_i = |\\mathbf{A}|.\n\nLet’s continue with the example above to find the characteristic vector.\n\nWe know one of the roots \\lambda = -9, so substituting in the characteristic matrix gives(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{bmatrix}\n-6 - (-9) & 3 \\\\\n3 & -6 - (-9)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n3 & 3 \\\\\n3 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nSince the coefficient matrix is linearly dependent, there are infinite number of solutions. The product of the matrices gives two equations which are identical:3v_1 + 3v_2 = 0 \\quad \\Rightarrow \\quad v_2 = -v_1.\n\nBy normalizing we havev_1^{2} + v_2^{2} = 1.\n\nSubstituting v_2 = -v_1 givesv_1^{2} + (-v_1)^{2} = 1 \\quad \\Rightarrow \\quad 2v_1^{2} = 1 \\quad \\Rightarrow \\quad v_1^{2} = \\frac{1}{2}.\n\nTaking the positive square root gives v_1 = \\sqrt{1/2} = \\frac{\\sqrt{2}}{2} and substituting into v_2 = -v_1 gives v_2 = -\\frac{\\sqrt{2}}{2}. That is\\mathbf{v}_1 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n-\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.\n\nUsing the second characteristic root \\lambda = -3:(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \n\\begin{bmatrix}\n-6 - (-3) & 3 \\\\\n3 & -6 - (-3)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-3 & 3 \\\\\n3 & -3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nMultiplying out gives -3v_1 + 3v_2 = 0 and 3v_1 - 3v_2 = 0, so v_1 = v_2.\n\nNormalizing as before:v_1^2 + v_2^2 = 1 \\quad \\Rightarrow \\quad 2v_1^2 = 1 \\quad \\Rightarrow \\quad v_1 = \\frac{\\sqrt{2}}{2}.\n\nHence,\\mathbf{v}_2 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-speci-0ac43389248427605f3cac9a745f1#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":17},{"hierarchy":{"lvl1":"3. Special Matrices"},"type":"lvl1","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339","position":0},{"hierarchy":{"lvl1":"3. Special Matrices"},"content":"Let’s look at some special matrices and determinants that are useful in economic analysis.","type":"content","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339","position":1},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Jacobian"},"type":"lvl2","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#the-jacobian","position":2},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Jacobian"},"content":"The Jacobian (named after German mathematician Karl Gustav Jacobi, 1804–1851) is generally used in conjunction with partial derivatives to provide an easy test for the existence of functional dependence (linear and nonlinear). A Jacobian determinant |\\mathbf{J}| is composed of all the first‑order partial derivatives of a system of equations, arranged in order sequence. Giveny_{1} = f_{1}(x_{1},x_{2},x_{3}) \\\\\ny_{2} = f_{2}(x_{1},x_{2},x_{3}) \\\\\ny_{3} = f_{3}(x_{1},x_{2},x_{3})\n\nThen|\\mathbf{J}| = \n\\begin{vmatrix}\n\\frac{\\partial y_{1}}{\\partial x_{1}} & \\frac{\\partial y_{1}}{\\partial x_{2}} & \\frac{\\partial y_{1}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} & \\frac{\\partial y_{2}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{3}}{\\partial x_{1}} & \\frac{\\partial y_{3}}{\\partial x_{2}} & \\frac{\\partial y_{3}}{\\partial x_{3}}\n\\end{vmatrix}\n\nFor example, say you havey_{1} = 5x_{1} + 3x_{2} \\\\\ny_{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}\n\nThen taking the first‑order partials gives\\frac{\\partial y_1}{\\partial x_1} = 5,\\qquad \n\\frac{\\partial y_1}{\\partial x_2} = 3,\\qquad \n\\frac{\\partial y_2}{\\partial x_1} = 50x_1 + 30x_2,\\qquad \n\\frac{\\partial y_2}{\\partial x_2} = 30x_1 + 18x_2\n\nAnd the Jacobian is|\\mathbf{J}| = \n\\begin{vmatrix}\n5 & 3 \\\\\n50x_1 + 30x_2 & 30x_1 + 18x_2\n\\end{vmatrix}\n\nThe determinant of the Jacobian matrix is|\\mathbf{J}| = 5(30x_1 + 18x_2) - 3(50x_1 + 30x_2) = 0\n\nSince |\\mathbf{J}| = 0, there is functional dependence between the equations. (Note: (5x_{1} + 3x_{2})^{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}.)","type":"content","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#the-jacobian","position":3},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Hessian"},"type":"lvl2","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#the-hessian","position":4},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Hessian"},"content":"Given that the first‑order conditions z_{x} = z_{y} = 0 are met, a sufficient condition for a multivariate function z = f(x,y) to be an optimum is\n\nz_{xx}, z_{yy} > 0 for a minimum; z_{xx}, z_{yy} < 0 for a maximum.\n\nz_{xx} z_{yy} > (z_{xy})^{2}.\n\nA convenient test for this second‑order condition is provided by the Hessian. A Hessian |\\mathbf{H}| is a determinant composed of all the second‑order partial derivatives, with the second‑order direct partials on the principal diagonal and the second‑order cross partials off the principal diagonal. That is,|\\mathbf{H}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n\nwhere (by Young’s Theorem) z_{xy} = z_{yx}.\n\nIf the first element on the principal diagonal, a.k.a. the first principal minor, |H_{1}| = z_{xx} is positive, and the second principal minor|H_{2}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n= z_{xx} z_{yy} - (z_{xy})^{2} > 0,\n\nthen the second‑order conditions for a minimum are set. That is, when |H_{1}| > 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be positive definite. And a positive definite Hessian fulfills the second‑order conditions for a minimum.\n\nIf the first principal minor |H_{1}| < 0, while the second principal minor remains positive, then the second‑order conditions for a maximum are met. That is, when |H_{1}| < 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example:\n\nGivenz = f(x,y) = 2x^{2} - xy + 2y^{2} - 5x - 6y + 20,\n\nthenz_x = 4x - y - 5,\\quad z_y = -x + 4y - 6,\\quad z_{xx} = 4,\\quad z_{yy} = 4,\\quad z_{xy} = z_{yx} = -1.\n\nThe Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\n4 & -1 \\\\\n-1 & 4\n\\end{vmatrix}\n\nWe have |H_{1}| = 4 > 0 and |H_{2}| = (4)(4) - (-1)^2 = 16 - 1 = 15 > 0. Since |H_{1}| > 0 and |H_{2}| > 0, i.e. the Hessian is positive definite, then the function z is characterized by a minimum at the critical values (can you find these?).","type":"content","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#the-hessian","position":5},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Discriminant"},"type":"lvl2","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#the-discriminant","position":6},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Discriminant"},"content":"Determinants can be used to test for positive and negative definiteness of any quadratic form. The determinant of a quadratic form is called a discriminant |\\mathbf{D}|. Given the quadratic formz = a x^{2} + b x y + c y^{2},\n\nthe determinant is formed by placing the coefficients of the squared terms on the principal diagonal and dividing the coefficients of the non‑squared terms equally between the off‑diagonal positions. Hence, we have|\\mathbf{D}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n\nWe then evaluate the principal minors like we did for the Hessian test, where|D_{1}| = a \\quad \\text{and} \\quad |D_{2}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n= a c - \\frac{b^{2}}{4}.\n\nIf |D_{1}|, |D_{2}| > 0, |\\mathbf{D}| is positive definite and z is positive for all nonzero values of x and y. If |D_{1}| < 0 and |D_{2}| > 0, |\\mathbf{D}| is negative definite and z is negative for all nonzero values of x and y. If |D_{2}| \\neq 0, z is not sign definite and z may assume both positive and negative values.\n\nLet’s take an example to test for sign definiteness of the following quadratic formz = 2x^{2} + 5xy + 8y^{2}.\n\nWe can easily form the discriminant|\\mathbf{D}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n\nThen evaluating the principal minors gives|D_{1}| = 2 > 0, \\qquad |D_{2}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n= 16 - 6.25 = 9.75 > 0.\n\nHence, z is positive definite, meaning that it will be greater than zero for all nonzero values of x and y.","type":"content","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#the-discriminant","position":7},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Quadratic Form"},"type":"lvl2","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#the-quadratic-form","position":8},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Quadratic Form"},"content":"A quadratic form is defined as a polynomial expression in which each component term has a uniform degree.\n\nHere are some examples:6x^{2} - 2xy + 3y^{2} \\quad \\text{is a quadratic form in 2 variables.} \\\\\nx^{2} + 2xy + 4xz + 2yz + y^{2} + z^{2} \\quad \\text{is a quadratic form in 3 variables.}\n\nIt would be useful to determine the sign definiteness so we can make statements concerning the optimum value of the function as to whether it is a minimum or a maximum.\n\nMore generally, a quadratic form in n variables (x_{1},x_{2},\\ldots ,x_{n}) can be written as \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} where\n\n\\mathbf{x}^{\\prime} is a row vector [x_{1},x_{2},\\dots,x_{n}] and\\mathbf{A} is an n\\times n matrix of scalar elements.\n\nTo see this more clearly,\\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = \n\\begin{bmatrix}\nx_{1} & x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 3 \\\\\n4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= \n\\begin{bmatrix}\nx_{1} + 4x_{2} & 3x_{1} + 5x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= x_{1}^{2} + 7x_{1}x_{2} + 5x_{2}^{2}.\n\nA quadratic form is said to be:\n\na) Positive definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} > 0 for all \\mathbf{x} \\neq \\mathbf{0}.b) Positive semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\geq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.c) Negative definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} < 0 for all \\mathbf{x} \\neq \\mathbf{0}.d) Negative semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\leq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.e) Sign indefinite if it takes both positive and negative values.\n\nQuadratic forms are particularly useful in determining the concavity or convexity of a differentiable function.\n\nFor a function y = f(x_{1},x_{2},\\dots,x_{n}), we can form the Hessian consisting of second‑order partial derivatives and construct a quadratic form as \\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x}. Then\n\na) The function y is strictly convex if the quadratic form is positive (implying that the Hessian is positive definite, i.e., the determinants of the principal minors are all positive).b) The function y is strictly concave if the quadratic form is negative (that is, the Hessian is negative definite, i.e. the determinants of the principal minors alternate in sign).\n\nLet’s take an example to see this more clearly.\n\nSay we havez = x^{2} + y^{2}.\n\nThenz_{x} = 2x,\\quad z_{xx} = 2,\\quad z_{xy} = 0, \\\\\nz_{y} = 2y,\\quad z_{yx} = 0,\\quad z_{yy} = 2.\n\nBut\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 2x^{2} + 2y^{2} > 0.\n\nHence the function z = x^{2} + y^{2} is characterized by a minimum at its optimal value and is therefore a strictly convex function. And it is easy to see that the Hessian is positive definite!\n\nHere’s another example:\n\nSay we have a quadratic form in 2 variablesz = 8x^{2} + 6xy + 2y^{2}.\n\nThis can be rearranged asz = 8x^{2} + 3xy + 3xy + 2y^{2}.\n\nWe can write this as\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 8x^{2} + 2y^{2} + (3)(3)xy = 8x^{2} + 2y^{2} + 9xy.\n\nThe Hessian is\\mathbf{H} = \n\\begin{bmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{bmatrix}\n\nThe principal minors are |H_{1}| = 8 and $|H_{2}| =\\begin{vmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{vmatrix}\n\nBoth are positive so we have the Hessian and the quadratic form as positive definite.","type":"content","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#the-quadratic-form","position":9},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Higher Order Hessian"},"type":"lvl2","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#higher-order-hessian","position":10},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Higher Order Hessian"},"content":"Given y = f(x_{1},x_{2},x_{3}), the third‑order Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\ny_{11} & y_{12} & y_{13} \\\\\ny_{21} & y_{22} & y_{23} \\\\\ny_{31} & y_{32} & y_{33}\n\\end{vmatrix}\n\nwhere the elements are the various second‑order partial derivatives of y:y_{11} = \\frac{\\partial^{2}y}{\\partial x_{1}^{2}},\\quad \ny_{12} = \\frac{\\partial^{2}y}{\\partial x_{2}\\partial x_{1}},\\quad \ny_{23} = \\frac{\\partial^{2}y}{\\partial x_{3}\\partial x_{2}},\\quad \\text{etc.}\n\nConditions for a relative minimum or maximum depend on the signs of the first, second, and third principal minors, respectively. If|H_{1}| > 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| > 0,\n\nthen |\\mathbf{H}| is positive definite and fulfills the second‑order conditions for a minimum.\n\nIf|H_{1}| < 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| < 0,\n\nthen |\\mathbf{H}| is negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example.\n\nGiven the functiony = -5x_{1}^{2} + 10x_{1} + x_{1}x_{3} - 2x_{2}^{2} + 4x_{2} + 2x_{2}x_{3} - 4x_{3}^{2}.\n\nThe first order conditions (F.O.C) are\\frac{\\partial y}{\\partial x_1} = y_1 = -10x_1 + 10 + x_3 = 0, \\\\\n\\frac{\\partial y}{\\partial x_2} = y_2 = -4x_2 + 2x_3 + 4 = 0, \\\\\n\\frac{\\partial y}{\\partial x_3} = y_3 = x_1 + 2x_2 - 8x_3 = 0.\n\nwhich can be expressed in matrix form as\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-10 \\\\ -4 \\\\ 0\n\\end{bmatrix}.\n\nUsing Cramer’s rule we get x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43 (please verify this).\n\nLet’s take the second partial derivatives from the first‑order conditions to create the Hessian,y_{11} = -10,\\quad y_{12} = 0,\\quad y_{13} = 1, \\\\\ny_{21} = 0,\\quad y_{22} = -4,\\quad y_{23} = 2, \\\\\ny_{31} = 1,\\quad y_{32} = 2,\\quad y_{33} = -8.\n\nThus,\\mathbf{H} = \n\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\nFinally, applying the Hessian test, we have|H_1| = -10 < 0,\\quad \n|H_2| = \n\\begin{vmatrix}\n-10 & 0 \\\\\n0 & -4\n\\end{vmatrix} = 40 > 0,\\quad \n|H_3| = |\\mathbf{H}| = \n\\begin{vmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{vmatrix} = -80 < 0.\n\nSince the principal minors alternate correctly in sign, the Hessian is negative definite and the function is maximized at x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43.","type":"content","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#higher-order-hessian","position":11},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#the-bordered-hessian","position":12},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Bordered Hessian"},"content":"To optimize a function f(x_1, x_2) subject to a constraint g(x_1, x_2) the first‑order conditions can be found by setting up what is known as the Lagrangian function F(x_1, x_2, \\lambda) = f(x_1, x_2) - \\lambda(g(x_1, x_2) - c).\n\nThe second‑order conditions can be expressed in terms of a bordered Hessian |\\bar{\\mathbf{H}}| as|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & F_{11} & F_{12} \\\\\ng_2 & F_{21} & F_{22}\n\\end{vmatrix}\n\nor|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & f_{11} & f_{12} \\\\\ng_2 & f_{21} & f_{22}\n\\end{vmatrix}\n\nwhich is the usual Hessian bordered by the first derivatives of the constraint with zero on the principal diagonal.\n\nThe order of a bordered principal minor is determined by the order of the principal minor being bordered. Hence |\\bar{H}_2| above represents a second bordered principal minor, because the principal minor being bordered has dimensions 2\\times 2.\n\nFor the bivariate case with a single constraint, we simply look at |\\bar{H}_2|. If this is negative, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum. However, if it is positive, the bordered Hessian is said to be negative definite, and meets the sufficient conditions for a maximum.\n\nLet’s try optimizing the following objective functionf(x_1, x_2) = 4x_1^2 + 3x_2^2 - 2x_1x_2\n\nsubject tox_1 + x_2 = 56.\n\nSetting up the Lagrangian function, taking the first‑order partials and solving gives x_1^* = 36, x_2^* = 20 (and \\lambda = 348). (You should verify this.)\n\nThe bordered Hessian for this optimization problem is|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & 1 & 1 \\\\\n1 & 8 & -2 \\\\\n1 & -2 & 6\n\\end{vmatrix}\n\nStarting with the second principal minor, we have|\\bar{H}_2| = |\\bar{\\mathbf{H}}| = 0 \\cdot \n\\begin{vmatrix}\n8 & -2 \\\\\n-2 & 6\n\\end{vmatrix}\n- 1 \\cdot \n\\begin{vmatrix}\n1 & -2 \\\\\n1 & 6\n\\end{vmatrix}\n+ 1 \\cdot \n\\begin{vmatrix}\n1 & 8 \\\\\n1 & -2\n\\end{vmatrix}\n= - (6+2) + (-2-8) = -8 -10 = -18.\n\nwhich is negative, hence |\\bar{\\mathbf{H}}| is positive definite and we have met sufficient conditions for a minimum.\n\nFor the more general case in which the objective function has say n variables, i.e., f(x_1, \\ldots , x_n) which is subject to some constraint g(x_1, \\ldots , x_n), we can set up the bordered Hessian as|\\bar{\\mathbf{H}}| = \n\\begin{bmatrix}\n0 & g_1 & g_2 & \\dots & g_n \\\\\ng_1 & F_{11} & F_{12} & \\dots & F_{1n} \\\\\ng_2 & F_{21} & F_{22} & \\dots & F_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ng_n & F_{n1} & F_{n2} & \\dots & F_{nn}\n\\end{bmatrix}\n\nwhere |\\bar{\\mathbf{H}}| = |\\bar{H}_n| because the n \\times n principal minor is bordered.\n\nIn this case, if all the principal minors are negative, i.e. |\\bar{H}_2|, |\\bar{H}_3|, \\ldots , |\\bar{H}_n| < 0, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum.\n\nOn the other hand, if the principal minors alternate consistently in sign from positive to negative, i.e. |\\bar{H}_2| > 0, |\\bar{H}_3| < 0, |\\bar{H}_4| > 0 etc., the bordered Hessian is negative definite, and meets the sufficient conditions for a maximum.","type":"content","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#the-bordered-hessian","position":13},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Input‑Output Analysis"},"type":"lvl2","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#input-output-analysis","position":14},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Input‑Output Analysis"},"content":"If a_{ij} is a technical coefficient representing the value of input i required to produce one dollar’s worth of product j, the total demand for good i can be expressed asx_{i} = a_{i1}x_{1} + a_{i2}x_{2} + \\dots + a_{in}x_{n} + b_{i}\n\nwhere b_i is the final demand for product i. What is important to realize here is that the total demand for a product consists of that product being the final demand plus that product being an intermediate good required for the production of other products.\n\nIn matrix form we have\\mathbf{X} = \\mathbf{A}\\mathbf{X} + \\mathbf{B}\n\nwhere, for an n sector economy,\\mathbf{X} = \n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n\\quad\n\\mathbf{A} = \n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nn}\n\\end{bmatrix},\n\\quad\n\\mathbf{B} = \n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n\\end{bmatrix}.\n\n\\mathbf{A} is called the matrix of technical coefficients.\n\nTo find the output (intermediate and final goods) needed to satisfy demand, all we have to do is to solve for \\mathbf{X}:\\mathbf{X} - \\mathbf{A}\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad (\\mathbf{I} - \\mathbf{A})\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad \\mathbf{X} = (\\mathbf{I} - \\mathbf{A})^{-1}\\mathbf{B}.","type":"content","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#input-output-analysis","position":15},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"type":"lvl2","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":16},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"content":"For a square matrix \\mathbf{A}:\n\na) All characteristic roots \\lambda are positive \\Rightarrow \\mathbf{A} is positive definite.b) All \\lambda’s are negative \\Rightarrow \\mathbf{A} is negative definite.c) All \\lambda’s are nonnegative and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is positive semi‑definite.d) All \\lambda’s are nonpositive and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is negative semi‑definite.e) Some \\lambda’s are positive and some negative \\Rightarrow \\mathbf{A} is sign indefinite.\n\nLet’s take an example.\n\nGiven a square matrix\\mathbf{A} = \n\\begin{bmatrix}\n-6 & 3 \\\\\n3 & -6\n\\end{bmatrix}\n\nTo find the characteristic roots (eigenvalues) of \\mathbf{A}, we simply set |\\mathbf{A} - \\lambda \\mathbf{I}| = 0:|\\mathbf{A} - \\lambda \\mathbf{I}| = \n\\begin{vmatrix}\n-6 - \\lambda & 3 \\\\\n3 & -6 - \\lambda\n\\end{vmatrix} = 0\n\nThis means(-6 - \\lambda)(-6 - \\lambda) - 9 = 0 \\\\\n\\lambda^2 + 12\\lambda + 27 = 0 \\\\\n(\\lambda + 9)(\\lambda + 3) = 0 \\\\\n\\lambda = -9, -3.\n\nSince both characteristic roots \\lambda are negative, we say \\mathbf{A} is negative definite.\n\nNote:\\text{(i) } \\sum \\lambda_i = \\operatorname{tr}(\\mathbf{A}), \\qquad\n\\text{(ii) } \\prod \\lambda_i = |\\mathbf{A}|.\n\nLet’s continue with the example above to find the characteristic vector.\n\nWe know one of the roots \\lambda = -9, so substituting in the characteristic matrix gives(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{bmatrix}\n-6 - (-9) & 3 \\\\\n3 & -6 - (-9)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n3 & 3 \\\\\n3 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nSince the coefficient matrix is linearly dependent, there are infinite number of solutions. The product of the matrices gives two equations which are identical:3v_1 + 3v_2 = 0 \\quad \\Rightarrow \\quad v_2 = -v_1.\n\nBy normalizing we havev_1^{2} + v_2^{2} = 1.\n\nSubstituting v_2 = -v_1 givesv_1^{2} + (-v_1)^{2} = 1 \\quad \\Rightarrow \\quad 2v_1^{2} = 1 \\quad \\Rightarrow \\quad v_1^{2} = \\frac{1}{2}.\n\nTaking the positive square root gives v_1 = \\sqrt{1/2} = \\frac{\\sqrt{2}}{2} and substituting into v_2 = -v_1 gives v_2 = -\\frac{\\sqrt{2}}{2}. That is\\mathbf{v}_1 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n-\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.\n\nUsing the second characteristic root \\lambda = -3:(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \n\\begin{bmatrix}\n-6 - (-3) & 3 \\\\\n3 & -6 - (-3)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-3 & 3 \\\\\n3 & -3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nMultiplying out gives -3v_1 + 3v_2 = 0 and 3v_1 - 3v_2 = 0, so v_1 = v_2.\n\nNormalizing as before:v_1^2 + v_2^2 = 1 \\quad \\Rightarrow \\quad 2v_1^2 = 1 \\quad \\Rightarrow \\quad v_1 = \\frac{\\sqrt{2}}{2}.\n\nHence,\\mathbf{v}_2 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-speci-2749fb587ed7db3f8c249b6150339#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"type":"lvl1","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines optimality.\n\nThe focus is on interpretation and application, rather than abstract theory.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#the-jacobian-matrix","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"content":"Consider a vector-valued function:\n$$\nf(x) =\\begin{bmatrix}\nf_1(x_1,\\dots,x_n) \\\\\n\\vdots \\\\\nf_m(x_1,\\dots,x_n)\n\\end{bmatrix}\n\n$$\n\nThe Jacobian matrix of f is the matrix of first-order partial derivatives:\n$$\nJ(x) =\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#the-jacobian-matrix","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#interpretation","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"content":"Each row shows how one equation responds to changes in variables.\n\nThe Jacobian generalizes the derivative to multivariate settings.\n\nIn economics, Jacobians appear in:\n\nsystems of equations,\n\nequilibrium conditions,\n\ncomparative statics.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#interpretation","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#the-hessian-matrix","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"content":"For a scalar-valued function f(x_1,\\dots,x_n), the Hessian matrix collects\nsecond-order partial derivatives:\n$$\nH(x) =\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#the-hessian-matrix","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#properties","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"content":"Under mild regularity conditions, the Hessian is symmetric.\n\nThe Hessian summarizes the curvature of a function.\n\nIt plays a crucial role in second-order conditions for optimization.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#properties","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"type":"lvl2","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#quadratic-forms","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"content":"Given a symmetric matrix A and a vector x, the expressionx' A x\n\nis called a quadratic form.\n\nQuadratic forms arise naturally when:\n\napproximating functions locally,\n\nanalyzing curvature,\n\nstudying stability.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#quadratic-forms","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"type":"lvl3","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#example","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"content":"If\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}\n\n\\quad\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}then\n\nx’Ax = a x_1^2 + 2b x_1 x_2 + c x_2^2.\n$$","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#example","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"type":"lvl2","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#definiteness-of-a-matrix","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"content":"A symmetric matrix A is:\n\nPositive definite if x'Ax > 0 for all x \\neq 0\n\nNegative definite if x'Ax < 0 for all x \\neq 0\n\nIndefinite if x'Ax takes both positive and negative values","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#definiteness-of-a-matrix","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"type":"lvl3","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#economic-meaning","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"content":"Positive definiteness corresponds to convexity.\n\nNegative definiteness corresponds to concavity.\n\nThese properties determine whether a critical point is a minimum or maximum.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#economic-meaning","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl2","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#principal-minors-and-the-determinant-test","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"content":"For a symmetric matrix, definiteness can be checked using principal minors.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#principal-minors-and-the-determinant-test","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#example-2-2-case","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"content":"Let\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{pbatrix}.\n$$\n\n- $A$ is positive definite if:\n  $$\n  a > 0, \\quad \\det(A) = ac - b^2 > 0.\n  $$\n\nThis test is especially useful in applied work, where symbolic eigenvalues\nmay be difficult to compute.\n\n---\n\n## Eigenvalues and Definiteness\n\nAn alternative characterization uses **eigenvalues**.\n\nA symmetric matrix is:\n- positive definite if all eigenvalues are positive,\n- negative definite if all eigenvalues are negative.\n\nEigenvalues provide geometric intuition: they describe how a quadratic form\nstretches space along principal directions.\n\n---\n\n## The Bordered Hessian\n\nWhen optimizing a function subject to constraints, the **bordered Hessian**\nis used to check second-order conditions.\n\n### Structure\n\nFor a problem with one constraint, the bordered Hessian takes the form:\n$$\nH_B =\n\\begin{bmatrix}\n0 & g_x' \\\\\ng_x & H\n\\end{bmatrix}\n\n$$\nwhere:\n\ng_x is the gradient of the constraint,\n\nH is the Hessian of the Lagrangian.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#example-2-2-case","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#interpretation-1","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"Principal Minors and the Determinant Test"},"content":"The bordered Hessian incorporates both curvature and constraints.\n\nThe sign pattern of its principal minors determines optimality.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#interpretation-1","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"type":"lvl2","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#input-output-matrices-in-economics","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"content":"Linear algebra is central to input–output analysis.\n\nLet A be the matrix of technical coefficients, and let x denote output.\nThe system:x = Ax + d\n\ncan be rewritten as:(I - A)x = d.\n\nIf (I - A) is invertible, the solution is:x = (I - A)^{-1} d.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#input-output-matrices-in-economics","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"type":"lvl3","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#economic-meaning-1","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"content":"(I - A)^{-1} is the Leontief inverse.\n\nIt captures direct and indirect production requirements.\n\nInvertibility reflects feasibility and stability of production systems.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#economic-meaning-1","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"type":"lvl2","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#why-these-matrices-matter","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"content":"Special matrices allow us to:\n\ncharacterize equilibrium behavior,\n\ndetermine optimality,\n\nanalyze stability,\n\nunderstand economic structure.\n\nThey form the mathematical backbone of:\n\nconstrained optimization,\n\ndynamic systems,\n\ngeneral equilibrium models.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#why-these-matrices-matter","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#what-comes-next","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"content":"With these tools in place, we are ready to move beyond linear algebra and\nintroduce calculus-based methods, beginning with multivariate calculus\nand optimization.","type":"content","url":"/linear-algebra-speci-2779c89eba0cca06b5793e668ca20#what-comes-next","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"type":"lvl1","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines\noptimality.\n\nThe focus is on interpretation and application, rather than abstract\ntheory.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#the-jacobian-matrix","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"content":"Consider a vector-valued function\n$$\nf(x) =\\begin{bmatrix}\nf_1(x_1,\\dots,x_n) \\\\\n\\vdots \\\\\nf_m(x_1,\\dots,x_n)\n\\end{bmatrix}\n\n$$\n\nThe Jacobian matrix of f is the matrix of first-order partial derivatives:\n$$\nJ(x) =\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#the-jacobian-matrix","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#interpretation","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"content":"Each row describes how one equation responds to changes in the variables.\n\nThe Jacobian generalizes the derivative to multivariate settings.\n\nIn economics, Jacobians appear in:\n\nsystems of equations,\n\nequilibrium conditions,\n\ncomparative statics.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#interpretation","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#the-hessian-matrix","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"content":"For a scalar-valued function f(x_1,\\dots,x_n), the Hessian matrix collects\nsecond-order partial derivatives:\n$$\nH(x) =\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2}\n& \\cdots\n& \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1}\n& \\cdots\n& \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#the-hessian-matrix","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#properties","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"content":"Under mild regularity conditions, the Hessian is symmetric.\n\nThe Hessian summarizes the curvature of a function.\n\nIt plays a central role in second-order conditions for optimization.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#properties","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"type":"lvl2","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#quadratic-forms","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"content":"Given a symmetric matrix A and a vector x, the expressionx' A x\n\nis called a quadratic form.\n\nQuadratic forms arise naturally when:\n\napproximating functions locally,\n\nanalyzing curvature,\n\nstudying stability.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#quadratic-forms","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"type":"lvl3","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#example","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"content":"If\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}\n\n\\qquad\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}then\n\nx’Ax = a x_1^2 + 2b x_1 x_2 + c x_2^2.\n$$","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#example","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"type":"lvl2","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#definiteness-of-a-matrix","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"content":"A symmetric matrix A is:\n\npositive definite if x'Ax > 0 for all x \\neq 0,\n\nnegative definite if x'Ax < 0 for all x \\neq 0,\n\nindefinite if x'Ax takes both positive and negative values.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#definiteness-of-a-matrix","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"type":"lvl3","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#economic-meaning","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"content":"Positive definiteness corresponds to convexity.\n\nNegative definiteness corresponds to concavity.\n\nThese properties determine whether a critical point is a minimum or a maximum.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#economic-meaning","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl2","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#principal-minors-and-the-determinant-test","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"content":"For a symmetric matrix, definiteness can be checked using principal minors.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#principal-minors-and-the-determinant-test","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#example-2-2-case","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"content":"Let\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}\n\n$$\n\nThe matrix A is positive definite ifa > 0,\n\\qquad\n\\det(A) = ac - b^2 > 0.\n\nThis test is especially useful in applied work, where computing symbolic\neigenvalues may be inconvenient.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#example-2-2-case","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"type":"lvl2","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#eigenvalues-and-definiteness","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"content":"An alternative characterization of definiteness uses eigenvalues.\n\nA symmetric matrix is:\n\npositive definite if all eigenvalues are positive,\n\nnegative definite if all eigenvalues are negative.\n\nEigenvalues provide geometric intuition: they describe how a quadratic form\nstretches space along its principal directions.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#eigenvalues-and-definiteness","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#the-bordered-hessian","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"content":"When optimizing a function subject to constraints, the bordered Hessian\nis used to check second-order conditions.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#the-bordered-hessian","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#structure","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"content":"For a problem with one constraint, the bordered Hessian takes the form\n$$\nH_B =\\begin{bmatrix}\n0 & g_x' \\\\\ng_x & H\n\\end{bmatrix}\n\n$$\nwhere:\n\ng_x is the gradient of the constraint,\n\nH is the Hessian of the Lagrangian.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#structure","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#interpretation-1","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"content":"The bordered Hessian incorporates both curvature and constraints.\n\nThe sign pattern of its principal minors determines optimality.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#interpretation-1","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"type":"lvl2","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#input-output-matrices-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"content":"Linear algebra is central to input–output analysis.\n\nLet A denote the matrix of technical coefficients, and let x denote output.\nThe systemx = Ax + d\n\ncan be rewritten as(I - A)x = d.\n\nIf (I - A) is invertible, the solution isx = (I - A)^{-1} d.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#input-output-matrices-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"type":"lvl3","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#economic-meaning-1","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"content":"(I - A)^{-1} is the Leontief inverse.\n\nIt captures both direct and indirect production requirements.\n\nInvertibility reflects feasibility and stability of the production system.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#economic-meaning-1","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"type":"lvl2","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#why-these-matrices-matter","position":34},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"content":"Special matrices allow us to:\n\ncharacterize equilibrium behavior,\n\ndetermine optimality,\n\nanalyze stability,\n\nunderstand economic structure.\n\nThey form the mathematical backbone of:\n\nconstrained optimization,\n\ndynamic systems,\n\ngeneral equilibrium models.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#why-these-matrices-matter","position":35},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#what-comes-next","position":36},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"content":"With these tools in place, we are ready to move beyond linear algebra and\nintroduce calculus-based methods, beginning with multivariate calculus\nand optimization.","type":"content","url":"/linear-algebra-speci-3c2739eae81e7bb21e54cdcacbce3#what-comes-next","position":37},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"type":"lvl1","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines\noptimality.\n\nThe focus is on interpretation and application, rather than abstract\ntheory.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#the-jacobian-matrix","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"content":"Consider a vector-valued functionf(x) =\n\\begin{bmatrix}\nf_1(x_1,\\dots,x_n) \\\\\n\\vdots \\\\\nf_m(x_1,\\dots,x_n)\n\\end{bmatrix}.\n\nThe Jacobian matrix of f is the matrix of first-order partial derivatives:J(x) =\n\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#the-jacobian-matrix","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#interpretation","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"content":"Each row describes how one equation responds to changes in the variables.\n\nThe Jacobian generalizes the derivative to multivariate settings.\n\nIn economics, Jacobians appear in:\n\nsystems of equations,\n\nequilibrium conditions,\n\ncomparative statics.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#interpretation","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#the-hessian-matrix","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"content":"For a scalar-valued function f(x_1,\\dots,x_n), the Hessian matrix collects\nsecond-order partial derivatives:H(x) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2}\n& \\cdots\n& \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1}\n& \\cdots\n& \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#the-hessian-matrix","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#properties","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"content":"Under mild regularity conditions, the Hessian is symmetric.\n\nThe Hessian summarizes the curvature of a function.\n\nIt plays a central role in second-order conditions for optimization.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#properties","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"type":"lvl2","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#quadratic-forms","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"content":"Given a symmetric matrix A and a vector x, the expressionx' A x\n\nis called a quadratic form.\n\nQuadratic forms arise naturally when:\n\napproximating functions locally,\n\nanalyzing curvature,\n\nstudying stability.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#quadratic-forms","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"type":"lvl3","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#example","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"content":"IfA =\n\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix},\n\\qquad\nx =\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix},\n\nthenx'Ax = a x_1^2 + 2b x_1 x_2 + c x_2^2.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#example","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"type":"lvl2","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#definiteness-of-a-matrix","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"content":"A symmetric matrix A is:\n\npositive definite if x'Ax > 0 for all x \\neq 0,\n\nnegative definite if x'Ax < 0 for all x \\neq 0,\n\nindefinite if x'Ax takes both positive and negative values.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#definiteness-of-a-matrix","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"type":"lvl3","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#economic-meaning","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"content":"Positive definiteness corresponds to convexity.\n\nNegative definiteness corresponds to concavity.\n\nThese properties determine whether a critical point is a minimum or a maximum.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#economic-meaning","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl2","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#principal-minors-and-the-determinant-test","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"content":"For a symmetric matrix, definiteness can be checked using principal minors.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#principal-minors-and-the-determinant-test","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#example-2-2-case","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"content":"LetA =\n\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}.\n\nThe matrix A is positive definite ifa > 0,\n\\qquad\n\\det(A) = ac - b^2 > 0.\n\nThis test is especially useful in applied work, where computing symbolic\neigenvalues may be inconvenient.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#example-2-2-case","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"type":"lvl2","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#eigenvalues-and-definiteness","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"content":"An alternative characterization of definiteness uses eigenvalues.\n\nA symmetric matrix is:\n\npositive definite if all eigenvalues are positive,\n\nnegative definite if all eigenvalues are negative.\n\nEigenvalues provide geometric intuition: they describe how a quadratic form\nstretches space along its principal directions.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#eigenvalues-and-definiteness","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#the-bordered-hessian","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"content":"When optimizing a function subject to constraints, the bordered Hessian\nis used to check second-order conditions.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#the-bordered-hessian","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#structure","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"content":"For a problem with one constraint, the bordered Hessian takes the formH_B =\n\\begin{bmatrix}\n0 & g_x' \\\\\ng_x & H\n\\end{bmatrix},\n\nwhere:\n\ng_x is the gradient of the constraint,\n\nH is the Hessian of the Lagrangian.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#structure","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#interpretation-1","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"content":"The bordered Hessian incorporates both curvature and constraints.\n\nThe sign pattern of its principal minors determines optimality.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#interpretation-1","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"type":"lvl2","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#input-output-matrices-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"content":"Linear algebra is central to input–output analysis.\n\nLet A denote the matrix of technical coefficients, and let x denote output.\nThe systemx = Ax + d\n\ncan be rewritten as(I - A)x = d.\n\nIf (I - A) is invertible, the solution isx = (I - A)^{-1} d.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#input-output-matrices-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"type":"lvl3","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#economic-meaning-1","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"content":"(I - A)^{-1} is the Leontief inverse.\n\nIt captures both direct and indirect production requirements.\n\nInvertibility reflects feasibility and stability of the production system.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#economic-meaning-1","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"type":"lvl2","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#why-these-matrices-matter","position":34},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"content":"Special matrices allow us to:\n\ncharacterize equilibrium behavior,\n\ndetermine optimality,\n\nanalyze stability,\n\nunderstand economic structure.\n\nThey form the mathematical backbone of:\n\nconstrained optimization,\n\ndynamic systems,\n\ngeneral equilibrium models.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#why-these-matrices-matter","position":35},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#what-comes-next","position":36},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"content":"With these tools in place, we are ready to move beyond linear algebra and\nintroduce calculus-based methods, beginning with multivariate calculus\nand optimization.","type":"content","url":"/linear-algebra-speci-44a8100ca29f29214bd451b75aa23#what-comes-next","position":37},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"type":"lvl1","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines\noptimality.\n\nThe focus is on interpretation and application, rather than abstract\ntheory.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#the-jacobian-matrix","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"content":"Consider a vector-valued functionf(x) =\n\\begin{bmatrix}\nf_1(x_1,\\dots,x_n) \\\\\n\\vdots \\\\\nf_m(x_1,\\dots,x_n)\n\\end{bmatrix}.\n\nThe Jacobian matrix of f is the matrix of first-order partial derivatives:J(x) =\n\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#the-jacobian-matrix","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#interpretation","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"content":"Each row describes how one equation responds to changes in the variables.\n\nThe Jacobian generalizes the derivative to multivariate settings.\n\nIn economics, Jacobians appear in:\n\nsystems of equations,\n\nequilibrium conditions,\n\ncomparative statics.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#interpretation","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#the-hessian-matrix","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"content":"For a scalar-valued function f(x_1,\\dots,x_n), the Hessian matrix collects\nsecond-order partial derivatives:H(x) =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2}\n& \\cdots\n& \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1}\n& \\cdots\n& \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#the-hessian-matrix","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#properties","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"content":"Under mild regularity conditions, the Hessian is symmetric.\n\nThe Hessian summarizes the curvature of a function.\n\nIt plays a central role in second-order conditions for optimization.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#properties","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"type":"lvl2","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#quadratic-forms","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"content":"Given a symmetric matrix A and a vector x, the expressionx' A x\n\nis called a quadratic form.\n\nQuadratic forms arise naturally when:\n\napproximating functions locally,\n\nanalyzing curvature,\n\nstudying stability.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#quadratic-forms","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"type":"lvl3","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#example","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"content":"IfA =\n\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix},\n\\qquad\nx =\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix},\n\nthenx'Ax = a x_1^2 + 2b x_1 x_2 + c x_2^2.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#example","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"type":"lvl2","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#definiteness-of-a-matrix","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"content":"A symmetric matrix A is:\n\npositive definite if x'Ax > 0 for all x \\neq 0,\n\nnegative definite if x'Ax < 0 for all x \\neq 0,\n\nindefinite if x'Ax takes both positive and negative values.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#definiteness-of-a-matrix","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"type":"lvl3","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#economic-meaning","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"content":"Positive definiteness corresponds to convexity.\n\nNegative definiteness corresponds to concavity.\n\nThese properties determine whether a critical point is a minimum or a maximum.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#economic-meaning","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl2","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#principal-minors-and-the-determinant-test","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"content":"For a symmetric matrix, definiteness can be checked using principal minors.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#principal-minors-and-the-determinant-test","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#example-2-2-case","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"content":"LetA =\n\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}.\n\nThe matrix A is positive definite ifa > 0,\n\\qquad\n\\det(A) = ac - b^2 > 0.\n\nThis test is especially useful in applied work, where computing symbolic\neigenvalues may be inconvenient.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#example-2-2-case","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"type":"lvl2","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#eigenvalues-and-definiteness","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"content":"An alternative characterization of definiteness uses eigenvalues.\n\nA symmetric matrix is:\n\npositive definite if all eigenvalues are positive,\n\nnegative definite if all eigenvalues are negative.\n\nEigenvalues provide geometric intuition: they describe how a quadratic form\nstretches space along its principal directions.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#eigenvalues-and-definiteness","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#the-bordered-hessian","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"content":"When optimizing a function subject to constraints, the bordered Hessian\nis used to check second-order conditions.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#the-bordered-hessian","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#structure","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"content":"For a problem with one constraint, the bordered Hessian takes the formH_B =\n\\begin{bmatrix}\n0 & g_x' \\\\\ng_x & H\n\\end{bmatrix},\n\nwhere:\n\ng_x is the gradient of the constraint,\n\nH is the Hessian of the Lagrangian.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#structure","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#interpretation-1","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"content":"The bordered Hessian incorporates both curvature and constraints.\n\nThe sign pattern of its principal minors determines optimality.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#interpretation-1","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"type":"lvl2","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#input-output-matrices-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"content":"Linear algebra is central to input–output analysis.\n\nLet A denote the matrix of technical coefficients, and let x denote output.\nThe systemx = Ax + d\n\ncan be rewritten as(I - A)x = d.\n\nIf (I - A) is invertible, the solution isx = (I - A)^{-1} d.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#input-output-matrices-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"type":"lvl3","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#economic-meaning-1","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"content":"(I - A)^{-1} is the Leontief inverse.\n\nIt captures both direct and indirect production requirements.\n\nInvertibility reflects feasibility and stability of the production system.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#economic-meaning-1","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"type":"lvl2","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#why-these-matrices-matter","position":34},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"content":"Special matrices allow us to:\n\ncharacterize equilibrium behavior,\n\ndetermine optimality,\n\nanalyze stability,\n\nunderstand economic structure.\n\nThey form the mathematical backbone of:\n\nconstrained optimization,\n\ndynamic systems,\n\ngeneral equilibrium models.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#why-these-matrices-matter","position":35},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#what-comes-next","position":36},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"content":"With these tools in place, we are ready to move beyond linear algebra and\nintroduce calculus-based methods, beginning with multivariate calculus\nand optimization.","type":"content","url":"/linear-algebra-speci-5116e088092f54512a107d90a8986#what-comes-next","position":37},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"type":"lvl1","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines optimality.\n\nThe focus is on interpretation and application, rather than abstract theory.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#the-jacobian-matrix","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"content":"Consider a vector-valued function:\n$$\nf(x) =\\begin{bmatrix}\nf_1(x_1,\\dots,x_n) \\\\\n\\vdots \\\\\nf_m(x_1,\\dots,x_n)\n\\end{bmatrix}\n\n$$\n\nThe Jacobian matrix of f is the matrix of first-order partial derivatives:\n$$\nJ(x) =\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#the-jacobian-matrix","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#interpretation","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"content":"Each row shows how one equation responds to changes in variables.\n\nThe Jacobian generalizes the derivative to multivariate settings.\n\nIn economics, Jacobians appear in:\n\nsystems of equations,\n\nequilibrium conditions,\n\ncomparative statics.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#interpretation","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#the-hessian-matrix","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"content":"For a scalar-valued function f(x_1,\\dots,x_n), the Hessian matrix collects\nsecond-order partial derivatives:\n$$\nH(x) =\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#the-hessian-matrix","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#properties","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"content":"Under mild regularity conditions, the Hessian is symmetric.\n\nThe Hessian summarizes the curvature of a function.\n\nIt plays a crucial role in second-order conditions for optimization.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#properties","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"type":"lvl2","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#quadratic-forms","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"content":"Given a symmetric matrix A and a vector x, the expressionx' A x\n\nis called a quadratic form.\n\nQuadratic forms arise naturally when:\n\napproximating functions locally,\n\nanalyzing curvature,\n\nstudying stability.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#quadratic-forms","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"type":"lvl3","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#example","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"content":"If\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}\n\n\\quad\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}then\n\nx’Ax = a x_1^2 + 2b x_1 x_2 + c x_2^2.\n$$","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#example","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"type":"lvl2","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#definiteness-of-a-matrix","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"content":"A symmetric matrix A is:\n\nPositive definite if x'Ax > 0 for all x \\neq 0\n\nNegative definite if x'Ax < 0 for all x \\neq 0\n\nIndefinite if x'Ax takes both positive and negative values","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#definiteness-of-a-matrix","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"type":"lvl3","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#economic-meaning","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"content":"Positive definiteness corresponds to convexity.\n\nNegative definiteness corresponds to concavity.\n\nThese properties determine whether a critical point is a minimum or maximum.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#economic-meaning","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl2","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#principal-minors-and-the-determinant-test","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"content":"For a symmetric matrix, definiteness can be checked using principal minors.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#principal-minors-and-the-determinant-test","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#example-2-2-case","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"content":"Let\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{pbatrix}.\n$$\n\n- $A$ is positive definite if:\n  $$\n  a > 0, \\quad \\det(A) = ac - b^2 > 0.\n  $$\n\nThis test is especially useful in applied work, where symbolic eigenvalues\nmay be difficult to compute.\n\n---\n\n## Eigenvalues and Definiteness\n\nAn alternative characterization uses **eigenvalues**.\n\nA symmetric matrix is:\n- positive definite if all eigenvalues are positive,\n- negative definite if all eigenvalues are negative.\n\nEigenvalues provide geometric intuition: they describe how a quadratic form\nstretches space along principal directions.\n\n---\n\n## The Bordered Hessian\n\nWhen optimizing a function subject to constraints, the **bordered Hessian**\nis used to check second-order conditions.\n\n### Structure\n\nFor a problem with one constraint, the bordered Hessian takes the form:\n$$\nH_B =\n\\begin{bmatrix}\n0 & g_x' \\\\\ng_x & H\n\\end{bmatrix}\n\n$$\nwhere:\n\ng_x is the gradient of the constraint,\n\nH is the Hessian of the Lagrangian.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#example-2-2-case","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#interpretation-1","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"Principal Minors and the Determinant Test"},"content":"The bordered Hessian incorporates both curvature and constraints.\n\nThe sign pattern of its principal minors determines optimality.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#interpretation-1","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"type":"lvl2","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#input-output-matrices-in-economics","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"content":"Linear algebra is central to input–output analysis.\n\nLet A be the matrix of technical coefficients, and let x denote output.\nThe system:x = Ax + d\n\ncan be rewritten as:(I - A)x = d.\n\nIf (I - A) is invertible, the solution is:x = (I - A)^{-1} d.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#input-output-matrices-in-economics","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"type":"lvl3","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#economic-meaning-1","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"content":"(I - A)^{-1} is the Leontief inverse.\n\nIt captures direct and indirect production requirements.\n\nInvertibility reflects feasibility and stability of production systems.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#economic-meaning-1","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"type":"lvl2","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#why-these-matrices-matter","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"content":"Special matrices allow us to:\n\ncharacterize equilibrium behavior,\n\ndetermine optimality,\n\nanalyze stability,\n\nunderstand economic structure.\n\nThey form the mathematical backbone of:\n\nconstrained optimization,\n\ndynamic systems,\n\ngeneral equilibrium models.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#why-these-matrices-matter","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#what-comes-next","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"content":"With these tools in place, we are ready to move beyond linear algebra and\nintroduce calculus-based methods, beginning with multivariate calculus\nand optimization.","type":"content","url":"/linear-algebra-speci-57f9bfceca5c2e1c0f0988fb1b9df#what-comes-next","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"type":"lvl1","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines\noptimality.\n\nThe focus is on interpretation and application, rather than abstract\ntheory.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#the-jacobian-matrix","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"content":"Consider a vector-valued function\n$$\nf(x) =\\begin{bmatrix}\nf_1(x_1,\\dots,x_n) \\\\\n\\vdots \\\\\nf_m(x_1,\\dots,x_n)\n\\end{bmatrix}\n\n$$\n\nThe Jacobian matrix of f is the matrix of first-order partial derivatives:\n$$\nJ(x) =\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#the-jacobian-matrix","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#interpretation","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"content":"Each row describes how one equation responds to changes in the variables.\n\nThe Jacobian generalizes the derivative to multivariate settings.\n\nIn economics, Jacobians appear in:\n\nsystems of equations,\n\nequilibrium conditions,\n\ncomparative statics.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#interpretation","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#the-hessian-matrix","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"content":"For a scalar-valued function f(x_1,\\dots,x_n), the Hessian matrix collects\nsecond-order partial derivatives:\n$$\nH(x) =\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2}\n& \\cdots\n& \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1}\n& \\cdots\n& \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#the-hessian-matrix","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#properties","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"content":"Under mild regularity conditions, the Hessian is symmetric.\n\nThe Hessian summarizes the curvature of a function.\n\nIt plays a central role in second-order conditions for optimization.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#properties","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"type":"lvl2","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#quadratic-forms","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"content":"Given a symmetric matrix A and a vector x, the expressionx' A x\n\nis called a quadratic form.\n\nQuadratic forms arise naturally when:\n\napproximating functions locally,\n\nanalyzing curvature,\n\nstudying stability.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#quadratic-forms","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"type":"lvl3","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#example","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"content":"If\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}\n\n\\qquad\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}then\n\nx’Ax = a x_1^2 + 2b x_1 x_2 + c x_2^2.\n$$","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#example","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"type":"lvl2","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#definiteness-of-a-matrix","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"content":"A symmetric matrix A is:\n\npositive definite if x'Ax > 0 for all x \\neq 0,\n\nnegative definite if x'Ax < 0 for all x \\neq 0,\n\nindefinite if x'Ax takes both positive and negative values.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#definiteness-of-a-matrix","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"type":"lvl3","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#economic-meaning","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"content":"Positive definiteness corresponds to convexity.\n\nNegative definiteness corresponds to concavity.\n\nThese properties determine whether a critical point is a minimum or a maximum.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#economic-meaning","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl2","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#principal-minors-and-the-determinant-test","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"content":"For a symmetric matrix, definiteness can be checked using principal minors.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#principal-minors-and-the-determinant-test","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#example-2-2-case","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"content":"Let\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}\n\n$$\n\nThe matrix A is positive definite ifa > 0,\n\\qquad\n\\det(A) = ac - b^2 > 0.\n\nThis test is especially useful in applied work, where computing symbolic\neigenvalues may be inconvenient.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#example-2-2-case","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"type":"lvl2","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#eigenvalues-and-definiteness","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"content":"An alternative characterization of definiteness uses eigenvalues.\n\nA symmetric matrix is:\n\npositive definite if all eigenvalues are positive,\n\nnegative definite if all eigenvalues are negative.\n\nEigenvalues provide geometric intuition: they describe how a quadratic form\nstretches space along its principal directions.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#eigenvalues-and-definiteness","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#the-bordered-hessian","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"content":"When optimizing a function subject to constraints, the bordered Hessian\nis used to check second-order conditions.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#the-bordered-hessian","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#structure","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"content":"For a problem with one constraint, the bordered Hessian takes the form\n$$\nH_B =\\begin{bmatrix}\n0 & g_x' \\\\\ng_x & H\n\\end{bmatrix}\n\n$$\nwhere:\n\ng_x is the gradient of the constraint,\n\nH is the Hessian of the Lagrangian.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#structure","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#interpretation-1","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"content":"The bordered Hessian incorporates both curvature and constraints.\n\nThe sign pattern of its principal minors determines optimality.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#interpretation-1","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"type":"lvl2","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#input-output-matrices-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"content":"Linear algebra is central to input–output analysis.\n\nLet A denote the matrix of technical coefficients, and let x denote output.\nThe systemx = Ax + d\n\ncan be rewritten as(I - A)x = d.\n\nIf (I - A) is invertible, the solution isx = (I - A)^{-1} d.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#input-output-matrices-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"type":"lvl3","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#economic-meaning-1","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"content":"(I - A)^{-1} is the Leontief inverse.\n\nIt captures both direct and indirect production requirements.\n\nInvertibility reflects feasibility and stability of the production system.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#economic-meaning-1","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"type":"lvl2","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#why-these-matrices-matter","position":34},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"content":"Special matrices allow us to:\n\ncharacterize equilibrium behavior,\n\ndetermine optimality,\n\nanalyze stability,\n\nunderstand economic structure.\n\nThey form the mathematical backbone of:\n\nconstrained optimization,\n\ndynamic systems,\n\ngeneral equilibrium models.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#why-these-matrices-matter","position":35},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#what-comes-next","position":36},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"content":"With these tools in place, we are ready to move beyond linear algebra and\nintroduce calculus-based methods, beginning with multivariate calculus\nand optimization.","type":"content","url":"/linear-algebra-speci-711cef9bf9833ac5e3a4b1764910f#what-comes-next","position":37},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"type":"lvl1","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines optimality.\n\nThe focus is on interpretation and application, rather than abstract theory.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#the-jacobian-matrix","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"content":"Consider a vector-valued function:\n$$\nf(x) =\\begin{pmatrix}\nf_1(x_1,\\dots,x_n) \\\\\n\\vdots \\\\\nf_m(x_1,\\dots,x_n)\n\\end{pmatrix}\n\n$$\n\nThe Jacobian matrix of f is the matrix of first-order partial derivatives:\n$$\nJ(x) =\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{pmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#the-jacobian-matrix","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#interpretation","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"content":"Each row shows how one equation responds to changes in variables.\n\nThe Jacobian generalizes the derivative to multivariate settings.\n\nIn economics, Jacobians appear in:\n\nsystems of equations,\n\nequilibrium conditions,\n\ncomparative statics.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#interpretation","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#the-hessian-matrix","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"content":"For a scalar-valued function f(x_1,\\dots,x_n), the Hessian matrix collects\nsecond-order partial derivatives:\n$$\nH(x) =\\begin{pmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{pmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#the-hessian-matrix","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#properties","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"content":"Under mild regularity conditions, the Hessian is symmetric.\n\nThe Hessian summarizes the curvature of a function.\n\nIt plays a crucial role in second-order conditions for optimization.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#properties","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"type":"lvl2","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#quadratic-forms","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"content":"Given a symmetric matrix A and a vector x, the expressionx' A x\n\nis called a quadratic form.\n\nQuadratic forms arise naturally when:\n\napproximating functions locally,\n\nanalyzing curvature,\n\nstudying stability.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#quadratic-forms","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"type":"lvl3","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#example","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"content":"If\n$$\nA =\\begin{pmatrix}\na & b \\\\\nb & c\n\\end{pmatrix}\n\n\\quad\nx =\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}then\n\nx’Ax = a x_1^2 + 2b x_1 x_2 + c x_2^2.\n$$","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#example","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"type":"lvl2","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#definiteness-of-a-matrix","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"content":"A symmetric matrix A is:\n\nPositive definite if x'Ax > 0 for all x \\neq 0\n\nNegative definite if x'Ax < 0 for all x \\neq 0\n\nIndefinite if x'Ax takes both positive and negative values","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#definiteness-of-a-matrix","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"type":"lvl3","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#economic-meaning","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"content":"Positive definiteness corresponds to convexity.\n\nNegative definiteness corresponds to concavity.\n\nThese properties determine whether a critical point is a minimum or maximum.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#economic-meaning","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl2","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#principal-minors-and-the-determinant-test","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"content":"For a symmetric matrix, definiteness can be checked using principal minors.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#principal-minors-and-the-determinant-test","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#example-2-2-case","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"content":"Let\n$$\nA =\\begin{pmatrix}\na & b \\\\\nb & c\n\\end{pmatrix}\n\n$$\n\nA is positive definite if:\na > 0, \\quad \\det(A) = ac - b^2 > 0.\n\nThis test is especially useful in applied work, where symbolic eigenvalues\nmay be difficult to compute.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#example-2-2-case","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"type":"lvl2","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#eigenvalues-and-definiteness","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"content":"An alternative characterization uses eigenvalues.\n\nA symmetric matrix is:\n\npositive definite if all eigenvalues are positive,\n\nnegative definite if all eigenvalues are negative.\n\nEigenvalues provide geometric intuition: they describe how a quadratic form\nstretches space along principal directions.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#eigenvalues-and-definiteness","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#the-bordered-hessian","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"content":"When optimizing a function subject to constraints, the bordered Hessian\nis used to check second-order conditions.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#the-bordered-hessian","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#structure","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"content":"For a problem with one constraint, the bordered Hessian takes the form:\n$$\nH_B =\\begin{pmatrix}\n0 & g_x' \\\\\ng_x & H\n\\end{pmatrix}\n\n$$\nwhere:\n\ng_x is the gradient of the constraint,\n\nH is the Hessian of the Lagrangian.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#structure","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#interpretation-1","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"content":"The bordered Hessian incorporates both curvature and constraints.\n\nThe sign pattern of its principal minors determines optimality.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#interpretation-1","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"type":"lvl2","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#input-output-matrices-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"content":"Linear algebra is central to input–output analysis.\n\nLet A be the matrix of technical coefficients, and let x denote output.\nThe system:x = Ax + d\n\ncan be rewritten as:(I - A)x = d.\n\nIf (I - A) is invertible, the solution is:x = (I - A)^{-1} d.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#input-output-matrices-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"type":"lvl3","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#economic-meaning-1","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"content":"(I - A)^{-1} is the Leontief inverse.\n\nIt captures direct and indirect production requirements.\n\nInvertibility reflects feasibility and stability of production systems.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#economic-meaning-1","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"type":"lvl2","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#why-these-matrices-matter","position":34},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"content":"Special matrices allow us to:\n\ncharacterize equilibrium behavior,\n\ndetermine optimality,\n\nanalyze stability,\n\nunderstand economic structure.\n\nThey form the mathematical backbone of:\n\nconstrained optimization,\n\ndynamic systems,\n\ngeneral equilibrium models.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#why-these-matrices-matter","position":35},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#what-comes-next","position":36},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"content":"With these tools in place, we are ready to move beyond linear algebra and\nintroduce calculus-based methods, beginning with multivariate calculus\nand optimization.","type":"content","url":"/linear-algebra-speci-931bd73a5d5ed6c53873c0adc6be9#what-comes-next","position":37},{"hierarchy":{"lvl1":"3. Special Matrices"},"type":"lvl1","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc","position":0},{"hierarchy":{"lvl1":"3. Special Matrices"},"content":"Let’s look at some special matrices and determinants that are useful in economic analysis.","type":"content","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc","position":1},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Jacobian"},"type":"lvl2","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#the-jacobian","position":2},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Jacobian"},"content":"The Jacobian (named after German mathematician Karl Gustav Jacobi, 1804–1851) is generally used in conjunction with partial derivatives to provide an easy test for the existence of functional dependence (linear and nonlinear). A Jacobian determinant |\\mathbf{J}| is composed of all the first‑order partial derivatives of a system of equations, arranged in order sequence. Giveny_{1} = f_{1}(x_{1},x_{2},x_{3}) \\\\\ny_{2} = f_{2}(x_{1},x_{2},x_{3}) \\\\\ny_{3} = f_{3}(x_{1},x_{2},x_{3})\n\nThen|\\mathbf{J}| = \n\\begin{vmatrix}\n\\frac{\\partial y_{1}}{\\partial x_{1}} & \\frac{\\partial y_{1}}{\\partial x_{2}} & \\frac{\\partial y_{1}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} & \\frac{\\partial y_{2}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{3}}{\\partial x_{1}} & \\frac{\\partial y_{3}}{\\partial x_{2}} & \\frac{\\partial y_{3}}{\\partial x_{3}}\n\\end{vmatrix}\n\nFor example, say you havey_{1} = 5x_{1} + 3x_{2} \\\\\ny_{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}\n\nThen taking the first‑order partials gives\\frac{\\partial y_1}{\\partial x_1} = 5,\\qquad \n\\frac{\\partial y_1}{\\partial x_2} = 3,\\qquad \n\\frac{\\partial y_2}{\\partial x_1} = 50x_1 + 30x_2,\\qquad \n\\frac{\\partial y_2}{\\partial x_2} = 30x_1 + 18x_2\n\nAnd the Jacobian is|\\mathbf{J}| = \n\\begin{vmatrix}\n5 & 3 \\\\\n50x_1 + 30x_2 & 30x_1 + 18x_2\n\\end{vmatrix}\n\nThe determinant of the Jacobian matrix is|\\mathbf{J}| = 5(30x_1 + 18x_2) - 3(50x_1 + 30x_2) = 0\n\nSince |\\mathbf{J}| = 0, there is functional dependence between the equations. (Note: (5x_{1} + 3x_{2})^{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}.)","type":"content","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#the-jacobian","position":3},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Hessian"},"type":"lvl2","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#the-hessian","position":4},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Hessian"},"content":"Given that the first‑order conditions z_{x} = z_{y} = 0 are met, a sufficient condition for a multivariate function z = f(x,y) to be an optimum is\n\nz_{xx}, z_{yy} > 0 for a minimum; z_{xx}, z_{yy} < 0 for a maximum.\n\nz_{xx} z_{yy} > (z_{xy})^{2}.\n\nA convenient test for this second‑order condition is provided by the Hessian. A Hessian |\\mathbf{H}| is a determinant composed of all the second‑order partial derivatives, with the second‑order direct partials on the principal diagonal and the second‑order cross partials off the principal diagonal. That is,|\\mathbf{H}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n\nwhere (by Young’s Theorem) z_{xy} = z_{yx}.\n\nIf the first element on the principal diagonal, a.k.a. the first principal minor, |H_{1}| = z_{xx} is positive, and the second principal minor|H_{2}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n= z_{xx} z_{yy} - (z_{xy})^{2} > 0,\n\nthen the second‑order conditions for a minimum are set. That is, when |H_{1}| > 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be positive definite. And a positive definite Hessian fulfills the second‑order conditions for a minimum.\n\nIf the first principal minor |H_{1}| < 0, while the second principal minor remains positive, then the second‑order conditions for a maximum are met. That is, when |H_{1}| < 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example:\n\nGivenz = f(x,y) = 2x^{2} - xy + 2y^{2} - 5x - 6y + 20,\n\nthenz_x = 4x - y - 5,\\quad z_y = -x + 4y - 6,\\quad z_{xx} = 4,\\quad z_{yy} = 4,\\quad z_{xy} = z_{yx} = -1.\n\nThe Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\n4 & -1 \\\\\n-1 & 4\n\\end{vmatrix}\n\nWe have |H_{1}| = 4 > 0 and |H_{2}| = (4)(4) - (-1)^2 = 16 - 1 = 15 > 0. Since |H_{1}| > 0 and |H_{2}| > 0, i.e. the Hessian is positive definite, then the function z is characterized by a minimum at the critical values (can you find these?).","type":"content","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#the-hessian","position":5},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Discriminant"},"type":"lvl2","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#the-discriminant","position":6},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Discriminant"},"content":"Determinants can be used to test for positive and negative definiteness of any quadratic form. The determinant of a quadratic form is called a discriminant |\\mathbf{D}|. Given the quadratic formz = a x^{2} + b x y + c y^{2},\n\nthe determinant is formed by placing the coefficients of the squared terms on the principal diagonal and dividing the coefficients of the non‑squared terms equally between the off‑diagonal positions. Hence, we have|\\mathbf{D}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n\nWe then evaluate the principal minors like we did for the Hessian test, where|D_{1}| = a \\quad \\text{and} \\quad |D_{2}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n= a c - \\frac{b^{2}}{4}.\n\nIf |D_{1}|, |D_{2}| > 0, |\\mathbf{D}| is positive definite and z is positive for all nonzero values of x and y. If |D_{1}| < 0 and |D_{2}| > 0, |\\mathbf{D}| is negative definite and z is negative for all nonzero values of x and y. If |D_{2}| \\neq 0, z is not sign definite and z may assume both positive and negative values.\n\nLet’s take an example to test for sign definiteness of the following quadratic formz = 2x^{2} + 5xy + 8y^{2}.\n\nWe can easily form the discriminant|\\mathbf{D}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n\nThen evaluating the principal minors gives|D_{1}| = 2 > 0, \\qquad |D_{2}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n= 16 - 6.25 = 9.75 > 0.\n\nHence, z is positive definite, meaning that it will be greater than zero for all nonzero values of x and y.","type":"content","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#the-discriminant","position":7},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Quadratic Form"},"type":"lvl2","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#the-quadratic-form","position":8},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Quadratic Form"},"content":"A quadratic form is defined as a polynomial expression in which each component term has a uniform degree.\n\nHere are some examples:6x^{2} - 2xy + 3y^{2} \\quad \\text{is a quadratic form in 2 variables.} \\\\\nx^{2} + 2xy + 4xz + 2yz + y^{2} + z^{2} \\quad \\text{is a quadratic form in 3 variables.}\n\nIt would be useful to determine the sign definiteness so we can make statements concerning the optimum value of the function as to whether it is a minimum or a maximum.\n\nMore generally, a quadratic form in n variables (x_{1},x_{2},\\ldots ,x_{n}) can be written as \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} where\n\n\\mathbf{x}^{\\prime} is a row vector [x_{1},x_{2},\\dots,x_{n}] and\\mathbf{A} is an n\\times n matrix of scalar elements.\n\nTo see this more clearly,\\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = \n\\begin{bmatrix}\nx_{1} & x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 3 \\\\\n4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= \n\\begin{bmatrix}\nx_{1} + 4x_{2} & 3x_{1} + 5x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= x_{1}^{2} + 7x_{1}x_{2} + 5x_{2}^{2}.\n\nA quadratic form is said to be:\n\na) Positive definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} > 0 for all \\mathbf{x} \\neq \\mathbf{0}.b) Positive semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\geq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.c) Negative definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} < 0 for all \\mathbf{x} \\neq \\mathbf{0}.d) Negative semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\leq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.e) Sign indefinite if it takes both positive and negative values.\n\nQuadratic forms are particularly useful in determining the concavity or convexity of a differentiable function.\n\nFor a function y = f(x_{1},x_{2},\\dots,x_{n}), we can form the Hessian consisting of second‑order partial derivatives and construct a quadratic form as \\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x}. Then\n\na) The function y is strictly convex if the quadratic form is positive (implying that the Hessian is positive definite, i.e., the determinants of the principal minors are all positive).b) The function y is strictly concave if the quadratic form is negative (that is, the Hessian is negative definite, i.e. the determinants of the principal minors alternate in sign).\n\nLet’s take an example to see this more clearly.\n\nSay we havez = x^{2} + y^{2}.\n\nThenz_{x} = 2x,\\quad z_{xx} = 2,\\quad z_{xy} = 0, \\\\\nz_{y} = 2y,\\quad z_{yx} = 0,\\quad z_{yy} = 2.\n\nBut\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 2x^{2} + 2y^{2} > 0.\n\nHence the function z = x^{2} + y^{2} is characterized by a minimum at its optimal value and is therefore a strictly convex function. And it is easy to see that the Hessian is positive definite!\n\nHere’s another example:\n\nSay we have a quadratic form in 2 variablesz = 8x^{2} + 6xy + 2y^{2}.\n\nThis can be rearranged asz = 8x^{2} + 3xy + 3xy + 2y^{2}.\n\nWe can write this as\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 8x^{2} + 2y^{2} + (3)(3)xy = 8x^{2} + 2y^{2} + 9xy.\n\nThe Hessian is\\mathbf{H} = \n\\begin{bmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{bmatrix}\n\nThe principal minors are |H_{1}| = 8 and $|H_{2}| =\\begin{vmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{vmatrix}\n\nBoth are positive so we have the Hessian and the quadratic form as positive definite.","type":"content","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#the-quadratic-form","position":9},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Higher Order Hessian"},"type":"lvl2","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#higher-order-hessian","position":10},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Higher Order Hessian"},"content":"Given y = f(x_{1},x_{2},x_{3}), the third‑order Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\ny_{11} & y_{12} & y_{13} \\\\\ny_{21} & y_{22} & y_{23} \\\\\ny_{31} & y_{32} & y_{33}\n\\end{vmatrix}\n\nwhere the elements are the various second‑order partial derivatives of y:y_{11} = \\frac{\\partial^{2}y}{\\partial x_{1}^{2}},\\quad \ny_{12} = \\frac{\\partial^{2}y}{\\partial x_{2}\\partial x_{1}},\\quad \ny_{23} = \\frac{\\partial^{2}y}{\\partial x_{3}\\partial x_{2}},\\quad \\text{etc.}\n\nConditions for a relative minimum or maximum depend on the signs of the first, second, and third principal minors, respectively. If|H_{1}| > 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| > 0,\n\nthen |\\mathbf{H}| is positive definite and fulfills the second‑order conditions for a minimum.\n\nIf|H_{1}| < 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| < 0,\n\nthen |\\mathbf{H}| is negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example.\n\nGiven the functiony = -5x_{1}^{2} + 10x_{1} + x_{1}x_{3} - 2x_{2}^{2} + 4x_{2} + 2x_{2}x_{3} - 4x_{3}^{2}.\n\nThe first order conditions (F.O.C) are\\frac{\\partial y}{\\partial x_1} = y_1 = -10x_1 + 10 + x_3 = 0, \\\\\n\\frac{\\partial y}{\\partial x_2} = y_2 = -4x_2 + 2x_3 + 4 = 0, \\\\\n\\frac{\\partial y}{\\partial x_3} = y_3 = x_1 + 2x_2 - 8x_3 = 0.\n\nwhich can be expressed in matrix form as\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-10 \\\\ -4 \\\\ 0\n\\end{bmatrix}.\n\nUsing Cramer’s rule we get x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43 (please verify this).\n\nLet’s take the second partial derivatives from the first‑order conditions to create the Hessian,y_{11} = -10,\\quad y_{12} = 0,\\quad y_{13} = 1, \\\\\ny_{21} = 0,\\quad y_{22} = -4,\\quad y_{23} = 2, \\\\\ny_{31} = 1,\\quad y_{32} = 2,\\quad y_{33} = -8.\n\nThus,\\mathbf{H} = \n\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\nFinally, applying the Hessian test, we have|H_1| = -10 < 0,\\quad \n|H_2| = \n\\begin{vmatrix}\n-10 & 0 \\\\\n0 & -4\n\\end{vmatrix} = 40 > 0,\\quad \n|H_3| = |\\mathbf{H}| = \n\\begin{vmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{vmatrix} = -80 < 0.\n\nSince the principal minors alternate correctly in sign, the Hessian is negative definite and the function is maximized at x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43.","type":"content","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#higher-order-hessian","position":11},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#the-bordered-hessian","position":12},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"The Bordered Hessian"},"content":"To optimize a function f(x_1, x_2) subject to a constraint g(x_1, x_2) the first‑order conditions can be found by setting up what is known as the Lagrangian function F(x_1, x_2, \\lambda) = f(x_1, x_2) - \\lambda(g(x_1, x_2) - c).\n\nThe second‑order conditions can be expressed in terms of a bordered Hessian |\\bar{\\mathbf{H}}| as|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & F_{11} & F_{12} \\\\\ng_2 & F_{21} & F_{22}\n\\end{vmatrix}\n\nor|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & f_{11} & f_{12} \\\\\ng_2 & f_{21} & f_{22}\n\\end{vmatrix}\n\nwhich is the usual Hessian bordered by the first derivatives of the constraint with zero on the principal diagonal.\n\nThe order of a bordered principal minor is determined by the order of the principal minor being bordered. Hence |\\bar{H}_2| above represents a second bordered principal minor, because the principal minor being bordered has dimensions 2\\times 2.\n\nFor the bivariate case with a single constraint, we simply look at |\\bar{H}_2|. If this is negative, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum. However, if it is positive, the bordered Hessian is said to be negative definite, and meets the sufficient conditions for a maximum.\n\nLet’s try optimizing the following objective functionf(x_1, x_2) = 4x_1^2 + 3x_2^2 - 2x_1x_2\n\nsubject tox_1 + x_2 = 56.\n\nSetting up the Lagrangian function, taking the first‑order partials and solving gives x_1^* = 36, x_2^* = 20 (and \\lambda = 348). (You should verify this.)\n\nThe bordered Hessian for this optimization problem is|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & 1 & 1 \\\\\n1 & 8 & -2 \\\\\n1 & -2 & 6\n\\end{vmatrix}\n\nStarting with the second principal minor, we have|\\bar{H}_2| = |\\bar{\\mathbf{H}}| = 0 \\cdot \n\\begin{vmatrix}\n8 & -2 \\\\\n-2 & 6\n\\end{vmatrix}\n- 1 \\cdot \n\\begin{vmatrix}\n1 & -2 \\\\\n1 & 6\n\\end{vmatrix}\n+ 1 \\cdot \n\\begin{vmatrix}\n1 & 8 \\\\\n1 & -2\n\\end{vmatrix}\n= - (6+2) + (-2-8) = -8 -10 = -18.\n\nwhich is negative, hence |\\bar{\\mathbf{H}}| is positive definite and we have met sufficient conditions for a minimum.\n\nFor the more general case in which the objective function has say n variables, i.e., f(x_1, \\ldots , x_n) which is subject to some constraint g(x_1, \\ldots , x_n), we can set up the bordered Hessian as|\\bar{\\mathbf{H}}| = \n\\begin{bmatrix}\n0 & g_1 & g_2 & \\dots & g_n \\\\\ng_1 & F_{11} & F_{12} & \\dots & F_{1n} \\\\\ng_2 & F_{21} & F_{22} & \\dots & F_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ng_n & F_{n1} & F_{n2} & \\dots & F_{nn}\n\\end{bmatrix}\n\nwhere |\\bar{\\mathbf{H}}| = |\\bar{H}_n| because the n \\times n principal minor is bordered.\n\nIn this case, if all the principal minors are negative, i.e. |\\bar{H}_2|, |\\bar{H}_3|, \\ldots , |\\bar{H}_n| < 0, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum.\n\nOn the other hand, if the principal minors alternate consistently in sign from positive to negative, i.e. |\\bar{H}_2| > 0, |\\bar{H}_3| < 0, |\\bar{H}_4| > 0 etc., the bordered Hessian is negative definite, and meets the sufficient conditions for a maximum.","type":"content","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#the-bordered-hessian","position":13},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Input‑Output Analysis"},"type":"lvl2","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#input-output-analysis","position":14},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Input‑Output Analysis"},"content":"If a_{ij} is a technical coefficient representing the value of input i required to produce one dollar’s worth of product j, the total demand for good i can be expressed asx_{i} = a_{i1}x_{1} + a_{i2}x_{2} + \\dots + a_{in}x_{n} + b_{i}\n\nwhere b_i is the final demand for product i. What is important to realize here is that the total demand for a product consists of that product being the final demand plus that product being an intermediate good required for the production of other products.\n\nIn matrix form we have\\mathbf{X} = \\mathbf{A}\\mathbf{X} + \\mathbf{B}\n\nwhere, for an n sector economy,\\mathbf{X} = \n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n\\quad\n\\mathbf{A} = \n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nn}\n\\end{bmatrix},\n\\quad\n\\mathbf{B} = \n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n\\end{bmatrix}.\n\n\\mathbf{A} is called the matrix of technical coefficients.\n\nTo find the output (intermediate and final goods) needed to satisfy demand, all we have to do is to solve for \\mathbf{X}:\\mathbf{X} - \\mathbf{A}\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad (\\mathbf{I} - \\mathbf{A})\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad \\mathbf{X} = (\\mathbf{I} - \\mathbf{A})^{-1}\\mathbf{B}.","type":"content","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#input-output-analysis","position":15},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"type":"lvl2","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":16},{"hierarchy":{"lvl1":"3. Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"content":"For a square matrix \\mathbf{A}:\n\na) All characteristic roots \\lambda are positive \\Rightarrow \\mathbf{A} is positive definite.b) All \\lambda’s are negative \\Rightarrow \\mathbf{A} is negative definite.c) All \\lambda’s are nonnegative and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is positive semi‑definite.d) All \\lambda’s are nonpositive and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is negative semi‑definite.e) Some \\lambda’s are positive and some negative \\Rightarrow \\mathbf{A} is sign indefinite.\n\nLet’s take an example.\n\nGiven a square matrix\\mathbf{A} = \n\\begin{bmatrix}\n-6 & 3 \\\\\n3 & -6\n\\end{bmatrix}\n\nTo find the characteristic roots (eigenvalues) of \\mathbf{A}, we simply set |\\mathbf{A} - \\lambda \\mathbf{I}| = 0:|\\mathbf{A} - \\lambda \\mathbf{I}| = \n\\begin{vmatrix}\n-6 - \\lambda & 3 \\\\\n3 & -6 - \\lambda\n\\end{vmatrix} = 0\n\nThis means(-6 - \\lambda)(-6 - \\lambda) - 9 = 0 \\\\\n\\lambda^2 + 12\\lambda + 27 = 0 \\\\\n(\\lambda + 9)(\\lambda + 3) = 0 \\\\\n\\lambda = -9, -3.\n\nSince both characteristic roots \\lambda are negative, we say \\mathbf{A} is negative definite.\n\nNote:\\text{(i) } \\sum \\lambda_i = \\operatorname{tr}(\\mathbf{A}), \\qquad\n\\text{(ii) } \\prod \\lambda_i = |\\mathbf{A}|.\n\nLet’s continue with the example above to find the characteristic vector.\n\nWe know one of the roots \\lambda = -9, so substituting in the characteristic matrix gives(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{bmatrix}\n-6 - (-9) & 3 \\\\\n3 & -6 - (-9)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n3 & 3 \\\\\n3 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nSince the coefficient matrix is linearly dependent, there are infinite number of solutions. The product of the matrices gives two equations which are identical:3v_1 + 3v_2 = 0 \\quad \\Rightarrow \\quad v_2 = -v_1.\n\nBy normalizing we havev_1^{2} + v_2^{2} = 1.\n\nSubstituting v_2 = -v_1 givesv_1^{2} + (-v_1)^{2} = 1 \\quad \\Rightarrow \\quad 2v_1^{2} = 1 \\quad \\Rightarrow \\quad v_1^{2} = \\frac{1}{2}.\n\nTaking the positive square root gives v_1 = \\sqrt{1/2} = \\frac{\\sqrt{2}}{2} and substituting into v_2 = -v_1 gives v_2 = -\\frac{\\sqrt{2}}{2}. That is\\mathbf{v}_1 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n-\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.\n\nUsing the second characteristic root \\lambda = -3:(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \n\\begin{bmatrix}\n-6 - (-3) & 3 \\\\\n3 & -6 - (-3)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-3 & 3 \\\\\n3 & -3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nMultiplying out gives -3v_1 + 3v_2 = 0 and 3v_1 - 3v_2 = 0, so v_1 = v_2.\n\nNormalizing as before:v_1^2 + v_2^2 = 1 \\quad \\Rightarrow \\quad 2v_1^2 = 1 \\quad \\Rightarrow \\quad v_1 = \\frac{\\sqrt{2}}{2}.\n\nHence,\\mathbf{v}_2 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-speci-9f21bd2eee5424da1a61524c5cdbc#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":17},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/linear-algebra-speci-a1399990b754ffce54bc10e8248e8","position":0},{"hierarchy":{"lvl1":""},"content":"","type":"content","url":"/linear-algebra-speci-a1399990b754ffce54bc10e8248e8","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"type":"lvl1","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines optimality.\n\nThe focus is on interpretation and application, rather than abstract theory.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#the-jacobian-matrix","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"content":"Consider a vector-valued function:\n$$\nf(x) =\\begin{bmatrix}\nf_1(x_1,\\dots,x_n) \\\\\n\\vdots \\\\\nf_m(x_1,\\dots,x_n)\n\\end{bmatrix}\n\n$$\n\nThe Jacobian matrix of f is the matrix of first-order partial derivatives:\n$$\nJ(x) =\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#the-jacobian-matrix","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#interpretation","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"content":"Each row shows how one equation responds to changes in variables.\n\nThe Jacobian generalizes the derivative to multivariate settings.\n\nIn economics, Jacobians appear in:\n\nsystems of equations,\n\nequilibrium conditions,\n\ncomparative statics.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#interpretation","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#the-hessian-matrix","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"content":"For a scalar-valued function f(x_1,\\dots,x_n), the Hessian matrix collects\nsecond-order partial derivatives:\n$$\nH(x) =\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#the-hessian-matrix","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#properties","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"content":"Under mild regularity conditions, the Hessian is symmetric.\n\nThe Hessian summarizes the curvature of a function.\n\nIt plays a crucial role in second-order conditions for optimization.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#properties","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"type":"lvl2","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#quadratic-forms","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"content":"Given a symmetric matrix A and a vector x, the expressionx' A x\n\nis called a quadratic form.\n\nQuadratic forms arise naturally when:\n\napproximating functions locally,\n\nanalyzing curvature,\n\nstudying stability.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#quadratic-forms","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"type":"lvl3","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#example","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"content":"If\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}\n\n\\quad\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}then\n\nx’Ax = a x_1^2 + 2b x_1 x_2 + c x_2^2.\n$$","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#example","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"type":"lvl2","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#definiteness-of-a-matrix","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"content":"A symmetric matrix A is:\n\nPositive definite if x'Ax > 0 for all x \\neq 0\n\nNegative definite if x'Ax < 0 for all x \\neq 0\n\nIndefinite if x'Ax takes both positive and negative values","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#definiteness-of-a-matrix","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"type":"lvl3","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#economic-meaning","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"content":"Positive definiteness corresponds to convexity.\n\nNegative definiteness corresponds to concavity.\n\nThese properties determine whether a critical point is a minimum or maximum.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#economic-meaning","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl2","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#principal-minors-and-the-determinant-test","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"content":"For a symmetric matrix, definiteness can be checked using principal minors.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#principal-minors-and-the-determinant-test","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#example-2-2-case","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"content":"Let\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{pbatrix}.\n$$\n\n- $A$ is positive definite if:\n  $$\n  a > 0, \\quad \\det(A) = ac - b^2 > 0.\n  $$\n\nThis test is especially useful in applied work, where symbolic eigenvalues\nmay be difficult to compute.\n\n---\n\n## Eigenvalues and Definiteness\n\nAn alternative characterization uses **eigenvalues**.\n\nA symmetric matrix is:\n- positive definite if all eigenvalues are positive,\n- negative definite if all eigenvalues are negative.\n\nEigenvalues provide geometric intuition: they describe how a quadratic form\nstretches space along principal directions.\n\n---\n\n## The Bordered Hessian\n\nWhen optimizing a function subject to constraints, the **bordered Hessian**\nis used to check second-order conditions.\n\n### Structure\n\nFor a problem with one constraint, the bordered Hessian takes the form:\n$$\nH_B =\n\\begin{bmatrix}\n0 & g_x' \\\\\ng_x & H\n\\end{bmatrix}\n\n$$\nwhere:\n\ng_x is the gradient of the constraint,\n\nH is the Hessian of the Lagrangian.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#example-2-2-case","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#interpretation-1","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"Principal Minors and the Determinant Test"},"content":"The bordered Hessian incorporates both curvature and constraints.\n\nThe sign pattern of its principal minors determines optimality.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#interpretation-1","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"type":"lvl2","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#input-output-matrices-in-economics","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"content":"Linear algebra is central to input–output analysis.\n\nLet A be the matrix of technical coefficients, and let x denote output.\nThe system:x = Ax + d\n\ncan be rewritten as:(I - A)x = d.\n\nIf (I - A) is invertible, the solution is:x = (I - A)^{-1} d.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#input-output-matrices-in-economics","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"type":"lvl3","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#economic-meaning-1","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"content":"(I - A)^{-1} is the Leontief inverse.\n\nIt captures direct and indirect production requirements.\n\nInvertibility reflects feasibility and stability of production systems.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#economic-meaning-1","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"type":"lvl2","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#why-these-matrices-matter","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"content":"Special matrices allow us to:\n\ncharacterize equilibrium behavior,\n\ndetermine optimality,\n\nanalyze stability,\n\nunderstand economic structure.\n\nThey form the mathematical backbone of:\n\nconstrained optimization,\n\ndynamic systems,\n\ngeneral equilibrium models.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#why-these-matrices-matter","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#what-comes-next","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"content":"With these tools in place, we are ready to move beyond linear algebra and\nintroduce calculus-based methods, beginning with multivariate calculus\nand optimization.","type":"content","url":"/linear-algebra-speci-a4e1e9c3e0259c85d24f099d0e53a#what-comes-next","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"type":"lvl1","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines optimality.\n\nThe focus is on interpretation and application, rather than abstract theory.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#the-jacobian-matrix","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"content":"Consider a vector-valued function:\n$$\nf(x) =\\begin{pmatrix}\nf_1(x_1,\\dots,x_n) \\\\\n\\vdots \\\\\nf_m(x_1,\\dots,x_n)\n\\end{pmatrix}\n\n$$\n\nThe Jacobian matrix of f is the matrix of first-order partial derivatives:\n$$\nJ(x) =\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{pmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#the-jacobian-matrix","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#interpretation","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"content":"Each row shows how one equation responds to changes in variables.\n\nThe Jacobian generalizes the derivative to multivariate settings.\n\nIn economics, Jacobians appear in:\n\nsystems of equations,\n\nequilibrium conditions,\n\ncomparative statics.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#interpretation","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#the-hessian-matrix","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"content":"For a scalar-valued function f(x_1,\\dots,x_n), the Hessian matrix collects\nsecond-order partial derivatives:\n$$\nH(x) =\\begin{pmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{pmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#the-hessian-matrix","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#properties","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"content":"Under mild regularity conditions, the Hessian is symmetric.\n\nThe Hessian summarizes the curvature of a function.\n\nIt plays a crucial role in second-order conditions for optimization.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#properties","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"type":"lvl2","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#quadratic-forms","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"content":"Given a symmetric matrix A and a vector x, the expressionx' A x\n\nis called a quadratic form.\n\nQuadratic forms arise naturally when:\n\napproximating functions locally,\n\nanalyzing curvature,\n\nstudying stability.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#quadratic-forms","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"type":"lvl3","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#example","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"content":"If\n$$\nA =\\begin{pmatrix}\na & b \\\\\nb & c\n\\end{pmatrix}\n\n\\quad\nx =\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}then\n\nx’Ax = a x_1^2 + 2b x_1 x_2 + c x_2^2.\n$$","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#example","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"type":"lvl2","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#definiteness-of-a-matrix","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"content":"A symmetric matrix A is:\n\nPositive definite if x'Ax > 0 for all x \\neq 0\n\nNegative definite if x'Ax < 0 for all x \\neq 0\n\nIndefinite if x'Ax takes both positive and negative values","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#definiteness-of-a-matrix","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"type":"lvl3","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#economic-meaning","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"content":"Positive definiteness corresponds to convexity.\n\nNegative definiteness corresponds to concavity.\n\nThese properties determine whether a critical point is a minimum or maximum.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#economic-meaning","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl2","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#principal-minors-and-the-determinant-test","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"content":"For a symmetric matrix, definiteness can be checked using principal minors.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#principal-minors-and-the-determinant-test","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#example-2-2-case","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"content":"Let\n$$\nA =\\begin{pmatrix}\na & b \\\\\nb & c\n\\end{pmatrix}\n\n$$\n\nA is positive definite if:\na > 0, \\quad \\det(A) = ac - b^2 > 0.\n\nThis test is especially useful in applied work, where symbolic eigenvalues\nmay be difficult to compute.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#example-2-2-case","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"type":"lvl2","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#eigenvalues-and-definiteness","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Eigenvalues and Definiteness"},"content":"An alternative characterization uses eigenvalues.\n\nA symmetric matrix is:\n\npositive definite if all eigenvalues are positive,\n\nnegative definite if all eigenvalues are negative.\n\nEigenvalues provide geometric intuition: they describe how a quadratic form\nstretches space along principal directions.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#eigenvalues-and-definiteness","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#the-bordered-hessian","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Bordered Hessian"},"content":"When optimizing a function subject to constraints, the bordered Hessian\nis used to check second-order conditions.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#the-bordered-hessian","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#structure","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Structure","lvl2":"The Bordered Hessian"},"content":"For a problem with one constraint, the bordered Hessian takes the form:\n$$\nH_B =\\begin{pmatrix}\n0 & g_x' \\\\\ng_x & H\n\\end{pmatrix}\n\n$$\nwhere:\n\ng_x is the gradient of the constraint,\n\nH is the Hessian of the Lagrangian.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#structure","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"type":"lvl3","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#interpretation-1","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Bordered Hessian"},"content":"The bordered Hessian incorporates both curvature and constraints.\n\nThe sign pattern of its principal minors determines optimality.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#interpretation-1","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"type":"lvl2","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#input-output-matrices-in-economics","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"content":"Linear algebra is central to input–output analysis.\n\nLet A be the matrix of technical coefficients, and let x denote output.\nThe system:x = Ax + d\n\ncan be rewritten as:(I - A)x = d.\n\nIf (I - A) is invertible, the solution is:x = (I - A)^{-1} d.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#input-output-matrices-in-economics","position":31},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"type":"lvl3","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#economic-meaning-1","position":32},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"content":"(I - A)^{-1} is the Leontief inverse.\n\nIt captures direct and indirect production requirements.\n\nInvertibility reflects feasibility and stability of production systems.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#economic-meaning-1","position":33},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"type":"lvl2","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#why-these-matrices-matter","position":34},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"content":"Special matrices allow us to:\n\ncharacterize equilibrium behavior,\n\ndetermine optimality,\n\nanalyze stability,\n\nunderstand economic structure.\n\nThey form the mathematical backbone of:\n\nconstrained optimization,\n\ndynamic systems,\n\ngeneral equilibrium models.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#why-these-matrices-matter","position":35},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#what-comes-next","position":36},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"content":"With these tools in place, we are ready to move beyond linear algebra and\nintroduce calculus-based methods, beginning with multivariate calculus\nand optimization.","type":"content","url":"/linear-algebra-speci-bc8d16fdd527254ff40414eec1f47#what-comes-next","position":37},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"type":"lvl1","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774","position":0},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications"},"content":"This chapter introduces special matrices that play a central role in\noptimization, comparative statics, and economic modeling. These matrices\nsummarize how systems respond to changes and how curvature determines optimality.\n\nThe focus is on interpretation and application, rather than abstract theory.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774","position":1},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#the-jacobian-matrix","position":2},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Jacobian Matrix"},"content":"Consider a vector-valued function:\n$$\nf(x) =\\begin{bmatrix}\nf_1(x_1,\\dots,x_n) \\\\\n\\vdots \\\\\nf_m(x_1,\\dots,x_n)\n\\end{bmatrix}\n\n$$\n\nThe Jacobian matrix of f is the matrix of first-order partial derivatives:\n$$\nJ(x) =\\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#the-jacobian-matrix","position":3},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#interpretation","position":4},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"The Jacobian Matrix"},"content":"Each row shows how one equation responds to changes in variables.\n\nThe Jacobian generalizes the derivative to multivariate settings.\n\nIn economics, Jacobians appear in:\n\nsystems of equations,\n\nequilibrium conditions,\n\ncomparative statics.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#interpretation","position":5},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"type":"lvl2","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#the-hessian-matrix","position":6},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"The Hessian Matrix"},"content":"For a scalar-valued function f(x_1,\\dots,x_n), the Hessian matrix collects\nsecond-order partial derivatives:\n$$\nH(x) =\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}\n\n$$","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#the-hessian-matrix","position":7},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"type":"lvl3","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#properties","position":8},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Properties","lvl2":"The Hessian Matrix"},"content":"Under mild regularity conditions, the Hessian is symmetric.\n\nThe Hessian summarizes the curvature of a function.\n\nIt plays a crucial role in second-order conditions for optimization.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#properties","position":9},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"type":"lvl2","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#quadratic-forms","position":10},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Quadratic Forms"},"content":"Given a symmetric matrix A and a vector x, the expressionx' A x\n\nis called a quadratic form.\n\nQuadratic forms arise naturally when:\n\napproximating functions locally,\n\nanalyzing curvature,\n\nstudying stability.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#quadratic-forms","position":11},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"type":"lvl3","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#example","position":12},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example","lvl2":"Quadratic Forms"},"content":"If\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}\n\n\\quad\nx =\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}then\n\nx’Ax = a x_1^2 + 2b x_1 x_2 + c x_2^2.\n$$","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#example","position":13},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"type":"lvl2","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#definiteness-of-a-matrix","position":14},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Definiteness of a Matrix"},"content":"A symmetric matrix A is:\n\nPositive definite if x'Ax > 0 for all x \\neq 0\n\nNegative definite if x'Ax < 0 for all x \\neq 0\n\nIndefinite if x'Ax takes both positive and negative values","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#definiteness-of-a-matrix","position":15},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"type":"lvl3","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#economic-meaning","position":16},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Definiteness of a Matrix"},"content":"Positive definiteness corresponds to convexity.\n\nNegative definiteness corresponds to concavity.\n\nThese properties determine whether a critical point is a minimum or maximum.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#economic-meaning","position":17},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl2","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#principal-minors-and-the-determinant-test","position":18},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Principal Minors and the Determinant Test"},"content":"For a symmetric matrix, definiteness can be checked using principal minors.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#principal-minors-and-the-determinant-test","position":19},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#example-2-2-case","position":20},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Example (2 × 2 case)","lvl2":"Principal Minors and the Determinant Test"},"content":"Let\n$$\nA =\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{pbatrix}.\n$$\n\n- $A$ is positive definite if:\n  $$\n  a > 0, \\quad \\det(A) = ac - b^2 > 0.\n  $$\n\nThis test is especially useful in applied work, where symbolic eigenvalues\nmay be difficult to compute.\n\n---\n\n## Eigenvalues and Definiteness\n\nAn alternative characterization uses **eigenvalues**.\n\nA symmetric matrix is:\n- positive definite if all eigenvalues are positive,\n- negative definite if all eigenvalues are negative.\n\nEigenvalues provide geometric intuition: they describe how a quadratic form\nstretches space along principal directions.\n\n---\n\n## The Bordered Hessian\n\nWhen optimizing a function subject to constraints, the **bordered Hessian**\nis used to check second-order conditions.\n\n### Structure\n\nFor a problem with one constraint, the bordered Hessian takes the form:\n$$\nH_B =\n\\begin{bmatrix}\n0 & g_x' \\\\\ng_x & H\n\\end{bmatrix}\n\n$$\nwhere:\n\ng_x is the gradient of the constraint,\n\nH is the Hessian of the Lagrangian.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#example-2-2-case","position":21},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"Principal Minors and the Determinant Test"},"type":"lvl3","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#interpretation-1","position":22},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Interpretation","lvl2":"Principal Minors and the Determinant Test"},"content":"The bordered Hessian incorporates both curvature and constraints.\n\nThe sign pattern of its principal minors determines optimality.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#interpretation-1","position":23},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"type":"lvl2","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#input-output-matrices-in-economics","position":24},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Input–Output Matrices in Economics"},"content":"Linear algebra is central to input–output analysis.\n\nLet A be the matrix of technical coefficients, and let x denote output.\nThe system:x = Ax + d\n\ncan be rewritten as:(I - A)x = d.\n\nIf (I - A) is invertible, the solution is:x = (I - A)^{-1} d.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#input-output-matrices-in-economics","position":25},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"type":"lvl3","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#economic-meaning-1","position":26},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl3":"Economic Meaning","lvl2":"Input–Output Matrices in Economics"},"content":"(I - A)^{-1} is the Leontief inverse.\n\nIt captures direct and indirect production requirements.\n\nInvertibility reflects feasibility and stability of production systems.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#economic-meaning-1","position":27},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"type":"lvl2","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#why-these-matrices-matter","position":28},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"Why These Matrices Matter"},"content":"Special matrices allow us to:\n\ncharacterize equilibrium behavior,\n\ndetermine optimality,\n\nanalyze stability,\n\nunderstand economic structure.\n\nThey form the mathematical backbone of:\n\nconstrained optimization,\n\ndynamic systems,\n\ngeneral equilibrium models.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#why-these-matrices-matter","position":29},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"type":"lvl2","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#what-comes-next","position":30},{"hierarchy":{"lvl1":"Linear Algebra: Special Matrices and Applications","lvl2":"What Comes Next"},"content":"With these tools in place, we are ready to move beyond linear algebra and\nintroduce calculus-based methods, beginning with multivariate calculus\nand optimization.","type":"content","url":"/linear-algebra-speci-fb3ed59ecdc0bd04e0ccb26f87774#what-comes-next","position":31},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/linear-algebra-speci-fc0090f79c1489d9b4765230e02ea","position":0},{"hierarchy":{"lvl1":""},"content":"","type":"content","url":"/linear-algebra-speci-fc0090f79c1489d9b4765230e02ea","position":1},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices"},"type":"lvl1","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf","position":0},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices"},"content":"Let’s look at some special matrices and determinants that are useful in economic analysis.","type":"content","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf","position":1},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"type":"lvl2","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#the-jacobian","position":2},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"content":"The Jacobian (named after German mathematician Karl Gustav Jacobi, 1804–1851) is generally used in conjunction with partial derivatives to provide an easy test for the existence of functional dependence (linear and nonlinear). A Jacobian determinant |\\mathbf{J}| is composed of all the first‑order partial derivatives of a system of equations, arranged in order sequence. Giveny_{1} = f_{1}(x_{1},x_{2},x_{3}) \\\\\ny_{2} = f_{2}(x_{1},x_{2},x_{3}) \\\\\ny_{3} = f_{3}(x_{1},x_{2},x_{3})\n\nThen|\\mathbf{J}| = \n\\begin{vmatrix}\n\\frac{\\partial y_{1}}{\\partial x_{1}} & \\frac{\\partial y_{1}}{\\partial x_{2}} & \\frac{\\partial y_{1}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} & \\frac{\\partial y_{2}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{3}}{\\partial x_{1}} & \\frac{\\partial y_{3}}{\\partial x_{2}} & \\frac{\\partial y_{3}}{\\partial x_{3}}\n\\end{vmatrix}\n\nFor example, say you havey_{1} = 5x_{1} + 3x_{2} \\\\\ny_{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}\n\nThen taking the first‑order partials gives\\frac{\\partial y_1}{\\partial x_1} = 5,\\qquad \n\\frac{\\partial y_1}{\\partial x_2} = 3,\\qquad \n\\frac{\\partial y_2}{\\partial x_1} = 50x_1 + 30x_2,\\qquad \n\\frac{\\partial y_2}{\\partial x_2} = 30x_1 + 18x_2\n\nAnd the Jacobian is|\\mathbf{J}| = \n\\begin{vmatrix}\n5 & 3 \\\\\n50x_1 + 30x_2 & 30x_1 + 18x_2\n\\end{vmatrix}\n\nThe determinant of the Jacobian matrix is|\\mathbf{J}| = 5(30x_1 + 18x_2) - 3(50x_1 + 30x_2) = 0\n\nSince |\\mathbf{J}| = 0, there is functional dependence between the equations. (Note: (5x_{1} + 3x_{2})^{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}.)","type":"content","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#the-jacobian","position":3},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Hessian"},"type":"lvl2","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#the-hessian","position":4},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Hessian"},"content":"Given that the first‑order conditions z_{x} = z_{y} = 0 are met, a sufficient condition for a multivariate function z = f(x,y) to be an optimum is\n\nz_{xx}, z_{yy} > 0 for a minimum; z_{xx}, z_{yy} < 0 for a maximum.\n\nz_{xx} z_{yy} > (z_{xy})^{2}.\n\nA convenient test for this second‑order condition is provided by the Hessian. A Hessian |\\mathbf{H}| is a determinant composed of all the second‑order partial derivatives, with the second‑order direct partials on the principal diagonal and the second‑order cross partials off the principal diagonal. That is,|\\mathbf{H}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n\nwhere (by Young’s Theorem) z_{xy} = z_{yx}.\n\nIf the first element on the principal diagonal, a.k.a. the first principal minor, |H_{1}| = z_{xx} is positive, and the second principal minor|H_{2}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n= z_{xx} z_{yy} - (z_{xy})^{2} > 0,\n\nthen the second‑order conditions for a minimum are set. That is, when |H_{1}| > 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be positive definite. And a positive definite Hessian fulfills the second‑order conditions for a minimum.\n\nIf the first principal minor |H_{1}| < 0, while the second principal minor remains positive, then the second‑order conditions for a maximum are met. That is, when |H_{1}| < 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example:\n\nGivenz = f(x,y) = 2x^{2} - xy + 2y^{2} - 5x - 6y + 20,\n\nthenz_x = 4x - y - 5,\\quad z_y = -x + 4y - 6,\\quad z_{xx} = 4,\\quad z_{yy} = 4,\\quad z_{xy} = z_{yx} = -1.\n\nThe Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\n4 & -1 \\\\\n-1 & 4\n\\end{vmatrix}\n\nWe have |H_{1}| = 4 > 0 and |H_{2}| = (4)(4) - (-1)^2 = 16 - 1 = 15 > 0. Since |H_{1}| > 0 and |H_{2}| > 0, i.e. the Hessian is positive definite, then the function z is characterized by a minimum at the critical values (can you find these?).","type":"content","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#the-hessian","position":5},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"type":"lvl2","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#the-discriminant","position":6},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"content":"Determinants can be used to test for positive and negative definiteness of any quadratic form. The determinant of a quadratic form is called a discriminant |\\mathbf{D}|. Given the quadratic formz = a x^{2} + b x y + c y^{2},\n\nthe determinant is formed by placing the coefficients of the squared terms on the principal diagonal and dividing the coefficients of the non‑squared terms equally between the off‑diagonal positions. Hence, we have|\\mathbf{D}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n\nWe then evaluate the principal minors like we did for the Hessian test, where|D_{1}| = a \\quad \\text{and} \\quad |D_{2}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n= a c - \\frac{b^{2}}{4}.\n\nIf |D_{1}|, |D_{2}| > 0, |\\mathbf{D}| is positive definite and z is positive for all nonzero values of x and y. If |D_{1}| < 0 and |D_{2}| > 0, |\\mathbf{D}| is negative definite and z is negative for all nonzero values of x and y. If |D_{2}| \\neq 0, z is not sign definite and z may assume both positive and negative values.\n\nLet’s take an example to test for sign definiteness of the following quadratic formz = 2x^{2} + 5xy + 8y^{2}.\n\nWe can easily form the discriminant|\\mathbf{D}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n\nThen evaluating the principal minors gives|D_{1}| = 2 > 0, \\qquad |D_{2}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n= 16 - 6.25 = 9.75 > 0.\n\nHence, z is positive definite, meaning that it will be greater than zero for all nonzero values of x and y.","type":"content","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#the-discriminant","position":7},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"type":"lvl2","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#the-quadratic-form","position":8},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"content":"A quadratic form is defined as a polynomial expression in which each component term has a uniform degree.\n\nHere are some examples:6x^{2} - 2xy + 3y^{2} \\quad \\text{is a quadratic form in 2 variables.} \\\\\nx^{2} + 2xy + 4xz + 2yz + y^{2} + z^{2} \\quad \\text{is a quadratic form in 3 variables.}\n\nIt would be useful to determine the sign definiteness so we can make statements concerning the optimum value of the function as to whether it is a minimum or a maximum.\n\nMore generally, a quadratic form in n variables (x_{1},x_{2},\\ldots ,x_{n}) can be written as \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} where\n\n\\mathbf{x}^{\\prime} is a row vector [x_{1},x_{2},\\dots,x_{n}] and\\mathbf{A} is an n\\times n matrix of scalar elements.\n\nTo see this more clearly,\\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = \n\\begin{bmatrix}\nx_{1} & x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 3 \\\\\n4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= \n\\begin{bmatrix}\nx_{1} + 4x_{2} & 3x_{1} + 5x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= x_{1}^{2} + 7x_{1}x_{2} + 5x_{2}^{2}.\n\nA quadratic form is said to be:\n\na) Positive definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} > 0 for all \\mathbf{x} \\neq \\mathbf{0}.b) Positive semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\geq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.c) Negative definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} < 0 for all \\mathbf{x} \\neq \\mathbf{0}.d) Negative semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\leq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.e) Sign indefinite if it takes both positive and negative values.\n\nQuadratic forms are particularly useful in determining the concavity or convexity of a differentiable function.\n\nFor a function y = f(x_{1},x_{2},\\dots,x_{n}), we can form the Hessian consisting of second‑order partial derivatives and construct a quadratic form as \\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x}. Then\n\na) The function y is strictly convex if the quadratic form is positive (implying that the Hessian is positive definite, i.e., the determinants of the principal minors are all positive).b) The function y is strictly concave if the quadratic form is negative (that is, the Hessian is negative definite, i.e. the determinants of the principal minors alternate in sign).\n\nLet’s take an example to see this more clearly.\n\nSay we havez = x^{2} + y^{2}.\n\nThenz_{x} = 2x,\\quad z_{xx} = 2,\\quad z_{xy} = 0, \\\\\nz_{y} = 2y,\\quad z_{yx} = 0,\\quad z_{yy} = 2.\n\nBut\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 2x^{2} + 2y^{2} > 0.\n\nHence the function z = x^{2} + y^{2} is characterized by a minimum at its optimal value and is therefore a strictly convex function. And it is easy to see that the Hessian is positive definite!\n\nHere’s another example:\n\nSay we have a quadratic form in 2 variablesz = 8x^{2} + 6xy + 2y^{2}.\n\nThis can be rearranged asz = 8x^{2} + 3xy + 3xy + 2y^{2}.\n\nWe can write this as\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 8x^{2} + 2y^{2} + (3)(3)xy = 8x^{2} + 2y^{2} + 9xy.\n\nThe Hessian is\\mathbf{H} = \n\\begin{bmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{bmatrix}\n\nThe principal minors are |H_{1}| = 8 and $|H_{2}| =\\begin{vmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{vmatrix}\n\nBoth are positive so we have the Hessian and the quadratic form as positive definite.","type":"content","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#the-quadratic-form","position":9},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"type":"lvl2","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#higher-order-hessian","position":10},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"content":"Given y = f(x_{1},x_{2},x_{3}), the third‑order Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\ny_{11} & y_{12} & y_{13} \\\\\ny_{21} & y_{22} & y_{23} \\\\\ny_{31} & y_{32} & y_{33}\n\\end{vmatrix}\n\nwhere the elements are the various second‑order partial derivatives of y:y_{11} = \\frac{\\partial^{2}y}{\\partial x_{1}^{2}},\\quad \ny_{12} = \\frac{\\partial^{2}y}{\\partial x_{2}\\partial x_{1}},\\quad \ny_{23} = \\frac{\\partial^{2}y}{\\partial x_{3}\\partial x_{2}},\\quad \\text{etc.}\n\nConditions for a relative minimum or maximum depend on the signs of the first, second, and third principal minors, respectively. If|H_{1}| > 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| > 0,\n\nthen |\\mathbf{H}| is positive definite and fulfills the second‑order conditions for a minimum.\n\nIf|H_{1}| < 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| < 0,\n\nthen |\\mathbf{H}| is negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example.\n\nGiven the functiony = -5x_{1}^{2} + 10x_{1} + x_{1}x_{3} - 2x_{2}^{2} + 4x_{2} + 2x_{2}x_{3} - 4x_{3}^{2}.\n\nThe first order conditions (F.O.C) are\\frac{\\partial y}{\\partial x_1} = y_1 = -10x_1 + 10 + x_3 = 0, \\\\\n\\frac{\\partial y}{\\partial x_2} = y_2 = -4x_2 + 2x_3 + 4 = 0, \\\\\n\\frac{\\partial y}{\\partial x_3} = y_3 = x_1 + 2x_2 - 8x_3 = 0.\n\nwhich can be expressed in matrix form as\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-10 \\\\ -4 \\\\ 0\n\\end{bmatrix}.\n\nUsing Cramer’s rule we get x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43 (please verify this).\n\nLet’s take the second partial derivatives from the first‑order conditions to create the Hessian,y_{11} = -10,\\quad y_{12} = 0,\\quad y_{13} = 1, \\\\\ny_{21} = 0,\\quad y_{22} = -4,\\quad y_{23} = 2, \\\\\ny_{31} = 1,\\quad y_{32} = 2,\\quad y_{33} = -8.\n\nThus,\\mathbf{H} = \n\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\nFinally, applying the Hessian test, we have|H_1| = -10 < 0,\\quad \n|H_2| = \n\\begin{vmatrix}\n-10 & 0 \\\\\n0 & -4\n\\end{vmatrix} = 40 > 0,\\quad \n|H_3| = |\\mathbf{H}| = \n\\begin{vmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{vmatrix} = -80 < 0.\n\nSince the principal minors alternate correctly in sign, the Hessian is negative definite and the function is maximized at x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43.","type":"content","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#higher-order-hessian","position":11},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#the-bordered-hessian","position":12},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"content":"To optimize a function f(x_1, x_2) subject to a constraint g(x_1, x_2) the first‑order conditions can be found by setting up what is known as the Lagrangian function F(x_1, x_2, \\lambda) = f(x_1, x_2) - \\lambda(g(x_1, x_2) - c).\n\nThe second‑order conditions can be expressed in terms of a bordered Hessian |\\bar{\\mathbf{H}}| as|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & F_{11} & F_{12} \\\\\ng_2 & F_{21} & F_{22}\n\\end{vmatrix}\n\nor|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & f_{11} & f_{12} \\\\\ng_2 & f_{21} & f_{22}\n\\end{vmatrix}\n\nwhich is the usual Hessian bordered by the first derivatives of the constraint with zero on the principal diagonal.\n\nThe order of a bordered principal minor is determined by the order of the principal minor being bordered. Hence |\\bar{H}_2| above represents a second bordered principal minor, because the principal minor being bordered has dimensions 2\\times 2.\n\nFor the bivariate case with a single constraint, we simply look at |\\bar{H}_2|. If this is negative, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum. However, if it is positive, the bordered Hessian is said to be negative definite, and meets the sufficient conditions for a maximum.\n\nLet’s try optimizing the following objective functionf(x_1, x_2) = 4x_1^2 + 3x_2^2 - 2x_1x_2\n\nsubject tox_1 + x_2 = 56.\n\nSetting up the Lagrangian function, taking the first‑order partials and solving gives x_1^* = 36, x_2^* = 20 (and \\lambda = 348). (You should verify this.)\n\nThe bordered Hessian for this optimization problem is|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & 1 & 1 \\\\\n1 & 8 & -2 \\\\\n1 & -2 & 6\n\\end{vmatrix}\n\nStarting with the second principal minor, we have|\\bar{H}_2| = |\\bar{\\mathbf{H}}| = 0 \\cdot \n\\begin{vmatrix}\n8 & -2 \\\\\n-2 & 6\n\\end{vmatrix}\n- 1 \\cdot \n\\begin{vmatrix}\n1 & -2 \\\\\n1 & 6\n\\end{vmatrix}\n+ 1 \\cdot \n\\begin{vmatrix}\n1 & 8 \\\\\n1 & -2\n\\end{vmatrix}\n= - (6+2) + (-2-8) = -8 -10 = -18.\n\nwhich is negative, hence |\\bar{\\mathbf{H}}| is positive definite and we have met sufficient conditions for a minimum.\n\nFor the more general case in which the objective function has say n variables, i.e., f(x_1, \\ldots , x_n) which is subject to some constraint g(x_1, \\ldots , x_n), we can set up the bordered Hessian as|\\bar{\\mathbf{H}}| = \n\\begin{bmatrix}\n0 & g_1 & g_2 & \\dots & g_n \\\\\ng_1 & F_{11} & F_{12} & \\dots & F_{1n} \\\\\ng_2 & F_{21} & F_{22} & \\dots & F_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ng_n & F_{n1} & F_{n2} & \\dots & F_{nn}\n\\end{bmatrix}\n\nwhere |\\bar{\\mathbf{H}}| = |\\bar{H}_n| because the n \\times n principal minor is bordered.\n\nIn this case, if all the principal minors are negative, i.e. |\\bar{H}_2|, |\\bar{H}_3|, \\ldots , |\\bar{H}_n| < 0, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum.\n\nOn the other hand, if the principal minors alternate consistently in sign from positive to negative, i.e. |\\bar{H}_2| > 0, |\\bar{H}_3| < 0, |\\bar{H}_4| > 0 etc., the bordered Hessian is negative definite, and meets the sufficient conditions for a maximum.","type":"content","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#the-bordered-hessian","position":13},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"type":"lvl2","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#input-output-analysis","position":14},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"content":"If a_{ij} is a technical coefficient representing the value of input i required to produce one dollar’s worth of product j, the total demand for good i can be expressed asx_{i} = a_{i1}x_{1} + a_{i2}x_{2} + \\dots + a_{in}x_{n} + b_{i}\n\nwhere b_i is the final demand for product i. What is important to realize here is that the total demand for a product consists of that product being the final demand plus that product being an intermediate good required for the production of other products.\n\nIn matrix form we have\\mathbf{X} = \\mathbf{A}\\mathbf{X} + \\mathbf{B}\n\nwhere, for an n sector economy,\\mathbf{X} = \n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n\\quad\n\\mathbf{A} = \n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nn}\n\\end{bmatrix},\n\\quad\n\\mathbf{B} = \n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n\\end{bmatrix}.\n\n\\mathbf{A} is called the matrix of technical coefficients.\n\nTo find the output (intermediate and final goods) needed to satisfy demand, all we have to do is to solve for \\mathbf{X}:\\mathbf{X} - \\mathbf{A}\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad (\\mathbf{I} - \\mathbf{A})\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad \\mathbf{X} = (\\mathbf{I} - \\mathbf{A})^{-1}\\mathbf{B}.","type":"content","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#input-output-analysis","position":15},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"type":"lvl2","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":16},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"content":"For a square matrix \\mathbf{A}:\n\na) All characteristic roots \\lambda are positive \\Rightarrow \\mathbf{A} is positive definite.b) All \\lambda’s are negative \\Rightarrow \\mathbf{A} is negative definite.c) All \\lambda’s are nonnegative and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is positive semi‑definite.d) All \\lambda’s are nonpositive and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is negative semi‑definite.e) Some \\lambda’s are positive and some negative \\Rightarrow \\mathbf{A} is sign indefinite.\n\nLet’s take an example.\n\nGiven a square matrix\\mathbf{A} = \n\\begin{bmatrix}\n-6 & 3 \\\\\n3 & -6\n\\end{bmatrix}\n\nTo find the characteristic roots (eigenvalues) of \\mathbf{A}, we simply set |\\mathbf{A} - \\lambda \\mathbf{I}| = 0:|\\mathbf{A} - \\lambda \\mathbf{I}| = \n\\begin{vmatrix}\n-6 - \\lambda & 3 \\\\\n3 & -6 - \\lambda\n\\end{vmatrix} = 0\n\nThis means(-6 - \\lambda)(-6 - \\lambda) - 9 = 0 \\\\\n\\lambda^2 + 12\\lambda + 27 = 0 \\\\\n(\\lambda + 9)(\\lambda + 3) = 0 \\\\\n\\lambda = -9, -3.\n\nSince both characteristic roots \\lambda are negative, we say \\mathbf{A} is negative definite.\n\nNote:\\text{(i) } \\sum \\lambda_i = \\operatorname{tr}(\\mathbf{A}), \\qquad\n\\text{(ii) } \\prod \\lambda_i = |\\mathbf{A}|.\n\nLet’s continue with the example above to find the characteristic vector.\n\nWe know one of the roots \\lambda = -9, so substituting in the characteristic matrix gives(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{bmatrix}\n-6 - (-9) & 3 \\\\\n3 & -6 - (-9)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n3 & 3 \\\\\n3 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nSince the coefficient matrix is linearly dependent, there are infinite number of solutions. The product of the matrices gives two equations which are identical:3v_1 + 3v_2 = 0 \\quad \\Rightarrow \\quad v_2 = -v_1.\n\nBy normalizing we havev_1^{2} + v_2^{2} = 1.\n\nSubstituting v_2 = -v_1 givesv_1^{2} + (-v_1)^{2} = 1 \\quad \\Rightarrow \\quad 2v_1^{2} = 1 \\quad \\Rightarrow \\quad v_1^{2} = \\frac{1}{2}.\n\nTaking the positive square root gives v_1 = \\sqrt{1/2} = \\frac{\\sqrt{2}}{2} and substituting into v_2 = -v_1 gives v_2 = -\\frac{\\sqrt{2}}{2}. That is\\mathbf{v}_1 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n-\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.\n\nUsing the second characteristic root \\lambda = -3:(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \n\\begin{bmatrix}\n-6 - (-3) & 3 \\\\\n3 & -6 - (-3)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-3 & 3 \\\\\n3 & -3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nMultiplying out gives -3v_1 + 3v_2 = 0 and 3v_1 - 3v_2 = 0, so v_1 = v_2.\n\nNormalizing as before:v_1^2 + v_2^2 = 1 \\quad \\Rightarrow \\quad 2v_1^2 = 1 \\quad \\Rightarrow \\quad v_1 = \\frac{\\sqrt{2}}{2}.\n\nHence,\\mathbf{v}_2 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-speci-fc4a3872cb330a9684bed77d421bf#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":17},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nOptimization\n\nIntegration\n\nDifference equations\n\nDifferential equations\n\nNotes prepared by -YY- (please do not distribute without permission) test ","type":"content","url":"/#topics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics"},"type":"lvl1","url":"/linear-algebra-basic","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics"},"content":"","type":"content","url":"/linear-algebra-basic","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}' = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.","type":"content","url":"/linear-algebra-basic#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Some Notes on Matrices"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:3x_1 + 4x_2 = 5 \\\\\n7x_1 - 2x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\\\ 7x_1 - 2x_2 \\end{pmatrix}\n\nThe original system is in fact equivalent to the matrix form:\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic#matrix-inversion","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Inversion"},"content":"To better understand the mechanics of finding the inverse of a square matrix, let’s begin by looking at an example.\\mathbf{A} = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nAssume we have:\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix}\n\nWhere does the 10 in the matrix after the = sign come from? It is the determinant of \\mathbf{A}.\n\nNow we have:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 10 & 0 \\\\ 0 & 10 \\end{bmatrix} = 10 \\mathbf{I}\n\nHence, (pre-)multiplying both sides of the equation by \\frac{1}{10} will give you:\\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\mathbf{I}\n\nThe matrix \\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} is in fact the inverse of \\mathbf{A}. Hence we have the relationship \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}. In fact, \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.\n\nHere are some properties of the inverse of a matrix worth knowing:\n\nProperty 1: For any nonsingular matrix \\mathbf{A}, (\\mathbf{A}^{-1})^{-1} = \\mathbf{A}.\n\nProperty 2: The determinant of the inverse of a matrix is equal to the reciprocal of the determinant of the matrix. That is |\\mathbf{A}^{-1}| = \\frac{1}{|\\mathbf{A}|}.\n\nProperty 3: The inverse of matrix \\mathbf{A} is unique.\n\nProperty 4: For any nonsingular matrix \\mathbf{A}, the inverse of the transpose of a matrix is equal to the transpose of the inverse of the matrix. (\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'.\n\nProperty 5: If \\mathbf{A} and \\mathbf{B} are nonsingular and of the same dimension, then \\mathbf{A}\\mathbf{B} is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices is equal to the product of their inverses in reverse order. (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}.","type":"content","url":"/linear-algebra-basic#matrix-inversion","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar value uniquely associated with a square non-singular matrix, and is usually denoted as |\\mathbf{A}|. The question of whether or not a matrix is nonsingular and therefore invertible is linked to the value of its determinant.\n\nWe can write a generic 2 \\times 2 matrix as:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}\n\nand its determinant is defined as:|\\mathbf{A}| = a_{11}a_{22} - a_{12}a_{21}\n\nFor a 3 \\times 3 matrix, say:\\mathbf{B} = \\begin{bmatrix} b_{11} & b_{12} & b_{13} \\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{bmatrix}\n\nits determinant is:|\\mathbf{B}| = b_{11}(b_{22}b_{33} - b_{23}b_{32}) - b_{12}(b_{21}b_{33} - b_{23}b_{31}) + b_{13}(b_{21}b_{32} - b_{22}b_{31})","type":"content","url":"/linear-algebra-basic#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic#properties-of-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"content":"|\\mathbf{A}| = |\\mathbf{A}'| and |\\mathbf{A}| \\cdot |\\mathbf{B}| = |\\mathbf{B}| \\cdot |\\mathbf{A}|.\n\nInterchanging any two rows or columns will affect the sign of the determinant, but not its absolute value.\n\nMultiplying a single row or column by a scalar will cause the value of the determinant to be multiplied by the scalar.\n\nAddition or subtraction of a nonzero multiple of any row or column to or from another row or column does not change the value of the determinant.\n\nThe determinant of a triangular matrix is equal to the product of the elements along the principal diagonal.\n\nIf any of the rows or columns equal zero, the determinant is also zero.\n\nIf two rows or columns are identical or proportional, i.e. linearly dependent, then the determinant is zero.","type":"content","url":"/linear-algebra-basic#properties-of-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix"},"content":"The inverse of a matrix is defined as:\\mathbf{A}^{-1} = \\frac{1}{|\\mathbf{A}|} \\operatorname{adj}(\\mathbf{A})\n\nLet’s say we have the following linear system:2x_1 + 9x_2 + 4x_3 = -13 \\\\\n7x_1 - 8x_2 + 6x_3 = 63 \\\\\n5x_1 + 3x_2 - 6x_3 = 6\n\nWriting the above system in the form \\mathbf{A}\\mathbf{x} = \\mathbf{b}, where:\\mathbf{A} = \\begin{bmatrix} 2 & 9 & 4 \\\\ 7 & -8 & 6 \\\\ 5 & 3 & -6 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nWe can therefore solve the system by \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}, which means that we need the inverse of \\mathbf{A}.\n\nStep 1. The minor |M_{ij}| is the determinant of the submatrix formed by deleting the i-th row and j-th column of the original matrix. So we have:\\begin{bmatrix}\n30 & -72 & 61 \\\\\n-66 & -32 & -39 \\\\\n86 & -16 & -79\n\\end{bmatrix}\n\nStep 2: The cofactor C_{ij} is a minor with the prescribed sign that follows the rule:C_{ij} = (-1)^{i+j} |M_{ij}|\n\nHence, we have:\\begin{bmatrix}\n30 & 72 & 61 \\\\\n66 & -32 & 39 \\\\\n86 & 16 & -79\n\\end{bmatrix}\n\nStep 3: An adjoint matrix is simply the transpose of a cofactor matrix. Continuing the example above, we have:\\operatorname{adj}(\\mathbf{A}) = \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nSince |\\mathbf{A}| = 952, we then have:\\mathbf{A}^{-1} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nand\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix} \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nNot a nice one to solve, though.","type":"content","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"content":"Cramer came up with a simplified method for solving a system of linear equations through the use of determinants.\n\nBasically, given \\mathbf{A}\\mathbf{x} = \\mathbf{b}, we have:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n\nDefine:\\Delta = |\\mathbf{A}|\n\nand\\Delta_1 = \\begin{vmatrix} b_1 & a_{12} & a_{13} \\\\ b_2 & a_{22} & a_{23} \\\\ b_3 & a_{32} & a_{33} \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} a_{11} & b_1 & a_{13} \\\\ a_{21} & b_2 & a_{23} \\\\ a_{31} & b_3 & a_{33} \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} a_{11} & a_{12} & b_1 \\\\ a_{21} & a_{22} & b_2 \\\\ a_{31} & a_{32} & b_3 \\end{vmatrix}\n\nwhere the columns in the \\mathbf{A} matrix are replaced one by one by the \\mathbf{b} vector as shown above.\n\nThen:x_i = \\frac{\\Delta_i}{\\Delta}\n\nVerify that you get \\{4.5, -3, 1.25\\}.\n\nLinear algebra 1-5.pptx (17107k) 8 Jan 31, 2018, 7:56 AM v.1Linear algebra 6.pptx (3924k) 8 Feb 5, 2018, 7:15 AM v.1","type":"content","url":"/linear-algebra-basic#cramers-rule","position":19},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices"},"type":"lvl1","url":"/linear-algebra-special","position":0},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices"},"content":"Let’s look at some special matrices and determinants that are useful in economic analysis.","type":"content","url":"/linear-algebra-special","position":1},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"type":"lvl2","url":"/linear-algebra-special#the-jacobian","position":2},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"content":"The Jacobian (named after German mathematician Karl Gustav Jacobi, 1804–1851) is generally used in conjunction with partial derivatives to provide an easy test for the existence of functional dependence (linear and nonlinear). A Jacobian determinant |\\mathbf{J}| is composed of all the first‑order partial derivatives of a system of equations, arranged in order sequence. Giveny_{1} = f_{1}(x_{1},x_{2},x_{3}) \\\\\ny_{2} = f_{2}(x_{1},x_{2},x_{3}) \\\\\ny_{3} = f_{3}(x_{1},x_{2},x_{3})\n\nThen|\\mathbf{J}| = \n\\begin{vmatrix}\n\\frac{\\partial y_{1}}{\\partial x_{1}} & \\frac{\\partial y_{1}}{\\partial x_{2}} & \\frac{\\partial y_{1}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} & \\frac{\\partial y_{2}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{3}}{\\partial x_{1}} & \\frac{\\partial y_{3}}{\\partial x_{2}} & \\frac{\\partial y_{3}}{\\partial x_{3}}\n\\end{vmatrix}\n\nFor example, say you havey_{1} = 5x_{1} + 3x_{2} \\\\\ny_{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}\n\nThen taking the first‑order partials gives\\frac{\\partial y_1}{\\partial x_1} = 5,\\qquad \n\\frac{\\partial y_1}{\\partial x_2} = 3,\\qquad \n\\frac{\\partial y_2}{\\partial x_1} = 50x_1 + 30x_2,\\qquad \n\\frac{\\partial y_2}{\\partial x_2} = 30x_1 + 18x_2\n\nAnd the Jacobian is|\\mathbf{J}| = \n\\begin{vmatrix}\n5 & 3 \\\\\n50x_1 + 30x_2 & 30x_1 + 18x_2\n\\end{vmatrix}\n\nThe determinant of the Jacobian matrix is|\\mathbf{J}| = 5(30x_1 + 18x_2) - 3(50x_1 + 30x_2) = 0\n\nSince |\\mathbf{J}| = 0, there is functional dependence between the equations. (Note: (5x_{1} + 3x_{2})^{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}.)","type":"content","url":"/linear-algebra-special#the-jacobian","position":3},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Hessian"},"type":"lvl2","url":"/linear-algebra-special#the-hessian","position":4},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Hessian"},"content":"Given that the first‑order conditions z_{x} = z_{y} = 0 are met, a sufficient condition for a multivariate function z = f(x,y) to be an optimum is\n\nz_{xx}, z_{yy} > 0 for a minimum; z_{xx}, z_{yy} < 0 for a maximum.\n\nz_{xx} z_{yy} > (z_{xy})^{2}.\n\nA convenient test for this second‑order condition is provided by the Hessian. A Hessian |\\mathbf{H}| is a determinant composed of all the second‑order partial derivatives, with the second‑order direct partials on the principal diagonal and the second‑order cross partials off the principal diagonal. That is,|\\mathbf{H}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n\nwhere (by Young’s Theorem) z_{xy} = z_{yx}.\n\nIf the first element on the principal diagonal, a.k.a. the first principal minor, |H_{1}| = z_{xx} is positive, and the second principal minor|H_{2}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n= z_{xx} z_{yy} - (z_{xy})^{2} > 0,\n\nthen the second‑order conditions for a minimum are set. That is, when |H_{1}| > 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be positive definite. And a positive definite Hessian fulfills the second‑order conditions for a minimum.\n\nIf the first principal minor |H_{1}| < 0, while the second principal minor remains positive, then the second‑order conditions for a maximum are met. That is, when |H_{1}| < 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example:\n\nGivenz = f(x,y) = 2x^{2} - xy + 2y^{2} - 5x - 6y + 20,\n\nthenz_x = 4x - y - 5,\\quad z_y = -x + 4y - 6,\\quad z_{xx} = 4,\\quad z_{yy} = 4,\\quad z_{xy} = z_{yx} = -1.\n\nThe Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\n4 & -1 \\\\\n-1 & 4\n\\end{vmatrix}\n\nWe have |H_{1}| = 4 > 0 and |H_{2}| = (4)(4) - (-1)^2 = 16 - 1 = 15 > 0. Since |H_{1}| > 0 and |H_{2}| > 0, i.e. the Hessian is positive definite, then the function z is characterized by a minimum at the critical values (can you find these?).","type":"content","url":"/linear-algebra-special#the-hessian","position":5},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"type":"lvl2","url":"/linear-algebra-special#the-discriminant","position":6},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"content":"Determinants can be used to test for positive and negative definiteness of any quadratic form. The determinant of a quadratic form is called a discriminant |\\mathbf{D}|. Given the quadratic formz = a x^{2} + b x y + c y^{2},\n\nthe determinant is formed by placing the coefficients of the squared terms on the principal diagonal and dividing the coefficients of the non‑squared terms equally between the off‑diagonal positions. Hence, we have|\\mathbf{D}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n\nWe then evaluate the principal minors like we did for the Hessian test, where|D_{1}| = a \\quad \\text{and} \\quad |D_{2}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n= a c - \\frac{b^{2}}{4}.\n\nIf |D_{1}|, |D_{2}| > 0, |\\mathbf{D}| is positive definite and z is positive for all nonzero values of x and y. If |D_{1}| < 0 and |D_{2}| > 0, |\\mathbf{D}| is negative definite and z is negative for all nonzero values of x and y. If |D_{2}| \\neq 0, z is not sign definite and z may assume both positive and negative values.\n\nLet’s take an example to test for sign definiteness of the following quadratic formz = 2x^{2} + 5xy + 8y^{2}.\n\nWe can easily form the discriminant|\\mathbf{D}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n\nThen evaluating the principal minors gives|D_{1}| = 2 > 0, \\qquad |D_{2}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n= 16 - 6.25 = 9.75 > 0.\n\nHence, z is positive definite, meaning that it will be greater than zero for all nonzero values of x and y.","type":"content","url":"/linear-algebra-special#the-discriminant","position":7},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"type":"lvl2","url":"/linear-algebra-special#the-quadratic-form","position":8},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"content":"A quadratic form is defined as a polynomial expression in which each component term has a uniform degree.\n\nHere are some examples:6x^{2} - 2xy + 3y^{2} \\quad \\text{is a quadratic form in 2 variables.} \\\\\nx^{2} + 2xy + 4xz + 2yz + y^{2} + z^{2} \\quad \\text{is a quadratic form in 3 variables.}\n\nIt would be useful to determine the sign definiteness so we can make statements concerning the optimum value of the function as to whether it is a minimum or a maximum.\n\nMore generally, a quadratic form in n variables (x_{1},x_{2},\\ldots ,x_{n}) can be written as \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} where\n\n\\mathbf{x}^{\\prime} is a row vector [x_{1},x_{2},\\dots,x_{n}] and\\mathbf{A} is an n\\times n matrix of scalar elements.\n\nTo see this more clearly,\\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = \n\\begin{bmatrix}\nx_{1} & x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 3 \\\\\n4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= \n\\begin{bmatrix}\nx_{1} + 4x_{2} & 3x_{1} + 5x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= x_{1}^{2} + 7x_{1}x_{2} + 5x_{2}^{2}.\n\nA quadratic form is said to be:\n\na) Positive definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} > 0 for all \\mathbf{x} \\neq \\mathbf{0}.b) Positive semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\geq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.c) Negative definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} < 0 for all \\mathbf{x} \\neq \\mathbf{0}.d) Negative semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\leq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.e) Sign indefinite if it takes both positive and negative values.\n\nQuadratic forms are particularly useful in determining the concavity or convexity of a differentiable function.\n\nFor a function y = f(x_{1},x_{2},\\dots,x_{n}), we can form the Hessian consisting of second‑order partial derivatives and construct a quadratic form as \\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x}. Then\n\na) The function y is strictly convex if the quadratic form is positive (implying that the Hessian is positive definite, i.e., the determinants of the principal minors are all positive).b) The function y is strictly concave if the quadratic form is negative (that is, the Hessian is negative definite, i.e. the determinants of the principal minors alternate in sign).\n\nLet’s take an example to see this more clearly.\n\nSay we havez = x^{2} + y^{2}.\n\nThenz_{x} = 2x,\\quad z_{xx} = 2,\\quad z_{xy} = 0, \\\\\nz_{y} = 2y,\\quad z_{yx} = 0,\\quad z_{yy} = 2.\n\nBut\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 2x^{2} + 2y^{2} > 0.\n\nHence the function z = x^{2} + y^{2} is characterized by a minimum at its optimal value and is therefore a strictly convex function. And it is easy to see that the Hessian is positive definite!\n\nHere’s another example:\n\nSay we have a quadratic form in 2 variablesz = 8x^{2} + 6xy + 2y^{2}.\n\nThis can be rearranged asz = 8x^{2} + 3xy + 3xy + 2y^{2}.\n\nWe can write this as\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 8x^{2} + 2y^{2} + (3)(3)xy = 8x^{2} + 2y^{2} + 9xy.\n\nThe Hessian is\\mathbf{H} = \n\\begin{bmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{bmatrix}\n\nThe principal minors are |H_{1}| = 8 and $|H_{2}| =\\begin{vmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{vmatrix}\n\nBoth are positive so we have the Hessian and the quadratic form as positive definite.","type":"content","url":"/linear-algebra-special#the-quadratic-form","position":9},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"type":"lvl2","url":"/linear-algebra-special#higher-order-hessian","position":10},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"content":"Given y = f(x_{1},x_{2},x_{3}), the third‑order Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\ny_{11} & y_{12} & y_{13} \\\\\ny_{21} & y_{22} & y_{23} \\\\\ny_{31} & y_{32} & y_{33}\n\\end{vmatrix}\n\nwhere the elements are the various second‑order partial derivatives of y:y_{11} = \\frac{\\partial^{2}y}{\\partial x_{1}^{2}},\\quad \ny_{12} = \\frac{\\partial^{2}y}{\\partial x_{2}\\partial x_{1}},\\quad \ny_{23} = \\frac{\\partial^{2}y}{\\partial x_{3}\\partial x_{2}},\\quad \\text{etc.}\n\nConditions for a relative minimum or maximum depend on the signs of the first, second, and third principal minors, respectively. If|H_{1}| > 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| > 0,\n\nthen |\\mathbf{H}| is positive definite and fulfills the second‑order conditions for a minimum.\n\nIf|H_{1}| < 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| < 0,\n\nthen |\\mathbf{H}| is negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example.\n\nGiven the functiony = -5x_{1}^{2} + 10x_{1} + x_{1}x_{3} - 2x_{2}^{2} + 4x_{2} + 2x_{2}x_{3} - 4x_{3}^{2}.\n\nThe first order conditions (F.O.C) are\\frac{\\partial y}{\\partial x_1} = y_1 = -10x_1 + 10 + x_3 = 0, \\\\\n\\frac{\\partial y}{\\partial x_2} = y_2 = -4x_2 + 2x_3 + 4 = 0, \\\\\n\\frac{\\partial y}{\\partial x_3} = y_3 = x_1 + 2x_2 - 8x_3 = 0.\n\nwhich can be expressed in matrix form as\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-10 \\\\ -4 \\\\ 0\n\\end{bmatrix}.\n\nUsing Cramer’s rule we get x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43 (please verify this).\n\nLet’s take the second partial derivatives from the first‑order conditions to create the Hessian,y_{11} = -10,\\quad y_{12} = 0,\\quad y_{13} = 1, \\\\\ny_{21} = 0,\\quad y_{22} = -4,\\quad y_{23} = 2, \\\\\ny_{31} = 1,\\quad y_{32} = 2,\\quad y_{33} = -8.\n\nThus,\\mathbf{H} = \n\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\nFinally, applying the Hessian test, we have|H_1| = -10 < 0,\\quad \n|H_2| = \n\\begin{vmatrix}\n-10 & 0 \\\\\n0 & -4\n\\end{vmatrix} = 40 > 0,\\quad \n|H_3| = |\\mathbf{H}| = \n\\begin{vmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{vmatrix} = -80 < 0.\n\nSince the principal minors alternate correctly in sign, the Hessian is negative definite and the function is maximized at x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43.","type":"content","url":"/linear-algebra-special#higher-order-hessian","position":11},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-special#the-bordered-hessian","position":12},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"content":"To optimize a function f(x_1, x_2) subject to a constraint g(x_1, x_2) the first‑order conditions can be found by setting up what is known as the Lagrangian function F(x_1, x_2, \\lambda) = f(x_1, x_2) - \\lambda(g(x_1, x_2) - c).\n\nThe second‑order conditions can be expressed in terms of a bordered Hessian |\\bar{\\mathbf{H}}| as|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & F_{11} & F_{12} \\\\\ng_2 & F_{21} & F_{22}\n\\end{vmatrix}\n\nor|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & f_{11} & f_{12} \\\\\ng_2 & f_{21} & f_{22}\n\\end{vmatrix}\n\nwhich is the usual Hessian bordered by the first derivatives of the constraint with zero on the principal diagonal.\n\nThe order of a bordered principal minor is determined by the order of the principal minor being bordered. Hence |\\bar{H}_2| above represents a second bordered principal minor, because the principal minor being bordered has dimensions 2\\times 2.\n\nFor the bivariate case with a single constraint, we simply look at |\\bar{H}_2|. If this is negative, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum. However, if it is positive, the bordered Hessian is said to be negative definite, and meets the sufficient conditions for a maximum.\n\nLet’s try optimizing the following objective functionf(x_1, x_2) = 4x_1^2 + 3x_2^2 - 2x_1x_2\n\nsubject tox_1 + x_2 = 56.\n\nSetting up the Lagrangian function, taking the first‑order partials and solving gives x_1^* = 36, x_2^* = 20 (and \\lambda = 348). (You should verify this.)\n\nThe bordered Hessian for this optimization problem is|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & 1 & 1 \\\\\n1 & 8 & -2 \\\\\n1 & -2 & 6\n\\end{vmatrix}\n\nStarting with the second principal minor, we have|\\bar{H}_2| = |\\bar{\\mathbf{H}}| = 0 \\cdot \n\\begin{vmatrix}\n8 & -2 \\\\\n-2 & 6\n\\end{vmatrix}\n- 1 \\cdot \n\\begin{vmatrix}\n1 & -2 \\\\\n1 & 6\n\\end{vmatrix}\n+ 1 \\cdot \n\\begin{vmatrix}\n1 & 8 \\\\\n1 & -2\n\\end{vmatrix}\n= - (6+2) + (-2-8) = -8 -10 = -18.\n\nwhich is negative, hence |\\bar{\\mathbf{H}}| is positive definite and we have met sufficient conditions for a minimum.\n\nFor the more general case in which the objective function has say n variables, i.e., f(x_1, \\ldots , x_n) which is subject to some constraint g(x_1, \\ldots , x_n), we can set up the bordered Hessian as|\\bar{\\mathbf{H}}| = \n\\begin{bmatrix}\n0 & g_1 & g_2 & \\dots & g_n \\\\\ng_1 & F_{11} & F_{12} & \\dots & F_{1n} \\\\\ng_2 & F_{21} & F_{22} & \\dots & F_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ng_n & F_{n1} & F_{n2} & \\dots & F_{nn}\n\\end{bmatrix}\n\nwhere |\\bar{\\mathbf{H}}| = |\\bar{H}_n| because the n \\times n principal minor is bordered.\n\nIn this case, if all the principal minors are negative, i.e. |\\bar{H}_2|, |\\bar{H}_3|, \\ldots , |\\bar{H}_n| < 0, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum.\n\nOn the other hand, if the principal minors alternate consistently in sign from positive to negative, i.e. |\\bar{H}_2| > 0, |\\bar{H}_3| < 0, |\\bar{H}_4| > 0 etc., the bordered Hessian is negative definite, and meets the sufficient conditions for a maximum.","type":"content","url":"/linear-algebra-special#the-bordered-hessian","position":13},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"type":"lvl2","url":"/linear-algebra-special#input-output-analysis","position":14},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"content":"If a_{ij} is a technical coefficient representing the value of input i required to produce one dollar’s worth of product j, the total demand for good i can be expressed asx_{i} = a_{i1}x_{1} + a_{i2}x_{2} + \\dots + a_{in}x_{n} + b_{i}\n\nwhere b_i is the final demand for product i. What is important to realize here is that the total demand for a product consists of that product being the final demand plus that product being an intermediate good required for the production of other products.\n\nIn matrix form we have\\mathbf{X} = \\mathbf{A}\\mathbf{X} + \\mathbf{B}\n\nwhere, for an n sector economy,\\mathbf{X} = \n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n\\quad\n\\mathbf{A} = \n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nn}\n\\end{bmatrix},\n\\quad\n\\mathbf{B} = \n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n\\end{bmatrix}.\n\n\\mathbf{A} is called the matrix of technical coefficients.\n\nTo find the output (intermediate and final goods) needed to satisfy demand, all we have to do is to solve for \\mathbf{X}:\\mathbf{X} - \\mathbf{A}\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad (\\mathbf{I} - \\mathbf{A})\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad \\mathbf{X} = (\\mathbf{I} - \\mathbf{A})^{-1}\\mathbf{B}.","type":"content","url":"/linear-algebra-special#input-output-analysis","position":15},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"type":"lvl2","url":"/linear-algebra-special#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":16},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"content":"For a square matrix \\mathbf{A}:\n\na) All characteristic roots \\lambda are positive \\Rightarrow \\mathbf{A} is positive definite.b) All \\lambda’s are negative \\Rightarrow \\mathbf{A} is negative definite.c) All \\lambda’s are nonnegative and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is positive semi‑definite.d) All \\lambda’s are nonpositive and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is negative semi‑definite.e) Some \\lambda’s are positive and some negative \\Rightarrow \\mathbf{A} is sign indefinite.\n\nLet’s take an example.\n\nGiven a square matrix\\mathbf{A} = \n\\begin{bmatrix}\n-6 & 3 \\\\\n3 & -6\n\\end{bmatrix}\n\nTo find the characteristic roots (eigenvalues) of \\mathbf{A}, we simply set |\\mathbf{A} - \\lambda \\mathbf{I}| = 0:|\\mathbf{A} - \\lambda \\mathbf{I}| = \n\\begin{vmatrix}\n-6 - \\lambda & 3 \\\\\n3 & -6 - \\lambda\n\\end{vmatrix} = 0\n\nThis means(-6 - \\lambda)(-6 - \\lambda) - 9 = 0 \\\\\n\\lambda^2 + 12\\lambda + 27 = 0 \\\\\n(\\lambda + 9)(\\lambda + 3) = 0 \\\\\n\\lambda = -9, -3.\n\nSince both characteristic roots \\lambda are negative, we say \\mathbf{A} is negative definite.\n\nNote:\\text{(i) } \\sum \\lambda_i = \\operatorname{tr}(\\mathbf{A}), \\qquad\n\\text{(ii) } \\prod \\lambda_i = |\\mathbf{A}|.\n\nLet’s continue with the example above to find the characteristic vector.\n\nWe know one of the roots \\lambda = -9, so substituting in the characteristic matrix gives(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{bmatrix}\n-6 - (-9) & 3 \\\\\n3 & -6 - (-9)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n3 & 3 \\\\\n3 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nSince the coefficient matrix is linearly dependent, there are infinite number of solutions. The product of the matrices gives two equations which are identical:3v_1 + 3v_2 = 0 \\quad \\Rightarrow \\quad v_2 = -v_1.\n\nBy normalizing we havev_1^{2} + v_2^{2} = 1.\n\nSubstituting v_2 = -v_1 givesv_1^{2} + (-v_1)^{2} = 1 \\quad \\Rightarrow \\quad 2v_1^{2} = 1 \\quad \\Rightarrow \\quad v_1^{2} = \\frac{1}{2}.\n\nTaking the positive square root gives v_1 = \\sqrt{1/2} = \\frac{\\sqrt{2}}{2} and substituting into v_2 = -v_1 gives v_2 = -\\frac{\\sqrt{2}}{2}. That is\\mathbf{v}_1 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n-\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.\n\nUsing the second characteristic root \\lambda = -3:(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \n\\begin{bmatrix}\n-6 - (-3) & 3 \\\\\n3 & -6 - (-3)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-3 & 3 \\\\\n3 & -3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nMultiplying out gives -3v_1 + 3v_2 = 0 and 3v_1 - 3v_2 = 0, so v_1 = v_2.\n\nNormalizing as before:v_1^2 + v_2^2 = 1 \\quad \\Rightarrow \\quad 2v_1^2 = 1 \\quad \\Rightarrow \\quad v_1 = \\frac{\\sqrt{2}}{2}.\n\nHence,\\mathbf{v}_2 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-special#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":17}]}