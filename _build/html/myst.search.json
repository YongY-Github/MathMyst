{"version":"1","records":[{"hierarchy":{"lvl1":"Absolute Basics"},"type":"lvl1","url":"/absolute-basics","position":0},{"hierarchy":{"lvl1":"Absolute Basics"},"content":"This chapter introduces the very basics which you need to know, but is not covered in depth here.","type":"content","url":"/absolute-basics","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Mathematics for Economists II"},"content":"These notes introduce core mathematical tools used in economics and related fields.\n\nThe material is organized by topic and is intended to be intuitive, example-driven, and minimally technical where possible.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"type":"lvl2","url":"/#topics","position":2},{"hierarchy":{"lvl1":"Mathematics for Economists II","lvl2":"Topics"},"content":"Absolute Basics\n\nLinear Algebra (Basics)\n\nLinear Algebra (Special Matrices)\n\nMore topics will be added:\n\nCalculus\n\nOptimization\n\nIntegration\n\nDifference equations\n\nDifferential equations\n\nNotes prepared by -YY- (please do not distribute without permission) test ","type":"content","url":"/#topics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics"},"type":"lvl1","url":"/linear-algebra-basic","position":0},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics"},"content":"","type":"content","url":"/linear-algebra-basic","position":1},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Basics"},"type":"lvl2","url":"/linear-algebra-basic#matrix-basics","position":2},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Basics"},"content":"Matrix addition and subtraction: If \\mathbf{A} = (a_{ij}) and \\mathbf{B} = (b_{ij}) are two m \\times n matrices of same dimension, then \\mathbf{A} + \\mathbf{B} is defined as (a_{ij} + b_{ij}). That is, we add element by element the two matrices.Clearly \\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}. The rule applies to matrix subtraction.\n\nTranspose of a matrix: If \\mathbf{A} is an m \\times n matrix, then \\mathbf{A}' is the n \\times m matrix whose rows are the columns of \\mathbf{A}. So \\mathbf{A}' = (a_{ji}). For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}' = \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\nNote: (\\mathbf{A} + \\mathbf{B})' = \\mathbf{A}' + \\mathbf{B}', however (\\mathbf{A}\\mathbf{B})' = \\mathbf{B}'\\mathbf{A}'.\n\nNull matrix: Has all elements as 0. Clearly \\mathbf{A} + \\mathbf{0}_{m,n} = \\mathbf{0}_{m,n} + \\mathbf{A} = \\mathbf{A} for all m \\times n matrices.\n\nScalar multiplication: If \\mathbf{A} = (a_{ij}) then for any constant k, define k\\mathbf{A} = (k a_{ij}). That is, multiply each element in \\mathbf{A} by k.\n\nMatrix multiplication: Say \\mathbf{A} is m \\times n, and \\mathbf{B} is n \\times p, then the m \\times p matrix \\mathbf{A}\\mathbf{B} is the product of \\mathbf{A} and \\mathbf{B}. For example:\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n= \\begin{bmatrix}\nae + bg & af + bh \\\\\nce + dg & cf + dh\n\\end{bmatrix}\n\nand the matrices \\mathbf{A} and \\mathbf{B} are conformable.\n\nDiagonal matrix: A square n \\times n matrix \\mathbf{A} is diagonal if all entries off the ‘main diagonal’ are zero, i.e.\\mathbf{A} = \\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\n0 & a_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\n\nNote: The elements in the diagonal of \\mathbf{A} are the same as those in \\mathbf{A}'.\n\nTrace of a square matrix: If \\mathbf{A} is a square matrix, the trace of \\mathbf{A}, denoted \\operatorname{tr}(\\mathbf{A}), is the sum of the elements on the main/leading diagonal of \\mathbf{A}.\n\nIdentity matrix: Denoted by \\mathbf{I}_n, the n \\times n diagonal matrix with a_{ii} = 1 for all i. That is:\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nSymmetric matrix: A square matrix \\mathbf{A} is symmetric if \\mathbf{A} = \\mathbf{A}'. The identity matrix and the square null matrix are symmetric.","type":"content","url":"/linear-algebra-basic#matrix-basics","position":3},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Idempotent matrix"},"type":"lvl2","url":"/linear-algebra-basic#idempotent-matrix","position":4},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Idempotent matrix"},"content":"A square matrix is said to be idempotent if \\mathbf{A}^2 = \\mathbf{A}. Below is an example (try squaring the matrix and see what you get).\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\nSingular Matrix: A matrix that is linearly dependent (in the columns or rows) is singular and has determinant of zero. Note that any two (or three) vectors \\mathbf{x} and \\mathbf{y} (and \\mathbf{z}) are said to be linearly dependent iff one can be written as a scalar multiple of the other. Two vectors \\mathbf{x} \\neq \\mathbf{0} and \\mathbf{y} \\neq \\mathbf{0} are said to be linearly independent iff the ONLY solution to a\\mathbf{x} + b\\mathbf{y} = \\mathbf{0} is a = 0 AND b = 0. And \\mathbf{x} and \\mathbf{y} are said to be orthogonal.","type":"content","url":"/linear-algebra-basic#idempotent-matrix","position":5},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Some Notes on Matrices"},"type":"lvl2","url":"/linear-algebra-basic#some-notes-on-matrices","position":6},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Some Notes on Matrices"},"content":"Usually \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}. For example, try \\mathbf{A} = \\begin{bmatrix}2 & 0 \\\\ 3 & 1\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}0 & 1 \\\\ 0 & 0\\end{bmatrix}.\n\nWe can have \\mathbf{A}\\mathbf{B} = \\mathbf{0} even if \\mathbf{A} or \\mathbf{B} are not zero. For example, try \\mathbf{A} = \\begin{bmatrix}6 & -12 \\\\ -3 & 6\\end{bmatrix} and \\mathbf{B} = \\begin{bmatrix}12 & 6 \\\\ 6 & 3\\end{bmatrix}.\n\nSay, \\mathbf{A} = \\begin{bmatrix}4 & 8 \\\\ 1 & 2\\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix}2 & 1 \\\\ 2 & 2\\end{bmatrix}, and \\mathbf{C} = \\begin{bmatrix}-2 & 1 \\\\ 4 & 2\\end{bmatrix}. We find that \\mathbf{A}\\mathbf{B} = \\mathbf{A}\\mathbf{C} even though \\mathbf{B} \\neq \\mathbf{C}.\n\nSay \\mathbf{A} and \\mathbf{B} are singular matrices. Then \\mathbf{A}\\mathbf{B} will not be zero, although the product will be a singular matrix.","type":"content","url":"/linear-algebra-basic#some-notes-on-matrices","position":7},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"type":"lvl2","url":"/linear-algebra-basic#systems-of-equations-in-matrix-form","position":8},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Systems of Equations in Matrix Form"},"content":"One of the uses of linear algebra in economics is to represent and then solve systems of equations. For instance, consider the system of two equations:3x_1 + 4x_2 = 5 \\\\\n7x_1 - 2x_2 = 2\n\nHere, we can define:\\mathbf{A} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix}\n\nThen we see that:\\mathbf{A}\\mathbf{x} = \\begin{pmatrix} 3 & 4 \\\\ 7 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3x_1 + 4x_2 \\\\ 7x_1 - 2x_2 \\end{pmatrix}\n\nThe original system is in fact equivalent to the matrix form:\\mathbf{A}\\mathbf{x} = \\mathbf{b}","type":"content","url":"/linear-algebra-basic#systems-of-equations-in-matrix-form","position":9},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Inversion"},"type":"lvl2","url":"/linear-algebra-basic#matrix-inversion","position":10},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Matrix Inversion"},"content":"To better understand the mechanics of finding the inverse of a square matrix, let’s begin by looking at an example.\\mathbf{A} = \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix}\n\nAssume we have:\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix}\n\nWhere does the 10 in the matrix after the = sign come from? It is the determinant of \\mathbf{A}.\n\nNow we have:\\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 10 & 0 \\\\ 0 & 10 \\end{bmatrix} = 10 \\mathbf{I}\n\nHence, (pre-)multiplying both sides of the equation by \\frac{1}{10} will give you:\\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -2 \\\\ 3 & 4 \\end{bmatrix} = \\mathbf{I}\n\nThe matrix \\frac{1}{10} \\begin{bmatrix} 4 & 2 \\\\ -3 & 1 \\end{bmatrix} is in fact the inverse of \\mathbf{A}. Hence we have the relationship \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}. In fact, \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.\n\nHere are some properties of the inverse of a matrix worth knowing:\n\nProperty 1: For any nonsingular matrix \\mathbf{A}, (\\mathbf{A}^{-1})^{-1} = \\mathbf{A}.\n\nProperty 2: The determinant of the inverse of a matrix is equal to the reciprocal of the determinant of the matrix. That is |\\mathbf{A}^{-1}| = \\frac{1}{|\\mathbf{A}|}.\n\nProperty 3: The inverse of matrix \\mathbf{A} is unique.\n\nProperty 4: For any nonsingular matrix \\mathbf{A}, the inverse of the transpose of a matrix is equal to the transpose of the inverse of the matrix. (\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'.\n\nProperty 5: If \\mathbf{A} and \\mathbf{B} are nonsingular and of the same dimension, then \\mathbf{A}\\mathbf{B} is also nonsingular.\n\nProperty 6: The inverse of the product of two matrices is equal to the product of their inverses in reverse order. (\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}.","type":"content","url":"/linear-algebra-basic#matrix-inversion","position":11},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic#the-determinant-of-a-matrix","position":12},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"The Determinant of a Matrix"},"content":"The determinant is a scalar value uniquely associated with a square non-singular matrix, and is usually denoted as |\\mathbf{A}|. The question of whether or not a matrix is nonsingular and therefore invertible is linked to the value of its determinant.\n\nWe can write a generic 2 \\times 2 matrix as:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}\n\nand its determinant is defined as:|\\mathbf{A}| = a_{11}a_{22} - a_{12}a_{21}\n\nFor a 3 \\times 3 matrix, say:\\mathbf{B} = \\begin{bmatrix} b_{11} & b_{12} & b_{13} \\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{bmatrix}\n\nits determinant is:|\\mathbf{B}| = b_{11}(b_{22}b_{33} - b_{23}b_{32}) - b_{12}(b_{21}b_{33} - b_{23}b_{31}) + b_{13}(b_{21}b_{32} - b_{22}b_{31})","type":"content","url":"/linear-algebra-basic#the-determinant-of-a-matrix","position":13},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"type":"lvl3","url":"/linear-algebra-basic#properties-of-determinants","position":14},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl3":"Properties of Determinants","lvl2":"The Determinant of a Matrix"},"content":"|\\mathbf{A}| = |\\mathbf{A}'| and |\\mathbf{A}| \\cdot |\\mathbf{B}| = |\\mathbf{B}| \\cdot |\\mathbf{A}|.\n\nInterchanging any two rows or columns will affect the sign of the determinant, but not its absolute value.\n\nMultiplying a single row or column by a scalar will cause the value of the determinant to be multiplied by the scalar.\n\nAddition or subtraction of a nonzero multiple of any row or column to or from another row or column does not change the value of the determinant.\n\nThe determinant of a triangular matrix is equal to the product of the elements along the principal diagonal.\n\nIf any of the rows or columns equal zero, the determinant is also zero.\n\nIf two rows or columns are identical or proportional, i.e. linearly dependent, then the determinant is zero.","type":"content","url":"/linear-algebra-basic#properties-of-determinants","position":15},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix"},"type":"lvl2","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix","position":16},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Finding the Inverse of a Matrix"},"content":"The inverse of a matrix is defined as:\\mathbf{A}^{-1} = \\frac{1}{|\\mathbf{A}|} \\operatorname{adj}(\\mathbf{A})\n\nLet’s say we have the following linear system:2x_1 + 9x_2 + 4x_3 = -13 \\\\\n7x_1 - 8x_2 + 6x_3 = 63 \\\\\n5x_1 + 3x_2 - 6x_3 = 6\n\nWriting the above system in the form \\mathbf{A}\\mathbf{x} = \\mathbf{b}, where:\\mathbf{A} = \\begin{bmatrix} 2 & 9 & 4 \\\\ 7 & -8 & 6 \\\\ 5 & 3 & -6 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nWe can therefore solve the system by \\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}, which means that we need the inverse of \\mathbf{A}.\n\nStep 1. The minor |M_{ij}| is the determinant of the submatrix formed by deleting the i-th row and j-th column of the original matrix. So we have:\\begin{bmatrix}\n30 & -72 & 61 \\\\\n-66 & -32 & -39 \\\\\n86 & -16 & -79\n\\end{bmatrix}\n\nStep 2: The cofactor C_{ij} is a minor with the prescribed sign that follows the rule:C_{ij} = (-1)^{i+j} |M_{ij}|\n\nHence, we have:\\begin{bmatrix}\n30 & 72 & 61 \\\\\n66 & -32 & 39 \\\\\n86 & 16 & -79\n\\end{bmatrix}\n\nStep 3: An adjoint matrix is simply the transpose of a cofactor matrix. Continuing the example above, we have:\\operatorname{adj}(\\mathbf{A}) = \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nSince |\\mathbf{A}| = 952, we then have:\\mathbf{A}^{-1} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix}\n\nand\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b} = \\frac{1}{952} \\begin{bmatrix}\n30 & 66 & 86 \\\\\n72 & -32 & 16 \\\\\n61 & 39 & -79\n\\end{bmatrix} \\begin{bmatrix} -13 \\\\ 63 \\\\ 6 \\end{bmatrix}\n\nNot a nice one to solve, though.","type":"content","url":"/linear-algebra-basic#finding-the-inverse-of-a-matrix","position":17},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"type":"lvl2","url":"/linear-algebra-basic#cramers-rule","position":18},{"hierarchy":{"lvl1":"2. Linear Algebra: Basics","lvl2":"Cramer’s Rule"},"content":"Cramer came up with a simplified method for solving a system of linear equations through the use of determinants.\n\nBasically, given \\mathbf{A}\\mathbf{x} = \\mathbf{b}, we have:\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}\n\nDefine:\\Delta = |\\mathbf{A}|\n\nand\\Delta_1 = \\begin{vmatrix} b_1 & a_{12} & a_{13} \\\\ b_2 & a_{22} & a_{23} \\\\ b_3 & a_{32} & a_{33} \\end{vmatrix}, \\quad \\Delta_2 = \\begin{vmatrix} a_{11} & b_1 & a_{13} \\\\ a_{21} & b_2 & a_{23} \\\\ a_{31} & b_3 & a_{33} \\end{vmatrix}, \\quad \\Delta_3 = \\begin{vmatrix} a_{11} & a_{12} & b_1 \\\\ a_{21} & a_{22} & b_2 \\\\ a_{31} & a_{32} & b_3 \\end{vmatrix}\n\nwhere the columns in the \\mathbf{A} matrix are replaced one by one by the \\mathbf{b} vector as shown above.\n\nThen:x_i = \\frac{\\Delta_i}{\\Delta}\n\nVerify that you get \\{4.5, -3, 1.25\\}.\n\nLinear algebra 1-5.pptx (17107k) 8 Jan 31, 2018, 7:56 AM v.1Linear algebra 6.pptx (3924k) 8 Feb 5, 2018, 7:15 AM v.1","type":"content","url":"/linear-algebra-basic#cramers-rule","position":19},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices"},"type":"lvl1","url":"/linear-algebra-special","position":0},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices"},"content":"Let’s look at some special matrices and determinants that are useful in economic analysis.","type":"content","url":"/linear-algebra-special","position":1},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"type":"lvl2","url":"/linear-algebra-special#the-jacobian","position":2},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Jacobian"},"content":"The Jacobian (named after German mathematician Karl Gustav Jacobi, 1804–1851) is generally used in conjunction with partial derivatives to provide an easy test for the existence of functional dependence (linear and nonlinear). A Jacobian determinant |\\mathbf{J}| is composed of all the first‑order partial derivatives of a system of equations, arranged in order sequence. Giveny_{1} = f_{1}(x_{1},x_{2},x_{3}) \\\\\ny_{2} = f_{2}(x_{1},x_{2},x_{3}) \\\\\ny_{3} = f_{3}(x_{1},x_{2},x_{3})\n\nThen|\\mathbf{J}| = \n\\begin{vmatrix}\n\\frac{\\partial y_{1}}{\\partial x_{1}} & \\frac{\\partial y_{1}}{\\partial x_{2}} & \\frac{\\partial y_{1}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} & \\frac{\\partial y_{2}}{\\partial x_{3}} \\\\[4pt]\n\\frac{\\partial y_{3}}{\\partial x_{1}} & \\frac{\\partial y_{3}}{\\partial x_{2}} & \\frac{\\partial y_{3}}{\\partial x_{3}}\n\\end{vmatrix}\n\nFor example, say you havey_{1} = 5x_{1} + 3x_{2} \\\\\ny_{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}\n\nThen taking the first‑order partials gives\\frac{\\partial y_1}{\\partial x_1} = 5,\\qquad \n\\frac{\\partial y_1}{\\partial x_2} = 3,\\qquad \n\\frac{\\partial y_2}{\\partial x_1} = 50x_1 + 30x_2,\\qquad \n\\frac{\\partial y_2}{\\partial x_2} = 30x_1 + 18x_2\n\nAnd the Jacobian is|\\mathbf{J}| = \n\\begin{vmatrix}\n5 & 3 \\\\\n50x_1 + 30x_2 & 30x_1 + 18x_2\n\\end{vmatrix}\n\nThe determinant of the Jacobian matrix is|\\mathbf{J}| = 5(30x_1 + 18x_2) - 3(50x_1 + 30x_2) = 0\n\nSince |\\mathbf{J}| = 0, there is functional dependence between the equations. (Note: (5x_{1} + 3x_{2})^{2} = 25x_{1}^{2} + 30x_{1}x_{2} + 9x_{2}^{2}.)","type":"content","url":"/linear-algebra-special#the-jacobian","position":3},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Hessian"},"type":"lvl2","url":"/linear-algebra-special#the-hessian","position":4},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Hessian"},"content":"Given that the first‑order conditions z_{x} = z_{y} = 0 are met, a sufficient condition for a multivariate function z = f(x,y) to be an optimum is\n\nz_{xx}, z_{yy} > 0 for a minimum; z_{xx}, z_{yy} < 0 for a maximum.\n\nz_{xx} z_{yy} > (z_{xy})^{2}.\n\nA convenient test for this second‑order condition is provided by the Hessian. A Hessian |\\mathbf{H}| is a determinant composed of all the second‑order partial derivatives, with the second‑order direct partials on the principal diagonal and the second‑order cross partials off the principal diagonal. That is,|\\mathbf{H}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n\nwhere (by Young’s Theorem) z_{xy} = z_{yx}.\n\nIf the first element on the principal diagonal, a.k.a. the first principal minor, |H_{1}| = z_{xx} is positive, and the second principal minor|H_{2}| = \n\\begin{vmatrix}\nz_{xx} & z_{xy} \\\\\nz_{yx} & z_{yy}\n\\end{vmatrix}\n= z_{xx} z_{yy} - (z_{xy})^{2} > 0,\n\nthen the second‑order conditions for a minimum are set. That is, when |H_{1}| > 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be positive definite. And a positive definite Hessian fulfills the second‑order conditions for a minimum.\n\nIf the first principal minor |H_{1}| < 0, while the second principal minor remains positive, then the second‑order conditions for a maximum are met. That is, when |H_{1}| < 0 and |H_{2}| > 0, the Hessian |\\mathbf{H}| is said to be negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example:\n\nGivenz = f(x,y) = 2x^{2} - xy + 2y^{2} - 5x - 6y + 20,\n\nthenz_x = 4x - y - 5,\\quad z_y = -x + 4y - 6,\\quad z_{xx} = 4,\\quad z_{yy} = 4,\\quad z_{xy} = z_{yx} = -1.\n\nThe Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\n4 & -1 \\\\\n-1 & 4\n\\end{vmatrix}\n\nWe have |H_{1}| = 4 > 0 and |H_{2}| = (4)(4) - (-1)^2 = 16 - 1 = 15 > 0. Since |H_{1}| > 0 and |H_{2}| > 0, i.e. the Hessian is positive definite, then the function z is characterized by a minimum at the critical values (can you find these?).","type":"content","url":"/linear-algebra-special#the-hessian","position":5},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"type":"lvl2","url":"/linear-algebra-special#the-discriminant","position":6},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Discriminant"},"content":"Determinants can be used to test for positive and negative definiteness of any quadratic form. The determinant of a quadratic form is called a discriminant |\\mathbf{D}|. Given the quadratic formz = a x^{2} + b x y + c y^{2},\n\nthe determinant is formed by placing the coefficients of the squared terms on the principal diagonal and dividing the coefficients of the non‑squared terms equally between the off‑diagonal positions. Hence, we have|\\mathbf{D}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n\nWe then evaluate the principal minors like we did for the Hessian test, where|D_{1}| = a \\quad \\text{and} \\quad |D_{2}| = \n\\begin{vmatrix}\na & b/2 \\\\\nb/2 & c\n\\end{vmatrix}\n= a c - \\frac{b^{2}}{4}.\n\nIf |D_{1}|, |D_{2}| > 0, |\\mathbf{D}| is positive definite and z is positive for all nonzero values of x and y. If |D_{1}| < 0 and |D_{2}| > 0, |\\mathbf{D}| is negative definite and z is negative for all nonzero values of x and y. If |D_{2}| \\neq 0, z is not sign definite and z may assume both positive and negative values.\n\nLet’s take an example to test for sign definiteness of the following quadratic formz = 2x^{2} + 5xy + 8y^{2}.\n\nWe can easily form the discriminant|\\mathbf{D}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n\nThen evaluating the principal minors gives|D_{1}| = 2 > 0, \\qquad |D_{2}| = \n\\begin{vmatrix}\n2 & 2.5 \\\\\n2.5 & 8\n\\end{vmatrix}\n= 16 - 6.25 = 9.75 > 0.\n\nHence, z is positive definite, meaning that it will be greater than zero for all nonzero values of x and y.","type":"content","url":"/linear-algebra-special#the-discriminant","position":7},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"type":"lvl2","url":"/linear-algebra-special#the-quadratic-form","position":8},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Quadratic Form"},"content":"A quadratic form is defined as a polynomial expression in which each component term has a uniform degree.\n\nHere are some examples:6x^{2} - 2xy + 3y^{2} \\quad \\text{is a quadratic form in 2 variables.} \\\\\nx^{2} + 2xy + 4xz + 2yz + y^{2} + z^{2} \\quad \\text{is a quadratic form in 3 variables.}\n\nIt would be useful to determine the sign definiteness so we can make statements concerning the optimum value of the function as to whether it is a minimum or a maximum.\n\nMore generally, a quadratic form in n variables (x_{1},x_{2},\\ldots ,x_{n}) can be written as \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} where\n\n\\mathbf{x}^{\\prime} is a row vector [x_{1},x_{2},\\dots,x_{n}] and\\mathbf{A} is an n\\times n matrix of scalar elements.\n\nTo see this more clearly,\\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = \n\\begin{bmatrix}\nx_{1} & x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 3 \\\\\n4 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= \n\\begin{bmatrix}\nx_{1} + 4x_{2} & 3x_{1} + 5x_{2}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2}\n\\end{bmatrix}\n= x_{1}^{2} + 7x_{1}x_{2} + 5x_{2}^{2}.\n\nA quadratic form is said to be:\n\na) Positive definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} > 0 for all \\mathbf{x} \\neq \\mathbf{0}.b) Positive semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\geq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.c) Negative definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} < 0 for all \\mathbf{x} \\neq \\mathbf{0}.d) Negative semi‑definite if \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} \\leq 0 and \\exists \\mathbf{x} \\neq \\mathbf{0} such that \\mathbf{x}^{\\prime}\\mathbf{A}\\mathbf{x} = 0.e) Sign indefinite if it takes both positive and negative values.\n\nQuadratic forms are particularly useful in determining the concavity or convexity of a differentiable function.\n\nFor a function y = f(x_{1},x_{2},\\dots,x_{n}), we can form the Hessian consisting of second‑order partial derivatives and construct a quadratic form as \\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x}. Then\n\na) The function y is strictly convex if the quadratic form is positive (implying that the Hessian is positive definite, i.e., the determinants of the principal minors are all positive).b) The function y is strictly concave if the quadratic form is negative (that is, the Hessian is negative definite, i.e. the determinants of the principal minors alternate in sign).\n\nLet’s take an example to see this more clearly.\n\nSay we havez = x^{2} + y^{2}.\n\nThenz_{x} = 2x,\\quad z_{xx} = 2,\\quad z_{xy} = 0, \\\\\nz_{y} = 2y,\\quad z_{yx} = 0,\\quad z_{yy} = 2.\n\nBut\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 2x^{2} + 2y^{2} > 0.\n\nHence the function z = x^{2} + y^{2} is characterized by a minimum at its optimal value and is therefore a strictly convex function. And it is easy to see that the Hessian is positive definite!\n\nHere’s another example:\n\nSay we have a quadratic form in 2 variablesz = 8x^{2} + 6xy + 2y^{2}.\n\nThis can be rearranged asz = 8x^{2} + 3xy + 3xy + 2y^{2}.\n\nWe can write this as\\mathbf{x}^{\\prime}\\mathbf{H}\\mathbf{x} = z_{xx}x^{2} + z_{yy}y^{2} + z_{xy}z_{yx}xy = 8x^{2} + 2y^{2} + (3)(3)xy = 8x^{2} + 2y^{2} + 9xy.\n\nThe Hessian is\\mathbf{H} = \n\\begin{bmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{bmatrix}\n\nThe principal minors are |H_{1}| = 8 and $|H_{2}| =\\begin{vmatrix}\n8 & 3 \\\\\n3 & 2\n\\end{vmatrix}\n\nBoth are positive so we have the Hessian and the quadratic form as positive definite.","type":"content","url":"/linear-algebra-special#the-quadratic-form","position":9},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"type":"lvl2","url":"/linear-algebra-special#higher-order-hessian","position":10},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Higher Order Hessian"},"content":"Given y = f(x_{1},x_{2},x_{3}), the third‑order Hessian is|\\mathbf{H}| = \n\\begin{vmatrix}\ny_{11} & y_{12} & y_{13} \\\\\ny_{21} & y_{22} & y_{23} \\\\\ny_{31} & y_{32} & y_{33}\n\\end{vmatrix}\n\nwhere the elements are the various second‑order partial derivatives of y:y_{11} = \\frac{\\partial^{2}y}{\\partial x_{1}^{2}},\\quad \ny_{12} = \\frac{\\partial^{2}y}{\\partial x_{2}\\partial x_{1}},\\quad \ny_{23} = \\frac{\\partial^{2}y}{\\partial x_{3}\\partial x_{2}},\\quad \\text{etc.}\n\nConditions for a relative minimum or maximum depend on the signs of the first, second, and third principal minors, respectively. If|H_{1}| > 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| > 0,\n\nthen |\\mathbf{H}| is positive definite and fulfills the second‑order conditions for a minimum.\n\nIf|H_{1}| < 0,\\quad \n|H_{2}| = \n\\begin{vmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22}\n\\end{vmatrix} > 0,\\quad \n\\text{and} \\quad |H_{3}| = |\\mathbf{H}| < 0,\n\nthen |\\mathbf{H}| is negative definite and fulfills the second‑order conditions for a maximum.\n\nLet’s take an example.\n\nGiven the functiony = -5x_{1}^{2} + 10x_{1} + x_{1}x_{3} - 2x_{2}^{2} + 4x_{2} + 2x_{2}x_{3} - 4x_{3}^{2}.\n\nThe first order conditions (F.O.C) are\\frac{\\partial y}{\\partial x_1} = y_1 = -10x_1 + 10 + x_3 = 0, \\\\\n\\frac{\\partial y}{\\partial x_2} = y_2 = -4x_2 + 2x_3 + 4 = 0, \\\\\n\\frac{\\partial y}{\\partial x_3} = y_3 = x_1 + 2x_2 - 8x_3 = 0.\n\nwhich can be expressed in matrix form as\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-10 \\\\ -4 \\\\ 0\n\\end{bmatrix}.\n\nUsing Cramer’s rule we get x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43 (please verify this).\n\nLet’s take the second partial derivatives from the first‑order conditions to create the Hessian,y_{11} = -10,\\quad y_{12} = 0,\\quad y_{13} = 1, \\\\\ny_{21} = 0,\\quad y_{22} = -4,\\quad y_{23} = 2, \\\\\ny_{31} = 1,\\quad y_{32} = 2,\\quad y_{33} = -8.\n\nThus,\\mathbf{H} = \n\\begin{bmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{bmatrix}\n\nFinally, applying the Hessian test, we have|H_1| = -10 < 0,\\quad \n|H_2| = \n\\begin{vmatrix}\n-10 & 0 \\\\\n0 & -4\n\\end{vmatrix} = 40 > 0,\\quad \n|H_3| = |\\mathbf{H}| = \n\\begin{vmatrix}\n-10 & 0 & 1 \\\\\n0 & -4 & 2 \\\\\n1 & 2 & -8\n\\end{vmatrix} = -80 < 0.\n\nSince the principal minors alternate correctly in sign, the Hessian is negative definite and the function is maximized at x_1 \\approx 1.04, x_2 \\approx 1.22 and x_3 \\approx 0.43.","type":"content","url":"/linear-algebra-special#higher-order-hessian","position":11},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"type":"lvl2","url":"/linear-algebra-special#the-bordered-hessian","position":12},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"The Bordered Hessian"},"content":"To optimize a function f(x_1, x_2) subject to a constraint g(x_1, x_2) the first‑order conditions can be found by setting up what is known as the Lagrangian function F(x_1, x_2, \\lambda) = f(x_1, x_2) - \\lambda(g(x_1, x_2) - c).\n\nThe second‑order conditions can be expressed in terms of a bordered Hessian |\\bar{\\mathbf{H}}| as|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & F_{11} & F_{12} \\\\\ng_2 & F_{21} & F_{22}\n\\end{vmatrix}\n\nor|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & g_1 & g_2 \\\\\ng_1 & f_{11} & f_{12} \\\\\ng_2 & f_{21} & f_{22}\n\\end{vmatrix}\n\nwhich is the usual Hessian bordered by the first derivatives of the constraint with zero on the principal diagonal.\n\nThe order of a bordered principal minor is determined by the order of the principal minor being bordered. Hence |\\bar{H}_2| above represents a second bordered principal minor, because the principal minor being bordered has dimensions 2\\times 2.\n\nFor the bivariate case with a single constraint, we simply look at |\\bar{H}_2|. If this is negative, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum. However, if it is positive, the bordered Hessian is said to be negative definite, and meets the sufficient conditions for a maximum.\n\nLet’s try optimizing the following objective functionf(x_1, x_2) = 4x_1^2 + 3x_2^2 - 2x_1x_2\n\nsubject tox_1 + x_2 = 56.\n\nSetting up the Lagrangian function, taking the first‑order partials and solving gives x_1^* = 36, x_2^* = 20 (and \\lambda = 348). (You should verify this.)\n\nThe bordered Hessian for this optimization problem is|\\bar{\\mathbf{H}}| = \n\\begin{vmatrix}\n0 & 1 & 1 \\\\\n1 & 8 & -2 \\\\\n1 & -2 & 6\n\\end{vmatrix}\n\nStarting with the second principal minor, we have|\\bar{H}_2| = |\\bar{\\mathbf{H}}| = 0 \\cdot \n\\begin{vmatrix}\n8 & -2 \\\\\n-2 & 6\n\\end{vmatrix}\n- 1 \\cdot \n\\begin{vmatrix}\n1 & -2 \\\\\n1 & 6\n\\end{vmatrix}\n+ 1 \\cdot \n\\begin{vmatrix}\n1 & 8 \\\\\n1 & -2\n\\end{vmatrix}\n= - (6+2) + (-2-8) = -8 -10 = -18.\n\nwhich is negative, hence |\\bar{\\mathbf{H}}| is positive definite and we have met sufficient conditions for a minimum.\n\nFor the more general case in which the objective function has say n variables, i.e., f(x_1, \\ldots , x_n) which is subject to some constraint g(x_1, \\ldots , x_n), we can set up the bordered Hessian as|\\bar{\\mathbf{H}}| = \n\\begin{bmatrix}\n0 & g_1 & g_2 & \\dots & g_n \\\\\ng_1 & F_{11} & F_{12} & \\dots & F_{1n} \\\\\ng_2 & F_{21} & F_{22} & \\dots & F_{2n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ng_n & F_{n1} & F_{n2} & \\dots & F_{nn}\n\\end{bmatrix}\n\nwhere |\\bar{\\mathbf{H}}| = |\\bar{H}_n| because the n \\times n principal minor is bordered.\n\nIn this case, if all the principal minors are negative, i.e. |\\bar{H}_2|, |\\bar{H}_3|, \\ldots , |\\bar{H}_n| < 0, the bordered Hessian is said to be positive definite and satisfies the second‑order condition for a minimum.\n\nOn the other hand, if the principal minors alternate consistently in sign from positive to negative, i.e. |\\bar{H}_2| > 0, |\\bar{H}_3| < 0, |\\bar{H}_4| > 0 etc., the bordered Hessian is negative definite, and meets the sufficient conditions for a maximum.","type":"content","url":"/linear-algebra-special#the-bordered-hessian","position":13},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"type":"lvl2","url":"/linear-algebra-special#input-output-analysis","position":14},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Input‑Output Analysis"},"content":"If a_{ij} is a technical coefficient representing the value of input i required to produce one dollar’s worth of product j, the total demand for good i can be expressed asx_{i} = a_{i1}x_{1} + a_{i2}x_{2} + \\dots + a_{in}x_{n} + b_{i}\n\nwhere b_i is the final demand for product i. What is important to realize here is that the total demand for a product consists of that product being the final demand plus that product being an intermediate good required for the production of other products.\n\nIn matrix form we have\\mathbf{X} = \\mathbf{A}\\mathbf{X} + \\mathbf{B}\n\nwhere, for an n sector economy,\\mathbf{X} = \n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n\\quad\n\\mathbf{A} = \n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nn}\n\\end{bmatrix},\n\\quad\n\\mathbf{B} = \n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n\\end{bmatrix}.\n\n\\mathbf{A} is called the matrix of technical coefficients.\n\nTo find the output (intermediate and final goods) needed to satisfy demand, all we have to do is to solve for \\mathbf{X}:\\mathbf{X} - \\mathbf{A}\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad (\\mathbf{I} - \\mathbf{A})\\mathbf{X} = \\mathbf{B} \\quad \\Rightarrow \\quad \\mathbf{X} = (\\mathbf{I} - \\mathbf{A})^{-1}\\mathbf{B}.","type":"content","url":"/linear-algebra-special#input-output-analysis","position":15},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"type":"lvl2","url":"/linear-algebra-special#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":16},{"hierarchy":{"lvl1":"3. Linear Algebra: Special Matrices","lvl2":"Characteristic Roots and Vectors (Eigenvalues and Eigenvectors)"},"content":"For a square matrix \\mathbf{A}:\n\na) All characteristic roots \\lambda are positive \\Rightarrow \\mathbf{A} is positive definite.b) All \\lambda’s are negative \\Rightarrow \\mathbf{A} is negative definite.c) All \\lambda’s are nonnegative and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is positive semi‑definite.d) All \\lambda’s are nonpositive and at least one \\lambda = 0 \\Rightarrow \\mathbf{A} is negative semi‑definite.e) Some \\lambda’s are positive and some negative \\Rightarrow \\mathbf{A} is sign indefinite.\n\nLet’s take an example.\n\nGiven a square matrix\\mathbf{A} = \n\\begin{bmatrix}\n-6 & 3 \\\\\n3 & -6\n\\end{bmatrix}\n\nTo find the characteristic roots (eigenvalues) of \\mathbf{A}, we simply set |\\mathbf{A} - \\lambda \\mathbf{I}| = 0:|\\mathbf{A} - \\lambda \\mathbf{I}| = \n\\begin{vmatrix}\n-6 - \\lambda & 3 \\\\\n3 & -6 - \\lambda\n\\end{vmatrix} = 0\n\nThis means(-6 - \\lambda)(-6 - \\lambda) - 9 = 0 \\\\\n\\lambda^2 + 12\\lambda + 27 = 0 \\\\\n(\\lambda + 9)(\\lambda + 3) = 0 \\\\\n\\lambda = -9, -3.\n\nSince both characteristic roots \\lambda are negative, we say \\mathbf{A} is negative definite.\n\nNote:\\text{(i) } \\sum \\lambda_i = \\operatorname{tr}(\\mathbf{A}), \\qquad\n\\text{(ii) } \\prod \\lambda_i = |\\mathbf{A}|.\n\nLet’s continue with the example above to find the characteristic vector.\n\nWe know one of the roots \\lambda = -9, so substituting in the characteristic matrix gives(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{bmatrix}\n-6 - (-9) & 3 \\\\\n3 & -6 - (-9)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n3 & 3 \\\\\n3 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nSince the coefficient matrix is linearly dependent, there are infinite number of solutions. The product of the matrices gives two equations which are identical:3v_1 + 3v_2 = 0 \\quad \\Rightarrow \\quad v_2 = -v_1.\n\nBy normalizing we havev_1^{2} + v_2^{2} = 1.\n\nSubstituting v_2 = -v_1 givesv_1^{2} + (-v_1)^{2} = 1 \\quad \\Rightarrow \\quad 2v_1^{2} = 1 \\quad \\Rightarrow \\quad v_1^{2} = \\frac{1}{2}.\n\nTaking the positive square root gives v_1 = \\sqrt{1/2} = \\frac{\\sqrt{2}}{2} and substituting into v_2 = -v_1 gives v_2 = -\\frac{\\sqrt{2}}{2}. That is\\mathbf{v}_1 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n-\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.\n\nUsing the second characteristic root \\lambda = -3:(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = \n\\begin{bmatrix}\n-6 - (-3) & 3 \\\\\n3 & -6 - (-3)\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n-3 & 3 \\\\\n3 & -3\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\ v_2\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}.\n\nMultiplying out gives -3v_1 + 3v_2 = 0 and 3v_1 - 3v_2 = 0, so v_1 = v_2.\n\nNormalizing as before:v_1^2 + v_2^2 = 1 \\quad \\Rightarrow \\quad 2v_1^2 = 1 \\quad \\Rightarrow \\quad v_1 = \\frac{\\sqrt{2}}{2}.\n\nHence,\\mathbf{v}_2 = \n\\begin{bmatrix}\n\\frac{\\sqrt{2}}{2} \\\\[4pt]\n\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}.","type":"content","url":"/linear-algebra-special#characteristic-roots-and-vectors-eigenvalues-and-eigenvectors","position":17}]}